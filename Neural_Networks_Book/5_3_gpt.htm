<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>5.3 GPT architecture</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="5_transformer.htm"> 5. Attention mechanisms </a>/ 5.3 GPT architecture
          </td>
          <td width="70" align="right">
          <a href="5_2_5_mh_attention_comparison.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="5_3_1_gpt_description.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H2"><span class="f_H2">5.3 GPT architecture</span></p>
<p class="p_Text"><span class="f_Text">In June 2018, OpenAI introduced </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text">, the neural network model, which immediately showed the best results in a number of language tests. In February 2019, they released </span><span class="f_Text" style="font-style: italic;">GPT-2</span><span class="f_Text">, and in May 2020, everyone learned about </span><span class="f_Text" style="font-style: italic;">GPT-3</span><span class="f_Text">. These models demonstrated the possibility of a neural network to generate texts. Experiments were also conducted on the generation of music and images. The main disadvantage of the models is the requirements for computing resources. It took a month to train the first </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> on a machine with 8 GPUs. This disadvantage is partially compensated by the ability to use pre-trained models to solve new problems. However, the size of the model requires resources for its functioning.</span></p>
<p class="p_Text"><span class="f_Text">Conceptually, </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> models are built on the basis of the transformer we have already looked at. The main idea is to pre-train a model without a teacher on a large volume of data and then fine-tune it on a relatively small amount of labeled data.</span></p>
<p class="p_Text"><span class="f_Text">The reason for the two-step training is the size of the model. Modern deep machine learning models, such as </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text">, have a large number of parameters, numbering in the hundreds of millions or more. Therefore, training of such neural networks requires a huge training dataset. When using supervised learning, creating a labeled training dataset can require significant effort. At the same time, there are numerous digitized texts available on the internet which are not unlabeled, making them suitable for unsupervised learning models. However, the results of unsupervised learning are statistically inferior to supervised learning. Therefore, after unsupervised learning, the model undergoes fine-tuning on a relatively small labeled dataset.</span></p>
<p class="p_Text"><span class="f_Text">Unsupervised learning allows </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> to learn a language model, while fine-tuning using labeled data tailors the model for specific tasks. In this way, a single pre-trained model can be replicated and configured to perform different language tasks. The limitation lies in the language of the source dataset for unsupervised learning. </span></p>
<p class="p_Text"><span class="f_Text">As practical experience has shown, such an approach yields good results across a wide range of language tasks. For example, the </span><span class="f_Text" style="font-style: italic;">GPT-3</span><span class="f_Text"> model is able to generate related texts on a given topic. But it's worth noting that the mentioned model contains 175 billion parameters and was pre-trained on a dataset of 570 GB.</span></p>
<p class="p_Text"><span class="f_Text">Despite the fact that </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> models were designed for natural language processing, they also showed impressive results in music and image generation tasks.</span></p>
<p class="p_Text"><span class="f_Text">Theoretically, it is possible to use </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> models with any sequences of digitized data. The question lies in having sufficient data and resources for unsupervised pre-training.</span></p>

</div>

</body>
</html>
