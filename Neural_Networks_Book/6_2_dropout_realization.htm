<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>6.2 Dropout</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="6_improvement_realization.htm"> 6. Architectural solutions for improving model convergence </a>/ 6.2 Dropout
          </td>
          <td width="70" align="right">
          <a href="6_1_5_batch_norm_comparison.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="6_2_1_dropout_mql.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H2"><span class="f_H2">6.2 Dropout</span></p>
<p class="p_Text"><span class="f_Text">As we continue discussing ways to increase the convergence of models, let's consider the </span><span class="f_Text" style="font-style: italic;">Dropout</span><span class="f_Text"> method.</span></p>
<p class="p_Text"><span class="f_Text">When training a neural network, a large number of features are fed into each neuron, and it is difficult to assess the influence of each of them. As a result, errors from some neurons are smoothed out by the correct values from others, and errors accumulate at the output of the neural network. As a result, training stops at a certain local minimum with a relatively large error. This effect is known as feature co-adaptation, where the influence of each feature seems to adapt to the surrounding environment. For us, it would be better to achieve the opposite effect, where the environment is decomposed into individual features, and the influence of each feature is evaluated separately.</span></p>
<div class="p_Text" style="text-align: center;"><div style="margin:0 auto 0 auto;width:600px"><img class="help" alt="Dropout" title="Dropout" width="600" height="400" style="width:600px;height:400px;border:none" src="dout_2hlpercp.png"/><p style="text-align:center"><span class="f_ImageCaption">Dropout</span></p></div></div>
<p class="p_Text"><span class="f_Text">To combat the complex co-adaptation of features, in July 2012, a group of scientists from the University of Toronto in the article </span><span class="f_Text" style="font-style: italic;"><a href="https://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="external" class="weblink" title="Improving neural networks by preventing co-adaptation of feature detectors">Improving neural networks by preventing co-adaptation of feature detectors</a></span><span class="f_Text"> proposed randomly excluding some of the neurons in the learning process. Reducing the number of features during training increases the significance of each one, and constant variation in the quantitative and qualitative composition of features reduces the risk of their co-adaptation. This method is called </span><span class="f_Text" style="font-style: italic;">Dropout</span><span class="f_Text">. Some compare the application of this method to decision trees because, by excluding some neurons, we get a new neural network with its own weights at each training iteration. According to the rules of combinatorics, the variability of such networks is quite high.</span></p>
<p class="p_Text"><span class="f_Text">During the operation of the neural network, all attributes and neurons are evaluated. Thus, we get the most accurate and independent assessment of the current state of the environment under consideration.</span></p>
<p class="p_Text"><span class="f_Text">The authors of the method in their paper mention the possibility of using it to improve the quality of pre-trained models as well.</span></p>
<p class="p_Text"><span class="f_Text">Describing the proposed solution from the mathematics point of view, we can say that each individual neuron is dropped out of the process with a certain given probability </span><span class="f_Text" style="font-style: italic;">p</span><span class="f_Text">, or the neuron will participate in the process of training a neural network with probability </span><span class="f_Text" style="font-style: italic;">q</span><span class="f_Text">.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:79px;height:17px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 316 68"><path d="M25 43 C23 46,21 48,19 50 C17 51,14 52,12 52 C10 52,8 51,6 49 C5 47,4 44,4 40 C4 36,5 32,6 28 C8 24,10 21,13 19 C16 16,19 15,23 15 C24 15,26 15,27 16 C28 16,30 17,31 18 L35 15 L37 16 L28 58 C27 60,27 61,27 62 C27 63,27 64,28 64 C28 65,29 65,31 65 L31 67 L19 67 L26 44 L25 43 Z M28 29 C28 27,29 25,29 24 C29 22,28 20,27 19 C27 19,25 18,23 18 C21 18,19 19,17 21 C15 23,13 26,12 30 C11 33,10 37,10 40 C10 43,11 45,11 46 C12 47,13 48,15 48 C16 48,16 48,17 48 C18 47,19 47,20 46 C21 45,21 44,22 43 C23 42,24 41,24 40 C25 39,26 38,26 36 C27 34,27 33,28 30 L28 29 Z" fill="#212121"/><path d="M68 25 L68 20 L114 20 L114 25 L68 25 Z M68 40 L68 35 L114 35 L114 40 L68 40 Z" fill="#212121"/><path d="M166 41 C166 43,166 44,166 44 C166 45,167 46,167 46 C167 47,168 47,169 47 C170 48,170 48,172 48 C173 48,174 48,176 48 L176 51 L149 51 L149 48 C152 48,154 48,155 48 C156 47,157 47,157 47 C158 46,158 46,159 45 C159 44,159 43,159 41 L159 11 C159 10,159 10,159 9 C158 9,158 9,157 9 C156 9,155 9,154 10 C152 11,151 12,149 13 L147 10 L164 0 L166 0 C166 2,166 6,166 10 L166 41 Z" fill="#212121"/><path d="M204 32 L204 27 L250 27 L250 32 L204 32 Z" fill="#212121"/><path d="M282 28 C283 25,283 23,283 22 C283 21,283 20,282 20 C282 19,282 19,281 19 C280 19,279 19,278 20 C278 21,276 22,275 23 L273 21 C275 19,277 17,278 16 C280 16,282 15,284 15 C285 15,286 16,287 17 C288 18,288 19,288 20 C288 22,288 23,288 25 L288 25 C293 18,297 15,302 15 C304 15,306 16,308 18 C309 20,310 23,310 27 C310 31,309 35,308 39 C306 43,304 46,301 49 C298 51,295 52,291 52 C288 52,285 51,283 50 L281 58 C281 60,281 61,281 62 C281 63,281 64,282 64 C282 65,283 65,285 65 L285 67 L273 67 L282 28 Z M286 38 C285 40,285 41,285 41 C285 42,285 43,285 43 C285 45,286 47,286 48 C287 49,289 49,291 49 C292 49,293 49,294 48 C295 48,296 47,297 46 C298 45,299 44,300 43 C300 41,301 40,302 38 C302 36,303 34,303 32 C303 30,303 29,303 27 C303 24,303 22,302 21 C302 20,301 19,299 19 C298 19,297 20,295 20 C294 21,293 23,291 25 C290 27,289 28,288 30 C287 32,287 34,286 37 L286 38 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">To determine the list of neurons to be dropped out, a pseudo-random number generator with a normal distribution is used. This approach provides the most uniform exclusion of neurons possible. In practice, we will generate a vector with a size equal to the input sequence. For the features used in the vector, we will set 1, and for the excluded elements, we will use 0. </span></p>
<p class="p_Text"><span class="f_Text">However, excluding analyzed features undoubtedly leads to a reduction in the sum at the input of the neuron activation function. To compensate for this effect, we will multiply the value of each feature by a factor of </span><span class="f_Text" style="font-style: italic;">1/q</span><span class="f_Text">. It is obvious that this coefficient will increase the values since the probability of </span><span class="f_Text" style="font-style: italic;">q</span><span class="f_Text"> will always be in the range from 0 to 1.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:85px;height:44px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 340 176"><path d="M26 99 C23 103,21 105,19 107 C17 108,15 109,12 109 C10 109,8 108,6 106 C5 104,4 101,4 97 C4 93,5 89,6 85 C8 81,10 78,13 76 C16 73,19 72,23 72 C24 72,26 72,27 73 C28 73,29 73,31 74 L33 66 C33 65,33 64,33 63 C33 62,33 62,33 61 C33 60,33 60,33 59 C33 59,32 59,32 58 C31 58,30 58,29 58 L30 56 L39 56 L41 56 L32 97 C31 99,31 101,31 102 C31 103,31 104,31 104 C32 105,32 105,33 105 C34 105,35 105,35 104 C36 104,37 103,39 101 L41 103 C39 105,37 107,35 108 C34 109,32 109,30 109 C29 109,28 109,27 108 C26 107,25 105,25 104 C25 103,26 101,26 100 L26 99 Z M28 86 C28 84,29 82,29 81 C29 79,28 77,27 76 C27 76,25 75,23 75 C21 75,19 76,17 78 C15 80,13 83,12 87 C11 90,10 94,10 97 C10 100,11 102,11 103 C12 104,13 105,15 105 C16 105,16 105,17 105 C18 104,19 104,20 103 C21 102,21 101,22 100 C23 99,24 98,24 97 C25 96,26 95,26 93 C27 91,27 90,28 87 L28 86 Z" fill="#212121"/><path d="M55 96 L53 102 L48 102 L49 96 L55 96 Z M48 124 C48 125,48 126,48 127 C48 127,48 128,48 128 C48 129,48 129,48 130 C48 130,49 130,49 130 C50 130,50 130,50 130 C51 130,51 130,51 129 C52 129,52 129,52 128 C53 128,53 127,54 127 L56 129 C55 130,54 131,53 131 C52 132,52 132,51 133 C50 133,50 134,49 134 C48 134,48 134,47 134 C46 134,46 134,45 134 C45 134,44 133,44 133 C43 132,43 132,43 131 C43 131,42 130,42 129 C42 129,43 128,43 127 C43 126,43 125,43 124 C44 123,44 122,44 121 C44 120,45 119,45 118 C45 117,45 116,45 115 C46 114,46 113,46 112 C46 112,46 112,46 111 C46 111,46 111,46 111 C46 110,46 110,45 110 C45 109,44 109,43 109 L43 107 L51 107 L52 107 L48 124 Z" fill="#212121"/><path d="M86 82 L86 77 L132 77 L132 82 L86 82 Z M86 97 L86 92 L132 92 L132 97 L86 97 Z" fill="#212121"/><path d="M184 41 C184 43,184 44,184 44 C184 45,185 46,185 46 C185 47,186 47,187 47 C188 48,188 48,190 48 C191 48,192 48,194 48 L194 51 L167 51 L167 48 C170 48,172 48,173 48 C174 47,175 47,175 47 C176 46,176 46,177 45 C177 44,177 43,177 41 L177 11 C177 10,177 10,177 9 C176 9,176 9,175 9 C174 9,173 9,172 10 C170 11,169 12,167 13 L165 10 L182 0 L184 0 C184 2,184 6,184 10 L184 41 Z" fill="#212121"/><path d="M184 150 C182 153,180 155,178 157 C176 158,173 159,171 159 C169 159,167 158,165 156 C164 154,163 151,163 147 C163 143,164 139,165 135 C167 131,169 128,172 126 C175 123,178 122,182 122 C183 122,185 122,186 123 C187 123,189 124,190 125 L194 122 L196 123 L187 165 C186 167,186 168,186 169 C186 170,186 171,187 171 C187 172,188 172,190 172 L190 174 L178 174 L185 151 L184 150 Z M187 136 C187 134,188 132,188 131 C188 129,187 127,186 126 C186 126,184 125,182 125 C180 125,178 126,176 128 C174 130,172 133,171 137 C170 140,169 144,169 147 C169 150,170 152,170 153 C171 154,172 155,174 155 C175 155,175 155,176 155 C177 154,178 154,179 153 C180 152,180 151,181 150 C182 149,183 148,183 147 C184 146,185 145,185 143 C186 141,186 140,187 137 L187 136 Z" fill="#212121"/><rect x="158" y="84" width="42" height="5" fill="#212121"/><path d="M230 91 C230 89,230 88,229 86 C229 84,229 82,228 81 C228 79,227 78,227 77 C227 77,227 77,226 76 C226 76,226 76,225 76 C225 76,224 76,224 76 C223 77,223 77,222 78 C222 78,221 79,220 80 L218 78 C220 76,221 75,222 74 C224 73,226 72,227 72 C228 72,229 72,229 72 C230 73,231 73,231 73 C231 74,232 74,232 75 C233 75,233 76,233 77 C234 78,234 79,234 81 C234 82,235 84,235 85 L235 85 C238 82,239 79,240 78 C242 76,243 75,244 74 C244 73,245 73,246 73 C247 72,248 72,249 72 C250 72,251 72,252 73 L250 80 L248 80 C248 79,247 78,247 78 C246 78,246 78,246 78 C246 78,245 78,245 79 C245 79,244 79,243 80 C243 81,242 82,241 83 C240 84,239 85,238 86 L236 89 C236 92,237 94,237 95 C238 97,238 99,238 100 C239 101,239 102,239 103 C239 103,240 104,240 104 C240 105,240 105,241 105 C241 105,241 105,242 105 C242 105,243 105,243 104 C244 104,245 103,246 101 L249 103 C247 105,245 107,244 108 C243 109,241 109,239 109 C238 109,237 109,236 108 C236 108,235 107,234 107 C234 106,233 104,233 103 C232 99,232 97,232 95 L231 95 C229 99,227 102,225 103 C224 105,223 106,222 107 C222 108,221 108,220 109 C219 109,218 109,217 109 C216 109,215 109,214 109 L216 101 L218 101 C218 102,219 103,219 103 C220 103,220 103,221 103 C221 103,221 102,222 101 C223 101,224 100,225 98 C226 97,228 94,230 91 L230 91 Z" fill="#212121"/><path d="M264 96 L262 102 L257 102 L258 96 L264 96 Z M257 124 C257 125,257 126,257 127 C257 127,257 128,257 128 C257 129,257 129,257 130 C257 130,258 130,258 130 C259 130,259 130,259 130 C260 130,260 130,260 129 C261 129,261 129,261 128 C262 128,262 127,263 127 L265 129 C264 130,263 131,262 131 C261 132,261 132,260 133 C259 133,259 134,258 134 C257 134,257 134,256 134 C255 134,255 134,254 134 C254 134,253 133,253 133 C252 132,252 132,252 131 C252 131,251 130,251 129 C251 129,252 128,252 127 C252 126,252 125,252 124 C253 123,253 122,253 121 C253 120,254 119,254 118 C254 117,254 116,254 115 C255 114,255 113,255 112 C255 112,255 112,255 111 C255 111,255 111,255 111 C255 110,255 110,254 110 C254 109,253 109,252 109 L252 107 L260 107 L261 107 L257 124 Z" fill="#212121"/><path d="M279 84 C280 82,280 80,280 79 C280 78,280 77,279 77 C279 76,279 76,278 76 C277 76,276 76,275 77 C275 78,273 79,272 80 L270 78 C272 76,274 74,276 73 C277 73,279 72,281 72 C282 72,283 73,284 74 C285 75,286 76,286 77 C286 79,285 80,285 82 L285 82 C287 78,290 76,292 74 C294 73,296 72,299 72 C301 72,303 73,304 74 C305 75,306 77,306 80 C306 81,305 84,305 86 L302 96 C302 99,301 101,301 102 C301 103,301 104,302 104 C302 105,303 105,303 105 C304 105,305 105,306 104 C306 104,308 103,309 101 L311 103 C309 105,307 107,306 108 C304 109,302 109,300 109 C299 109,298 108,297 107 C296 106,295 105,295 103 C295 101,296 99,296 96 L298 89 C299 87,299 85,299 84 C299 83,299 82,299 81 C299 79,299 78,299 77 C298 76,297 76,296 76 C295 76,294 77,292 77 C291 78,290 79,289 81 C287 83,286 84,285 86 C285 88,284 90,283 92 L280 108 L274 108 L279 84 Z" fill="#212121"/><path d="M324 96 L322 102 L317 102 L318 96 L324 96 Z M317 124 C317 125,317 126,317 127 C317 127,317 128,317 128 C317 129,317 129,317 130 C317 130,318 130,318 130 C319 130,319 130,319 130 C320 130,320 130,320 129 C321 129,321 129,321 128 C322 128,322 127,323 127 L325 129 C324 130,323 131,322 131 C321 132,321 132,320 133 C319 133,319 134,318 134 C317 134,317 134,316 134 C315 134,315 134,314 134 C314 134,313 133,313 133 C312 132,312 132,312 131 C312 131,311 130,311 129 C311 129,312 128,312 127 C312 126,312 125,312 124 C313 123,313 122,313 121 C313 120,314 119,314 118 C314 117,314 116,314 115 C315 114,315 113,315 112 C315 112,315 112,315 111 C315 111,315 111,315 111 C315 110,315 110,314 110 C314 109,313 109,312 109 L312 107 L320 107 L321 107 L317 124 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">where:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li" style="font-style: italic;">d</span><span class="f_li" style="font-size: 7pt; font-style: italic; vertical-align: sub;">i</span><span class="f_li"> = elements of the </span><span class="f_li" style="font-style: italic;">Dropout</span><span class="f_li"> result vector</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">q</span><span class="f_li"> = probability of using a neuron in the learning process</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">x</span><span class="f_li" style="font-size: 7pt; font-style: italic; vertical-align: sub;">i</span><span class="f_li"> = elements of the masking vector</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">n</span><span class="f_li" style="font-size: 7pt; font-style: italic; vertical-align: sub;">i</span><span class="f_li"> = elements of the input sequence</span></li>
</ul>
<p class="p_Text"><span class="f_Text">During the backpropagation in the training process, the error gradient is multiplied by the derivative of the above-mentioned function. In the case of </span><span class="f_Text" style="font-style: italic;">Dropout</span><span class="f_Text">, the backpropagation pass will be similar to the feed-forward pass using the masking vector from the feed-forward pass. </span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:124px;height:52px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 496 208"><path d="M38 12 L40 15 C31 24,25 36,20 53 C15 69,13 87,13 107 C13 127,15 145,20 162 C25 179,31 192,40 200 L38 203 C28 195,20 182,15 164 C9 147,6 128,6 107 C6 87,9 68,15 50 C20 33,28 21,38 12 L38 12 Z" fill="#212121"/><path d="M69 62 C69 64,69 65,69 65 C69 66,70 67,70 67 C70 68,71 68,72 68 C73 69,73 69,75 69 C76 69,77 69,79 69 L79 72 L52 72 L52 69 C55 69,57 69,58 69 C59 68,60 68,60 68 C61 67,61 67,62 66 C62 65,62 64,62 62 L62 32 C62 31,62 31,62 30 C61 30,61 30,60 30 C59 30,58 30,57 31 C55 32,54 33,52 34 L50 31 L67 21 L69 21 C69 23,69 27,69 31 L69 62 Z" fill="#212121"/><path d="M69 171 C67 174,65 176,63 178 C61 179,58 180,56 180 C54 180,52 179,50 177 C49 175,48 172,48 168 C48 164,49 160,50 156 C52 152,54 149,57 147 C60 144,63 143,67 143 C68 143,70 143,71 144 C72 144,74 145,75 146 L79 143 L81 144 L72 186 C71 188,71 189,71 190 C71 191,71 192,72 192 C72 193,73 193,75 193 L75 195 L63 195 L70 172 L69 171 Z M72 157 C72 155,73 153,73 152 C73 150,72 148,71 147 C71 147,69 146,67 146 C65 146,63 147,61 149 C59 151,57 154,56 158 C55 161,54 165,54 168 C54 171,55 173,55 174 C56 175,57 176,59 176 C60 176,60 176,61 176 C62 175,63 175,64 174 C65 173,65 172,66 171 C67 170,68 169,68 168 C69 167,70 166,70 164 C71 162,71 161,72 158 L72 157 Z" fill="#212121"/><rect x="43" y="105" width="42" height="5" fill="#212121"/><path d="M115 112 C115 110,115 109,114 107 C114 105,114 103,113 102 C113 100,112 99,112 98 C112 98,112 98,111 97 C111 97,111 97,110 97 C110 97,109 97,109 97 C108 98,108 98,107 99 C107 99,106 100,105 101 L103 99 C105 97,106 96,107 95 C109 94,111 93,112 93 C113 93,114 93,114 93 C115 94,116 94,116 94 C116 95,117 95,117 96 C118 96,118 97,118 98 C119 99,119 100,119 102 C119 103,120 105,120 106 L120 106 C123 103,124 100,125 99 C127 97,128 96,129 95 C129 94,130 94,131 94 C132 93,133 93,134 93 C135 93,136 93,137 94 L135 101 L133 101 C133 100,132 99,132 99 C131 99,131 99,131 99 C131 99,130 99,130 100 C130 100,129 100,128 101 C128 102,127 103,126 104 C125 105,124 106,123 107 L121 110 C121 113,122 115,122 116 C123 118,123 120,123 121 C124 122,124 123,124 124 C124 124,125 125,125 125 C125 126,125 126,126 126 C126 126,126 126,127 126 C127 126,128 126,128 125 C129 125,130 124,131 122 L134 124 C132 126,130 128,129 129 C128 130,126 130,124 130 C123 130,122 130,121 129 C121 129,120 128,119 128 C119 127,118 125,118 124 C117 120,117 118,117 116 L116 116 C114 120,112 123,110 124 C109 126,108 127,107 128 C107 129,106 129,105 130 C104 130,103 130,102 130 C101 130,100 130,99 130 L101 122 L103 122 C103 123,104 124,104 124 C105 124,105 124,106 124 C106 124,106 123,107 122 C108 122,109 121,110 119 C111 118,113 115,115 112 L115 112 Z" fill="#212121"/><path d="M149 117 L147 123 L142 123 L143 117 L149 117 Z M142 145 C142 146,142 147,142 148 C142 148,142 149,142 149 C142 150,142 150,142 151 C142 151,143 151,143 151 C144 151,144 151,144 151 C145 151,145 151,145 150 C146 150,146 150,146 149 C147 149,147 148,148 148 L150 150 C149 151,148 152,147 152 C146 153,146 153,145 154 C144 154,144 155,143 155 C142 155,142 155,141 155 C140 155,140 155,139 155 C139 155,138 154,138 154 C137 153,137 153,137 152 C137 152,136 151,136 150 C136 150,137 149,137 148 C137 147,137 146,137 145 C138 144,138 143,138 142 C138 141,139 140,139 139 C139 138,139 137,139 136 C140 135,140 134,140 133 C140 133,140 133,140 132 C140 132,140 132,140 132 C140 131,140 131,139 131 C139 130,138 130,137 130 L137 128 L145 128 L146 128 L142 145 Z" fill="#212121"/><path d="M164 105 C165 103,165 101,165 100 C165 99,165 98,164 98 C164 97,164 97,163 97 C162 97,161 97,160 98 C160 99,158 100,157 101 L155 99 C157 97,159 95,161 94 C162 94,164 93,166 93 C167 93,168 94,169 95 C170 96,171 97,171 98 C171 100,170 101,170 103 L170 103 C172 99,175 97,177 95 C179 94,181 93,184 93 C186 93,188 94,189 95 C190 96,191 98,191 101 C191 102,190 105,190 107 L187 117 C187 120,186 122,186 123 C186 124,186 125,187 125 C187 126,188 126,188 126 C189 126,190 126,191 125 C191 125,193 124,194 122 L196 124 C194 126,192 128,191 129 C189 130,187 130,185 130 C184 130,183 129,182 128 C181 127,180 126,180 124 C180 122,181 120,181 117 L183 110 C184 108,184 106,184 105 C184 104,184 103,184 102 C184 100,184 99,184 98 C183 97,182 97,181 97 C180 97,179 98,177 98 C176 99,175 100,174 102 C172 104,171 105,170 107 C170 109,169 111,168 113 L165 129 L159 129 L164 105 Z" fill="#212121"/><path d="M209 117 L207 123 L202 123 L203 117 L209 117 Z M202 145 C202 146,202 147,202 148 C202 148,202 149,202 149 C202 150,202 150,202 151 C202 151,203 151,203 151 C204 151,204 151,204 151 C205 151,205 151,205 150 C206 150,206 150,206 149 C207 149,207 148,208 148 L210 150 C209 151,208 152,207 152 C206 153,206 153,205 154 C204 154,204 155,203 155 C202 155,202 155,201 155 C200 155,200 155,199 155 C199 155,198 154,198 154 C197 153,197 153,197 152 C197 152,196 151,196 150 C196 150,197 149,197 148 C197 147,197 146,197 145 C198 144,198 143,198 142 C198 141,199 140,199 139 C199 138,199 137,199 136 C200 135,200 134,200 133 C200 133,200 133,200 132 C200 132,200 132,200 132 C200 131,200 131,199 131 C199 130,198 130,197 130 L197 128 L205 128 L206 128 L202 145 Z" fill="#212121"/><path d="M218 12 C228 21,236 33,241 50 C247 68,250 87,250 107 C250 128,247 147,241 164 C236 182,228 195,218 203 L216 200 C225 192,231 179,236 162 C241 145,243 127,243 107 C243 87,241 69,236 53 C231 36,225 24,216 15 L218 12 Z" fill="#212121"/><path d="M268 0 L268 2 L259 24 L256 23 L261 0 L268 0 Z" fill="#212121"/><path d="M299 103 L299 98 L345 98 L345 103 L299 103 Z M299 118 L299 113 L345 113 L345 118 L299 118 Z" fill="#212121"/><path d="M397 62 C397 64,397 65,397 65 C397 66,398 67,398 67 C398 68,399 68,400 68 C401 69,401 69,403 69 C404 69,405 69,407 69 L407 72 L380 72 L380 69 C383 69,385 69,386 69 C387 68,388 68,388 68 C389 67,389 67,390 66 C390 65,390 64,390 62 L390 32 C390 31,390 31,390 30 C389 30,389 30,388 30 C387 30,386 30,385 31 C383 32,382 33,380 34 L378 31 L395 21 L397 21 C397 23,397 27,397 31 L397 62 Z" fill="#212121"/><path d="M397 171 C395 174,393 176,391 178 C389 179,386 180,384 180 C382 180,380 179,378 177 C377 175,376 172,376 168 C376 164,377 160,378 156 C380 152,382 149,385 147 C388 144,391 143,395 143 C396 143,398 143,399 144 C400 144,402 145,403 146 L407 143 L409 144 L400 186 C399 188,399 189,399 190 C399 191,399 192,400 192 C400 193,401 193,403 193 L403 195 L391 195 L398 172 L397 171 Z M400 157 C400 155,401 153,401 152 C401 150,400 148,399 147 C399 147,397 146,395 146 C393 146,391 147,389 149 C387 151,385 154,384 158 C383 161,382 165,382 168 C382 171,383 173,383 174 C384 175,385 176,387 176 C388 176,388 176,389 176 C390 175,391 175,392 174 C393 173,393 172,394 171 C395 170,396 169,396 168 C397 167,398 166,398 164 C399 162,399 161,400 158 L400 157 Z" fill="#212121"/><rect x="371" y="105" width="42" height="5" fill="#212121"/><path d="M443 112 C443 110,443 109,442 107 C442 105,442 103,441 102 C441 100,440 99,440 98 C440 98,440 98,439 97 C439 97,439 97,438 97 C438 97,437 97,437 97 C436 98,436 98,435 99 C435 99,434 100,433 101 L431 99 C433 97,434 96,435 95 C437 94,439 93,440 93 C441 93,442 93,442 93 C443 94,444 94,444 94 C444 95,445 95,445 96 C446 96,446 97,446 98 C447 99,447 100,447 102 C447 103,448 105,448 106 L448 106 C451 103,452 100,453 99 C455 97,456 96,457 95 C457 94,458 94,459 94 C460 93,461 93,462 93 C463 93,464 93,465 94 L463 101 L461 101 C461 100,460 99,460 99 C459 99,459 99,459 99 C459 99,458 99,458 100 C458 100,457 100,456 101 C456 102,455 103,454 104 C453 105,452 106,451 107 L449 110 C449 113,450 115,450 116 C451 118,451 120,451 121 C452 122,452 123,452 124 C452 124,453 125,453 125 C453 126,453 126,454 126 C454 126,454 126,455 126 C455 126,456 126,456 125 C457 125,458 124,459 122 L462 124 C460 126,458 128,457 129 C456 130,454 130,452 130 C451 130,450 130,449 129 C449 129,448 128,447 128 C447 127,446 125,446 124 C445 120,445 118,445 116 L444 116 C442 120,440 123,438 124 C437 126,436 127,435 128 C435 129,434 129,433 130 C432 130,431 130,430 130 C429 130,428 130,427 130 L429 122 L431 122 C431 123,432 124,432 124 C433 124,433 124,434 124 C434 124,434 123,435 122 C436 122,437 121,438 119 C439 118,441 115,443 112 L443 112 Z" fill="#212121"/><path d="M477 117 L475 123 L470 123 L471 117 L477 117 Z M470 145 C470 146,470 147,470 148 C470 148,470 149,470 149 C470 150,470 150,470 151 C470 151,471 151,471 151 C472 151,472 151,472 151 C473 151,473 151,473 150 C474 150,474 150,474 149 C475 149,475 148,476 148 L478 150 C477 151,476 152,475 152 C474 153,474 153,473 154 C472 154,472 155,471 155 C470 155,470 155,469 155 C468 155,468 155,467 155 C467 155,466 154,466 154 C465 153,465 153,465 152 C465 152,464 151,464 150 C464 150,465 149,465 148 C465 147,465 146,465 145 C466 144,466 143,466 142 C466 141,467 140,467 139 C467 138,467 137,467 136 C468 135,468 134,468 133 C468 133,468 133,468 132 C468 132,468 132,468 132 C468 131,468 131,467 131 C467 130,466 130,465 130 L465 128 L473 128 L474 128 L470 145 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">During the operation of the neural network, the masking vector is filled with ones, allowing values to be transmitted in both directions without hindrance.</span></p>
<p class="p_Text"><span class="f_Text">In practice, the coefficient of </span><span class="f_Text" style="font-style: italic;">1/q</span><span class="f_Text"> is constant throughout the training, so we can easily calculate this coefficient once and write it instead of one in the masking tensor. This way, we combine the coefficient recalculation and multiplication by 1 in each training iteration.</span></p>

</div>

</body>
</html>
