<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>5.2.2.2 Multi-Head Self-Attention backpropagation methods</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="5_transformer.htm"> 5. Attention mechanisms </a> / <a class="h_m" href="5_2_mh_attention.htm"> 5.2 Multi-Head attention </a> / <a class="h_m" href="5_2_2_mh_attention_mql.htm"> 5.2.2 Building Multi-Head Self-Attention in MQL5 </a>/ 5.2.2.2 Multi-Head Self-Attention backpropagation methods
          </td>
          <td width="70" align="right">
          <a href="5_2_2_1_mh_attention_feedforward.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="5_2_2_3_mh_attention_save_load.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H3"><span class="f_H3">5.2.2.2 Multi-Head Self-Attention backpropagation methods</span></p>
<p class="p_Text"><span class="f_Text">We are confidently moving forward in our learning path. Let's proceed with the implementation of our </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> class. In the previous sections, we have already implemented initialization methods and feed-forward methods. However, the neural layer training algorithm is based on the error gradient backpropagation algorithm. We now proceed to implement backpropagation methods.</span></p>
<p class="p_Text"><span class="f_Text">We have already mentioned that the Multi-Head Self-Attention algorithm is a logical extension of </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text">. That's why we created our class based on the </span><span class="f_Text" style="font-style: italic;">CNeuronAttention</span><span class="f_Text"> class. And yes, the processes are all very similar. However, there are still some minor differences in the implementation of multi-head attention. To implement these differences, we created a new class </span><span class="f_Text" style="font-style: italic;">CNeuronMHAttention</span><span class="f_Text">.</span></p>
<p class="p_Text"><span class="f_Text">As we progress in creating the methods of the class, let's take a look at the implementation of these differences in the methods of the backpropagation algorithm.</span></p>
<p class="p_Text"><span class="f_Text">In the parent class, we have overridden three virtual methods to implement the backpropagation algorithm:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li" style="font-style: italic;">CNeuronAttention::CalcHiddenGradient</span><span class="f_li"> – method for calculating the error gradient through the hidden layer</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">CNeuronAttention::CalcDeltaWeights</span><span class="f_li"> – method for calculating the error gradient to the level of the weights matrix</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">CNeuronAttention::UpdateWeights </span><span class="f_li"> – method for updating the weights</span></li>
</ul>
<p class="p_Text"><span class="f_Text">So, we will also need to override the corresponding methods to organize the multi-head attention backpropagation pass. Let's start with the method of distributing the error gradient through the hidden layer of the </span><span class="f_Text" style="font-style: italic;">CalcHiddenGradient</span><span class="f_Text"> neural network. </span></p>
<p class="p_Text"><span class="f_Text">As in the parent class method, in the parameters of the method, we receive a pointer to the object of the previous neural layer. It is in its error gradient buffer that we are going to record the result of the work being done.</span></p>
<p class="p_Text"><span class="f_Text">At the beginning of the </span><span class="f_Text" style="font-style: italic;">CNeuronMHAttention::CalcHiddenGradient</span><span class="f_Text"> method body, there is the customary and essential attribute of any method: a check of pointers to the objects used in the method. Here, as in the similar method of the parent class, we will perform control checks only for pointers to objects that will be directly accessed from this method without using the methods of internal neural layers. The reason is that all inner neural layer methods have a similar block of controls. By calling them, we again validate the passed pointers to objects. This is an additional cost in resources and time. We can't disable the checks in the methods of the nested neural layers, so we will eliminate explicit duplication of controls in the current method.</span></p>
<p class="p_Text"><span class="f_Text">We should immediately point out that we only exclude </span><span class="f_Text" style="font-style: italic;">explicit duplication</span><span class="f_Text">, but not </span><span class="f_Text" style="font-style: italic;">possible</span><span class="f_Text">. It's a fine line, but behind it lie great risks.</span></p>
<p class="p_Text"><span class="f_Text" style="font-style: italic;">Explicit</span><span class="f_Text"> is the duplication that will happen anyway. If we see such duplication, we try to keep only one control point before the first use of the object whenever possible.</span></p>
<p class="p_Text"><span class="f_Text">Note, that there must be at least one control point before the object is accessed for the first time.</span></p>
<p class="p_Text"><span class="f_Text">I call duplication </span><span class="f_Text" style="font-style: italic;">possible</span><span class="f_Text"> when it can occur under certain circumstances. In some cases, it may not happen. We do not eliminate such duplication because the risk of a critical error in the absence of control outweighs the potential benefits of improving program performance.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronMHAttention</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;check&nbsp;the&nbsp;relevance&nbsp;of&nbsp;all&nbsp;objects</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Total</span><span class="f_CodeExample">()&nbsp;!=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Total</span><span class="f_CodeExample">())</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After successfully passing the control block, we proceed directly to the error gradient distribution procedure. As you may recall, in the feed-forward pass, the data is normalized at the output of the neural layer. Also, we need to adjust the error gradient by the derivative of the normalization function. In the parent class, we derived this procedure in a separate method entitled </span><span class="f_Text" style="font-style: italic;">CNeuronAttention::NormlizeBufferGradient</span><span class="f_Text">. Now we just need to call it with the appropriate parameters.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;scale&nbsp;the&nbsp;gradient&nbsp;to&nbsp;normalization</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">NormlizeBufferGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cStd</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">1</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, we run the error gradient through the inner neural layers of the </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> block. These are the two convolutional layers: </span><span class="f_Text" style="font-style: italic;">m_cFF2</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">m_cFF1</span><span class="f_Text">. To propagate the gradient through these neural layers, we sequentially call the analogous methods of the mentioned neural layers. Don't forget to check the results of the operations.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;propagate&nbsp;the&nbsp;error&nbsp;gradient&nbsp;through&nbsp;the&nbsp;Feed&nbsp;Forward&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">)))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">)))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After passing the error gradient via the </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> block, we recall that before normalizing the data at the output of the neural layer, we added up the tensors of the results of the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> blocks. Hence, we must also propagate the error gradient along both directions. For this purpose, after obtaining the error gradient from the </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> block in the buffer of the inner neural layer </span><span class="f_Text" style="font-style: italic;">m_cW0</span><span class="f_Text">, we add up the two tensors.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">SumArray</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Let's adjust it for the derivative of the data normalization process.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;adjust&nbsp;the&nbsp;gradient&nbsp;for&nbsp;normalization</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">NormlizeBufferGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">(),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">(),</span>
<br><span class="f_Functions">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cStd</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">0</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We continue utilizing internal neural layer methods. We will call the convolution layer gradient distribution method </span><span class="f_Text" style="font-style: italic;">m_cW0</span><span class="f_Text"> and check the result of the operations.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;distribution&nbsp;of&nbsp;the&nbsp;error&nbsp;gradient&nbsp;by&nbsp;attention&nbsp;heads</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">)))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, we need to propagate the error gradient from the concatenated result of the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> block to the internal neural layers </span><span class="f_Text" style="font-style: italic;">m_cQuerys, m_cKeys, </span><span class="f_Text">and </span><span class="f_Text" style="font-style: italic;">m_cValues</span><span class="f_Text">. As you may recall, in the feed-forward pass, the path to </span><span class="f_Text" style="font-style: italic;">m_cAttentionOut</span><span class="f_Text"> from the specified inner neural layers was completely recreated inside the method. Similarly, we will have to recreate the progression of the reverse signal.</span></p>
<p class="p_Text"><span class="f_Text">Since we are creating a new thread of operations, according to our concept, it is necessary to organize two parallel threads of operations: using standard MQL5 tools and in the paradigm of multi-threaded operations using OpenCL.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;branching&nbsp;of&nbsp;the&nbsp;algorithm&nbsp;by&nbsp;computing&nbsp;device</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">gradients</span><span class="f_CodeExample">[];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys</span><span class="f_CodeExample">[],&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">Zeros</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">[],&nbsp;</span><span class="f_CodeExample" style="color: #333333;">keys_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">Zeros</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">values</span><span class="f_CodeExample">[],&nbsp;</span><span class="f_CodeExample" style="color: #333333;">values_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">Zeros</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">attention_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">As always, in this section, we will consider the implementation using MQL5. We will proceed to the organization of multi-threaded operations later.</span></p>
<p class="p_Text"><span class="f_Text">So, first, we're going to do some preparatory work. As in the forward pass, in this block, we organize the work separately for individual attention heads. As all the data is stored in concatenated buffers, we will prepare local matrices and split the buffers into individual matrices according to the attention heads.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Vsplit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Vsplit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Vsplit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">values</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">attention_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">attention_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Vsplit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">gradients</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, we will create a loop with the number of iterations equal to the number of attention heads used.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">During the feed-forward pass, the values of the concatenated buffer of results are assembled by multiplying the values of the </span><span class="f_Text" style="font-style: italic;">m_cValues</span><span class="f_Text"> neural layer's tensor results with the corresponding elements of the dependency coefficient matrix, followed by vector addition. Now we need to organize the reverse process: propagating the error gradient along these two directions.</span></p>
<p class="p_Text"><span class="f_Text">First, we transfer the error gradient to the inner neural layer </span><span class="f_Text" style="font-style: italic;">m_cValues</span><span class="f_Text">. Before that, let's do some preparatory work.</span></p>
<p class="p_Text"><span class="f_Text">To propagate the gradient to the </span><span class="f_Text" style="font-style: italic;">m_cValues</span><span class="f_Text"> neural layer, it is necessary to multiply the error gradient matrix by the dependency coefficient matrix. Hence, we first need to extract such a matrix for the attention head we analyze.</span></p>
<p class="p_Text"><span class="f_Text">We then multiply the matrices and add the result to a local copy of the concatenated gradient matrix of the </span><span class="f_Text" style="font-style: italic;">m_cValues</span><span class="f_Text"> layer.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;gradient&nbsp;propagation&nbsp;to&nbsp;Values</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">Zeros</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cScores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">),&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">gradients</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">])).</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">values_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After that, we will propagate the gradient along the second path of the algorithm, through the dependency coefficient matrix to the neural layers </span><span class="f_Text" style="font-style: italic;">m_cQuerys</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">m_cKeys</span><span class="f_Text">. In essence, we first need to determine the error gradient at the level of the dependency coefficient matrix and then propagate the error gradient from there to the specified internal neural layers.</span></p>
<p class="p_Text"><span class="f_Text">Here we should recall that the dependency coefficient matrix is normalized by the </span><span class="f_Text" style="font-style: italic;">Softmax</span><span class="f_Text"> function in the </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text"> section. To properly adjust the error gradient for the derivative of the </span><span class="f_Text" style="font-style: italic;">Softmax </span><span class="f_Text">function, we need at least the full vector of error gradients for the values involved in a single normalization operation. We can write it into a local matrix.</span></p>
<p class="p_Text"><span class="f_Text">The task is clear, and we can proceed to implementation. To propagate the error gradient to the dependency coefficient matrix, it is sufficient to multiply the obtained gradient by the matrix of the results from the last feed-forward pass of the </span><span class="f_Text" style="font-style: italic;">m_cValues</span><span class="f_Text"> neural layer.</span></p>
<p class="p_Text"><span class="f_Text">After obtaining the error gradient vector at the dependency coefficient matrix level, we should adjust it using the derivative of the </span><span class="f_Text" style="font-style: italic;">Softmax</span><span class="f_Text"> function.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:286px;height:49px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 1144 196"><path d="M6 108 C6 111,7 113,8 115 C10 116,12 117,15 117 C18 117,21 116,23 114 C24 112,25 110,25 106 C25 105,25 103,25 102 C24 101,23 100,22 99 C21 98,20 97,18 95 C16 94,14 93,13 91 C12 90,11 89,10 88 C10 86,9 84,9 83 C9 80,10 77,11 75 C13 73,15 71,17 70 C20 69,23 68,26 68 C28 68,30 68,32 69 C34 69,36 69,39 70 L37 79 L33 79 C33 77,33 76,32 74 C32 73,31 72,30 72 C29 71,27 71,26 71 C24 71,22 72,20 72 C19 73,18 74,17 76 C16 77,16 79,16 81 C16 82,16 84,17 85 C18 87,20 89,23 90 C25 92,27 94,28 95 C29 96,30 98,31 99 C32 101,32 103,32 104 C32 108,31 110,30 113 C28 115,26 117,24 118 C21 119,18 120,15 120 C13 120,10 120,8 120 C5 119,3 119,1 118 L3 108 L6 108 Z" fill="#212121"/><path d="M56 120 C52 120,49 119,47 117 C45 115,44 112,44 107 C44 106,44 103,45 101 C46 97,47 94,49 91 C51 89,53 87,55 85 C58 84,61 83,64 83 C68 83,71 84,73 86 C75 89,76 92,76 96 C76 99,75 102,74 105 C73 108,72 111,70 113 C69 116,67 117,64 118 C62 120,59 120,56 120 L56 120 Z M50 109 C50 112,51 114,52 115 C53 116,55 117,57 117 C60 117,62 116,64 114 C65 112,67 109,68 105 C69 101,70 97,70 94 C70 92,69 90,68 88 C67 87,65 86,63 86 C61 86,58 87,57 89 C55 92,53 95,52 99 C51 103,50 106,50 109 L50 109 Z" fill="#212121"/><path d="M96 120 C95 125,93 129,91 131 C88 134,85 135,82 135 C81 135,80 135,79 135 L80 132 C80 132,81 132,82 132 C83 132,84 132,85 131 C85 131,86 130,87 128 C87 127,88 125,89 123 L97 88 L90 88 L91 86 C93 86,94 86,94 86 C95 86,96 85,96 85 C97 85,97 84,97 84 C97 83,98 82,98 80 C100 76,102 73,104 70 C107 68,110 67,115 67 C117 67,119 67,121 68 L119 74 L116 74 C116 73,115 72,115 71 C114 70,113 70,112 70 C111 70,110 70,109 71 C108 72,107 73,106 74 C106 76,105 78,104 80 L103 84 L113 84 L112 88 L103 88 L96 120 Z" fill="#212121"/><path d="M146 112 C144 115,142 117,140 118 C138 119,136 120,134 120 C129 120,127 118,127 112 C127 111,127 109,127 107 L131 88 L125 88 L126 86 C127 86,128 86,129 86 C130 86,130 85,131 85 C131 85,132 84,132 83 C133 83,133 82,134 81 C134 80,135 78,135 75 L140 75 L138 84 L149 84 L149 88 L138 88 L134 102 C134 105,133 107,133 109 C133 110,133 111,133 111 C133 115,134 116,137 116 C138 116,139 116,140 115 C141 114,143 112,144 110 L146 112 Z" fill="#212121"/><path d="M167 93 C169 89,172 87,174 85 C176 84,178 83,181 83 C183 83,185 84,186 85 C187 86,188 88,188 91 L188 91 C188 91,188 91,188 92 C190 89,192 87,194 85 C196 84,198 83,200 83 C203 83,204 84,206 85 C207 86,208 88,208 91 C208 92,207 95,206 97 L204 107 C203 110,203 112,203 113 C203 114,203 115,204 115 C204 116,204 116,205 116 C206 116,207 116,207 115 C208 115,209 114,211 112 L213 114 C211 116,209 118,208 119 C206 120,204 120,202 120 C201 120,199 119,198 118 C197 117,197 116,197 114 C197 112,197 110,198 107 L200 100 C200 98,201 96,201 95 C201 94,201 93,201 92 C201 90,201 89,200 88 C200 87,199 87,198 87 C197 87,195 88,194 88 C193 89,192 90,191 92 C189 94,188 95,187 97 C187 98,186 100,185 103 L182 119 L176 119 L180 100 C181 98,181 96,181 95 C181 94,181 93,181 92 C181 90,181 89,180 88 C180 87,179 87,177 87 C177 87,176 88,174 88 C173 89,172 90,171 92 C169 94,168 95,167 97 C167 99,166 101,165 103 L162 119 L156 119 L161 95 C162 93,162 91,162 90 C162 89,162 88,161 88 C161 87,161 87,160 87 C159 87,158 87,157 88 C157 89,155 90,154 91 L152 89 C154 87,156 86,157 85 C159 84,161 83,163 83 C164 83,165 84,166 85 C167 86,168 87,168 88 C168 90,167 91,167 93 L167 93 Z" fill="#212121"/><path d="M245 86 L249 83 L251 84 L246 108 C245 110,245 112,245 113 C245 114,245 115,245 115 C246 116,246 116,247 116 C248 116,249 116,249 115 C250 115,251 114,253 112 L255 114 C253 116,251 118,249 119 C248 120,246 120,244 120 C243 120,242 120,241 119 C240 118,239 116,239 115 C239 114,240 112,240 111 L240 110 C237 114,235 116,233 118 C231 119,229 120,226 120 C224 120,222 119,220 117 C219 115,218 112,218 108 C218 104,219 100,220 96 C222 92,224 89,227 87 C230 84,233 83,237 83 C238 83,240 83,241 84 C243 84,244 85,245 86 L245 86 Z M242 97 C242 96,243 95,243 94 C243 93,243 93,243 92 C243 90,242 88,242 87 C241 87,239 86,237 86 C235 86,233 87,231 89 C229 91,227 94,226 98 C225 101,224 105,224 108 C224 111,225 113,225 114 C226 115,227 116,229 116 C231 116,232 115,234 114 C235 113,237 110,238 108 C240 105,241 102,242 98 L242 97 Z" fill="#212121"/><path d="M273 102 C273 100,273 99,272 97 C272 95,272 93,271 92 C271 90,270 89,270 88 C270 88,270 88,269 87 C269 87,269 87,268 87 C268 87,267 87,267 87 C266 88,266 88,265 89 C265 89,264 90,263 91 L261 89 C263 87,264 86,265 85 C267 84,269 83,270 83 C271 83,272 83,272 83 C273 84,274 84,274 84 C274 85,275 85,275 86 C276 86,276 87,276 88 C277 89,277 90,277 92 C277 93,278 95,278 96 L278 96 C281 93,282 90,283 89 C285 87,286 86,287 85 C287 84,288 84,289 84 C290 83,291 83,292 83 C293 83,294 83,295 84 L293 91 L291 91 C291 90,290 89,290 89 C289 89,289 89,289 89 C289 89,288 89,288 90 C288 90,287 90,286 91 C286 92,285 93,284 94 C283 95,282 96,281 97 L279 100 C279 103,280 105,280 106 C281 108,281 110,281 111 C282 112,282 113,282 114 C282 114,283 115,283 115 C283 116,283 116,284 116 C284 116,284 116,285 116 C285 116,286 116,286 115 C287 115,288 114,289 112 L292 114 C290 116,288 118,287 119 C286 120,284 120,282 120 C281 120,280 120,279 119 C279 119,278 118,277 118 C277 117,276 115,276 114 C275 110,275 108,275 106 L274 106 C272 110,270 113,268 114 C267 116,266 117,265 118 C265 119,264 119,263 120 C262 120,261 120,260 120 C259 120,258 120,257 120 L259 112 L261 112 C261 113,262 114,262 114 C263 114,263 114,264 114 C264 114,264 113,265 112 C266 112,267 111,268 109 C269 108,271 105,273 102 L273 102 Z" fill="#212121"/><path d="M311 101 C311 109,312 117,315 122 C318 128,322 131,328 133 L327 136 C320 134,314 130,311 124 C307 118,305 110,305 101 C305 92,307 85,311 78 C314 72,320 68,327 66 L328 69 C322 71,318 75,315 80 C312 86,311 92,311 101 L311 101 Z" fill="#212121"/><path d="M346 102 C346 100,346 99,345 97 C345 95,345 93,344 92 C344 90,343 89,343 88 C343 88,343 88,342 87 C342 87,342 87,341 87 C341 87,340 87,340 87 C339 88,339 88,338 89 C338 89,337 90,336 91 L334 89 C336 87,337 86,338 85 C340 84,342 83,343 83 C344 83,345 83,345 83 C346 84,347 84,347 84 C347 85,348 85,348 86 C349 86,349 87,349 88 C350 89,350 90,350 92 C350 93,351 95,351 96 L351 96 C354 93,355 90,356 89 C358 87,359 86,360 85 C360 84,361 84,362 84 C363 83,364 83,365 83 C366 83,367 83,368 84 L366 91 L364 91 C364 90,363 89,363 89 C362 89,362 89,362 89 C362 89,361 89,361 90 C361 90,360 90,359 91 C359 92,358 93,357 94 C356 95,355 96,354 97 L352 100 C352 103,353 105,353 106 C354 108,354 110,354 111 C355 112,355 113,355 114 C355 114,356 115,356 115 C356 116,356 116,357 116 C357 116,357 116,358 116 C358 116,359 116,359 115 C360 115,361 114,362 112 L365 114 C363 116,361 118,360 119 C359 120,357 120,355 120 C354 120,353 120,352 119 C352 119,351 118,350 118 C350 117,349 115,349 114 C348 110,348 108,348 106 L347 106 C345 110,343 113,341 114 C340 116,339 117,338 118 C338 119,337 119,336 120 C335 120,334 120,333 120 C332 120,331 120,330 120 L332 112 L334 112 C334 113,335 114,335 114 C336 114,336 114,337 114 C337 114,337 113,338 112 C339 112,340 111,341 109 C342 108,344 105,346 102 L346 102 Z" fill="#212121"/><path d="M380 107 L378 113 L373 113 L374 107 L380 107 Z M373 135 C373 136,373 137,373 138 C373 138,373 139,373 139 C373 140,373 140,373 141 C373 141,374 141,374 141 C375 141,375 141,375 141 C376 141,376 141,376 140 C377 140,377 140,377 139 C378 139,378 138,379 138 L381 140 C380 141,379 142,378 142 C377 143,377 143,376 144 C375 144,375 145,374 145 C373 145,373 145,372 145 C371 145,371 145,370 145 C370 145,369 144,369 144 C368 143,368 143,368 142 C368 142,367 141,367 140 C367 140,368 139,368 138 C368 137,368 136,368 135 C369 134,369 133,369 132 C369 131,370 130,370 129 C370 128,370 127,370 126 C371 125,371 124,371 123 C371 123,371 123,371 122 C371 122,371 122,371 122 C371 121,371 121,370 121 C370 120,369 120,368 120 L368 118 L376 118 L377 118 L373 135 Z" fill="#212121"/><path d="M403 101 C403 92,402 86,399 80 C396 75,392 71,386 69 L387 66 C394 68,400 72,403 78 C407 85,409 92,409 101 C409 110,407 118,403 124 C400 130,394 134,387 136 L386 133 C392 131,396 128,399 122 C402 117,403 109,403 101 L403 101 Z" fill="#212121"/><path d="M432 62 L432 64 L423 85 L420 84 L425 62 L432 62 Z" fill="#212121"/><path d="M461 93 L461 88 L507 88 L507 93 L461 93 Z M461 108 L461 103 L507 103 L507 108 L461 108 Z" fill="#212121"/><path d="M537 95 C540 95,543 93,545 90 C547 87,548 83,548 77 C548 74,547 69,546 62 C545 54,544 48,544 43 C544 30,546 21,551 14 C555 7,561 3,568 2 L568 5 C566 6,563 7,561 8 C559 10,557 12,556 15 C555 18,553 21,553 25 C552 29,551 34,551 39 C551 43,552 49,553 57 C554 65,554 71,554 73 C554 80,553 85,551 89 C549 93,546 95,544 96 L544 97 C546 98,549 100,551 104 C553 108,554 114,554 120 C554 122,554 127,553 136 C552 146,551 152,551 154 C551 159,552 164,553 168 C553 172,554 175,556 178 C557 181,559 183,561 185 C563 187,565 188,568 188 L568 191 C561 191,555 187,551 180 C547 173,544 164,544 152 C544 148,545 141,546 133 C547 124,548 118,548 116 C548 110,547 106,545 103 C543 100,540 98,537 98 L537 95 Z" fill="#212121"/><path d="M591 3 L589 10 L583 10 L584 3 L591 3 Z M579 29 C579 26,580 24,580 23 C580 22,579 21,579 21 C578 20,577 20,576 20 L576 18 L585 17 L587 17 L582 42 C582 44,581 46,581 47 C581 48,581 49,582 49 C582 50,583 50,583 50 C584 50,585 50,586 49 C586 49,588 48,589 46 L591 48 C589 50,587 52,585 53 C584 54,582 54,580 54 C579 54,578 53,577 52 C576 51,575 50,575 48 C575 46,576 44,576 41 L579 29 Z" fill="#212121"/><path d="M622 27 L622 22 L668 22 L668 27 L622 27 Z M622 42 L622 37 L668 37 L668 42 L622 42 Z" fill="#212121"/><path d="M718 3 L717 10 L710 10 L712 3 L718 3 Z M707 53 C707 57,706 60,704 62 C703 65,701 66,699 67 C697 68,694 69,691 69 C690 69,689 69,688 69 L689 66 C690 66,691 66,692 66 C693 66,694 66,695 66 C696 65,696 65,697 64 C698 63,698 62,699 60 C700 59,700 57,701 55 L707 28 C707 26,707 24,707 23 C707 22,707 21,707 21 C706 20,705 20,704 20 L704 18 L713 17 L715 17 L707 53 Z" fill="#212121"/><path d="M733 44 C734 46,734 48,734 49 C734 51,734 53,733 54 C733 55,732 57,730 58 C729 60,727 61,725 63 L723 61 C724 60,725 59,726 58 C726 57,726 56,727 54 C727 53,727 52,727 50 C727 48,727 46,727 44 L733 44 Z" fill="#212121"/><path d="M843 53 C842 49,841 44,840 38 C839 31,838 27,837 25 C837 23,836 22,836 22 C836 21,835 21,835 21 C834 21,833 21,832 22 C832 23,831 24,830 25 L827 23 C829 21,831 20,832 19 C834 18,835 17,837 17 C838 17,839 17,840 17 C840 18,841 18,841 19 C842 19,842 20,842 20 C843 21,843 22,843 23 C844 24,844 26,844 28 C845 31,846 34,846 38 C847 41,847 45,847 48 C850 44,852 41,854 38 C855 34,856 32,857 29 C858 27,858 25,858 23 C858 22,858 21,857 21 C857 20,856 20,855 20 L855 18 L865 18 L866 20 C864 25,862 30,858 36 C855 42,852 48,848 53 C844 58,841 62,838 65 C836 66,834 68,833 68 C832 69,830 69,829 69 C828 69,828 69,827 69 C826 69,826 69,825 69 L827 62 L830 62 C830 63,830 64,831 64 C833 64,834 63,836 61 C838 60,840 57,843 53 L843 53 Z" fill="#212121"/><path d="M872 43 L870 49 L865 49 L866 43 L872 43 Z M865 71 C865 72,865 73,865 74 C865 74,865 75,865 75 C865 76,865 76,865 77 C865 77,866 77,866 77 C867 77,867 77,867 77 C868 77,868 77,868 76 C869 76,869 76,869 75 C870 75,870 74,871 74 L873 76 C872 77,871 78,870 78 C869 79,869 79,868 80 C867 80,867 81,866 81 C865 81,865 81,864 81 C863 81,863 81,862 81 C862 81,861 80,861 80 C860 79,860 79,860 78 C860 78,859 77,859 76 C859 76,860 75,860 74 C860 73,860 72,860 71 C861 70,861 69,861 68 C861 67,862 66,862 65 C862 64,862 63,862 62 C863 61,863 60,863 59 C863 59,863 59,863 58 C863 58,863 58,863 58 C863 57,863 57,862 57 C862 56,861 56,860 56 L860 54 L868 54 L869 54 L865 71 Z" fill="#212121"/><path d="M890 35 C890 43,891 51,894 56 C897 62,901 65,907 67 L906 70 C899 68,893 64,890 58 C886 52,884 44,884 35 C884 26,886 19,890 12 C893 6,899 2,906 0 L907 3 C901 5,897 9,894 14 C891 20,890 26,890 35 L890 35 Z" fill="#212121"/><path d="M934 43 C934 45,934 46,934 46 C934 47,935 48,935 48 C935 49,936 49,937 49 C938 50,938 50,940 50 C941 50,942 50,944 50 L944 53 L917 53 L917 50 C920 50,922 50,923 50 C924 49,925 49,925 49 C926 48,926 48,927 47 C927 46,927 45,927 43 L927 13 C927 12,927 12,927 11 C926 11,926 11,925 11 C924 11,923 11,922 12 C920 13,919 14,917 15 L915 12 L932 2 L934 2 C934 4,934 8,934 12 L934 43 Z" fill="#212121"/><path d="M972 34 L972 29 L1018 29 L1018 34 L972 34 Z" fill="#212121"/><path d="M1058 53 C1057 49,1056 44,1055 38 C1054 31,1053 27,1052 25 C1052 23,1051 22,1051 22 C1051 21,1050 21,1050 21 C1049 21,1048 21,1047 22 C1047 23,1046 24,1045 25 L1042 23 C1044 21,1046 20,1047 19 C1049 18,1050 17,1052 17 C1053 17,1054 17,1055 17 C1055 18,1056 18,1056 19 C1057 19,1057 20,1057 20 C1058 21,1058 22,1058 23 C1059 24,1059 26,1059 28 C1060 31,1061 34,1061 38 C1062 41,1062 45,1062 48 C1065 44,1067 41,1069 38 C1070 34,1071 32,1072 29 C1073 27,1073 25,1073 23 C1073 22,1073 21,1072 21 C1072 20,1071 20,1070 20 L1070 18 L1080 18 L1081 20 C1079 25,1077 30,1073 36 C1070 42,1067 48,1063 53 C1059 58,1056 62,1053 65 C1051 66,1049 68,1048 68 C1047 69,1045 69,1044 69 C1043 69,1043 69,1042 69 C1041 69,1041 69,1040 69 L1042 62 L1045 62 C1045 63,1045 64,1046 64 C1048 64,1049 63,1051 61 C1053 60,1055 57,1058 53 L1058 53 Z" fill="#212121"/><path d="M1087 43 L1085 49 L1080 49 L1081 43 L1087 43 Z M1080 71 C1080 72,1080 73,1080 74 C1080 74,1080 75,1080 75 C1080 76,1080 76,1080 77 C1080 77,1081 77,1081 77 C1082 77,1082 77,1082 77 C1083 77,1083 77,1083 76 C1084 76,1084 76,1084 75 C1085 75,1085 74,1086 74 L1088 76 C1087 77,1086 78,1085 78 C1084 79,1084 79,1083 80 C1082 80,1082 81,1081 81 C1080 81,1080 81,1079 81 C1078 81,1078 81,1077 81 C1077 81,1076 80,1076 80 C1075 79,1075 79,1075 78 C1075 78,1074 77,1074 76 C1074 76,1075 75,1075 74 C1075 73,1075 72,1075 71 C1076 70,1076 69,1076 68 C1076 67,1077 66,1077 65 C1077 64,1077 63,1077 62 C1078 61,1078 60,1078 59 C1078 59,1078 59,1078 58 C1078 58,1078 58,1078 58 C1078 57,1078 57,1077 57 C1077 56,1076 56,1075 56 L1075 54 L1083 54 L1084 54 L1080 71 Z" fill="#212121"/><path d="M1110 35 C1110 26,1109 20,1106 14 C1103 9,1099 5,1093 3 L1094 0 C1101 2,1107 6,1110 12 C1114 19,1116 26,1116 35 C1116 44,1114 52,1110 58 C1107 64,1101 68,1094 70 L1093 67 C1099 65,1103 62,1106 56 C1109 51,1110 43,1110 35 L1110 35 Z" fill="#212121"/><path d="M591 105 L589 112 L583 112 L584 105 L591 105 Z M579 131 C579 128,580 126,580 125 C580 124,579 123,579 123 C578 122,577 122,576 122 L576 120 L585 119 L587 119 L582 144 C582 146,581 148,581 149 C581 150,581 151,582 151 C582 152,583 152,583 152 C584 152,585 152,586 151 C586 151,588 150,589 148 L591 150 C589 152,587 154,585 155 C584 156,582 156,580 156 C579 156,578 155,577 154 C576 153,575 152,575 150 C575 148,576 146,576 143 L579 131 Z" fill="#212121"/><path d="M622 144 L622 139 L641 139 L645 129 L622 129 L622 124 L647 124 L652 112 L657 112 L652 124 L668 124 L668 129 L650 129 L645 139 L668 139 L668 144 L643 144 L638 157 L633 157 L639 144 L622 144 Z" fill="#212121"/><path d="M718 105 L717 112 L710 112 L712 105 L718 105 Z M707 155 C707 159,706 162,704 164 C703 167,701 168,699 169 C697 170,694 171,691 171 C690 171,689 171,688 171 L689 168 C690 168,691 168,692 168 C693 168,694 168,695 168 C696 167,696 167,697 166 C698 165,698 164,699 162 C700 161,700 159,701 157 L707 130 C707 128,707 126,707 125 C707 124,707 123,707 123 C706 122,705 122,704 122 L704 120 L713 119 L715 119 L707 155 Z" fill="#212121"/><path d="M733 146 C734 148,734 150,734 151 C734 153,734 155,733 156 C733 157,732 159,730 160 C729 162,727 163,725 165 L723 163 C724 162,725 161,726 160 C726 159,726 158,727 156 C727 155,727 154,727 152 C727 150,727 148,727 146 L733 146 Z" fill="#212121"/><path d="M885 136 L885 131 L931 131 L931 136 L885 136 Z" fill="#212121"/><path d="M971 155 C970 151,969 146,968 140 C967 133,966 129,965 127 C965 125,964 124,964 124 C964 123,963 123,963 123 C962 123,961 123,960 124 C960 125,959 126,958 127 L955 125 C957 123,959 122,960 121 C962 120,963 119,965 119 C966 119,967 119,968 119 C968 120,969 120,969 121 C970 121,970 122,970 122 C971 123,971 124,971 125 C972 126,972 128,972 130 C973 133,974 136,974 140 C975 143,975 147,975 150 C978 146,980 143,982 140 C983 136,984 134,985 131 C986 129,986 127,986 125 C986 124,986 123,985 123 C985 122,984 122,983 122 L983 120 L993 120 L994 122 C992 127,990 132,986 138 C983 144,980 150,976 155 C972 160,969 164,966 167 C964 168,962 170,961 170 C960 171,958 171,957 171 C956 171,956 171,955 171 C954 171,954 171,953 171 L955 164 L958 164 C958 165,958 166,959 166 C961 166,962 165,964 163 C966 162,968 159,971 155 L971 155 Z" fill="#212121"/><path d="M1000 145 L998 151 L993 151 L994 145 L1000 145 Z M993 173 C993 174,993 175,993 176 C993 176,993 177,993 177 C993 178,993 178,993 179 C993 179,994 179,994 179 C995 179,995 179,995 179 C996 179,996 179,996 178 C997 178,997 178,997 177 C998 177,998 176,999 176 L1001 178 C1000 179,999 180,998 180 C997 181,997 181,996 182 C995 182,995 183,994 183 C993 183,993 183,992 183 C991 183,991 183,990 183 C990 183,989 182,989 182 C988 181,988 181,988 180 C988 180,987 179,987 178 C987 178,988 177,988 176 C988 175,988 174,988 173 C989 172,989 171,989 170 C989 169,990 168,990 167 C990 166,990 165,990 164 C991 163,991 162,991 161 C991 161,991 161,991 160 C991 160,991 160,991 160 C991 159,991 159,990 159 C990 158,989 158,988 158 L988 156 L996 156 L997 156 L993 173 Z" fill="#212121"/><path d="M1023 155 C1022 151,1021 146,1020 140 C1019 133,1018 129,1017 127 C1017 125,1016 124,1016 124 C1016 123,1015 123,1015 123 C1014 123,1013 123,1012 124 C1012 125,1011 126,1010 127 L1007 125 C1009 123,1011 122,1012 121 C1014 120,1015 119,1017 119 C1018 119,1019 119,1020 119 C1020 120,1021 120,1021 121 C1022 121,1022 122,1022 122 C1023 123,1023 124,1023 125 C1024 126,1024 128,1024 130 C1025 133,1026 136,1026 140 C1027 143,1027 147,1027 150 C1030 146,1032 143,1034 140 C1035 136,1036 134,1037 131 C1038 129,1038 127,1038 125 C1038 124,1038 123,1037 123 C1037 122,1036 122,1035 122 L1035 120 L1045 120 L1046 122 C1044 127,1042 132,1038 138 C1035 144,1032 150,1028 155 C1024 160,1021 164,1018 167 C1016 168,1014 170,1013 170 C1012 171,1010 171,1009 171 C1008 171,1008 171,1007 171 C1006 171,1006 171,1005 171 L1007 164 L1010 164 C1010 165,1010 166,1011 166 C1013 166,1014 165,1016 163 C1018 162,1020 159,1023 155 L1023 155 Z" fill="#212121"/><path d="M1064 145 L1063 151 L1057 151 L1058 145 L1064 145 Z M1056 182 C1055 185,1055 186,1054 188 C1053 189,1052 190,1051 191 C1050 192,1049 193,1048 193 C1046 194,1045 194,1043 194 C1043 194,1042 194,1042 194 C1041 194,1041 194,1041 194 L1041 191 C1042 191,1042 191,1042 191 C1043 191,1043 191,1043 191 C1044 191,1045 191,1046 191 C1047 190,1047 190,1048 189 C1048 189,1049 188,1049 187 C1050 186,1050 184,1050 183 C1051 180,1052 177,1052 175 C1053 172,1053 170,1054 168 C1054 167,1054 165,1054 164 C1055 163,1055 162,1055 162 C1055 161,1055 161,1055 161 C1055 160,1055 160,1055 160 C1055 159,1055 159,1054 159 C1054 158,1053 158,1052 158 L1052 156 L1059 156 L1061 156 L1056 182 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">We will organize a loop in which we adjust the error gradient using the derivative of the Softmax normalization function.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;gradient&nbsp;distribution&nbsp;up&nbsp;to&nbsp;Score</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">gradients</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">]&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">gradients</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">values</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">());</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;gradient&nbsp;correction&nbsp;by&nbsp;Softmax&nbsp;derivative</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">r</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">r</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">r</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">ident</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">Identity</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">ones</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">Ones</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">result</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">Zeros</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">result</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">r</span><span class="f_CodeExample">),&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">result</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">ones</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">result</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">result</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">result</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">()&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">ident</span><span class="f_CodeExample">&nbsp;-&nbsp;</span><span class="f_CodeExample" style="color: #333333;">result</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">gradients</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">result</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">gradients</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">r</span><span class="f_CodeExample">))&nbsp;/&nbsp;</span>
<br><span class="f_CodeExample" style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Functions">sqrt</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">r</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">In the next step, we distribute the error gradient to the result values of the </span><span class="f_Text" style="font-style: italic;">m_cQuerys</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">m_cKeys</span><span class="f_Text"> neural layers. However, we will not immediately write the values into the data buffers of the specified neural layers. We will only accumulate the sums of the error gradients into the pre-prepared matrices </span><span class="f_Text" style="font-style: italic;">querys_grad</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">keys_grad</span><span class="f_Text">.</span></p>
<p class="p_Text"><span class="f_Text">Technically, we multiply the adjusted error gradient by the opposite matrix. Multiplying it by the </span><span class="f_Text" style="font-style: italic;">Keys</span><span class="f_Text"> matrix, we get the error gradient for </span><span class="f_Text" style="font-style: italic;">Querys</span><span class="f_Text">, and vice versa. We reformat the obtained matrices and add them to the corresponding local matrices.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;gradient&nbsp;propagation&nbsp;to&nbsp;Querys&nbsp;and&nbsp;Keys</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">gradients</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">])).</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">querys_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">gradients</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">querys</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">])).</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">keys_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After completing the iterations of the loop, we obtain concatenated matrices of error gradients for all internal layers. Finally, we need to format the matrices as required and copy the values into the respective data buffers.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">querys_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">keys_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">values_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">keys_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">values_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//&nbsp;OpenCL&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">As a result, we have propagated the error gradient to the level of internal neural layers. We have successfully addressed the previously set task and are concluding the section on algorithm partitioning based on the computational device. In the multi-threaded operations branch, we will temporarily set the method exit with a false result. We will complete this part later.</span></p>
<p class="p_Text"><span class="f_Text">We haven't propagated the error gradient to the previous layer yet. We will further propagate the error gradient using internal neural layer methods.</span></p>
<p class="p_Text"><span class="f_Text">We've already filled the error gradient buffers of all the inner layers. We only need to call the method for error gradient propagation through the layer to obtain the error gradient at the level of the original data. However, one question remains open: all three internal neural layers (</span><span class="f_Text" style="font-style: italic;">m_cQuerys, m_cKeys, m_cValues</span><span class="f_Text">) use the same tensor from the previous layer as their input data. This means that all three layers must pass the error gradient to the previous layer's buffer. In addition, the result of the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention </span><span class="f_Text">block was added to the tensor of the original data before normalization. Hence, this is the fourth thread of the error gradient that we need to pass to the previous layer level.</span></p>
<p class="p_Text"><span class="f_Text">However, our gradient propagation methods are constructed in a way that when the error gradient is saved in the buffer of the previous layer, it overwrites the previous values, erasing the prior information. This is done intentionally to avoid unnecessary buffer-clearing operations before starting each iteration of the backpropagation pass. To address this issue, after running the </span><span class="f_Text" style="font-style: italic;">CalcHiddenGradient</span><span class="f_Text"> method for each internal neural layer, we will copy the error gradient data to a separate buffer, where we will accumulate it with the previously stored values At this point we should recall that the error gradient at the output of the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention </span><span class="f_Text">block is already contained in the error gradient buffer of the neural layer </span><span class="f_Text" style="font-style: italic;">m_cW0</span><span class="f_Text">. It might seem that this buffer would be suitable for accumulating the error gradient for the previous layer. But that's a misconception. If we were to accumulate the error gradient in the mentioned buffer right now, it would distort the data during the subsequent error gradient propagation to the weight matrix of that layer. At the same time, we can implement the error gradient propagation to the matrix of the </span><span class="f_Text" style="font-style: italic;">m_cW0 layer </span><span class="f_Text">right now. There's all the data you need to do that. We call the </span><span class="f_Text" style="font-style: italic;">CalcDeltaWeights</span><span class="f_Text"> method of the specified neural layer and then use its buffer to accumulate the total error gradient.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;propagate&nbsp;the&nbsp;error&nbsp;gradient&nbsp;to&nbsp;the&nbsp;previous&nbsp;layer</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">attention_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">attention_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SumArray</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">attention_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SumArray</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">SumArray</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">attention_grad</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Attention should be paid to the last group of commands. During the previous operations, we copied data from the gradient buffer of the previous layer, but at the end of the method, we reversed the process by taking the cumulative error gradient from the internal neural layer's buffer and adding it to the values of the buffer of the previous layer. It is in the buffer of the previous layer where we need to obtain the result. From it, the methods of the previous layer will take the error gradient and distribute it further through the neural network.</span></p>
<p class="p_Text"><span class="f_Text">This completes the task set for this method. We complete the method with a positive result.</span></p>
<p class="p_Text"><span class="f_Text">Next, we will work on two more methods that will continue the execution of the error backpropagation algorithm in this class.</span></p>
<p class="p_Text"><span class="f_Text">After propagating the error through all the neural layers of our network, we need to propagate the error gradient to the level of each weight. Our </span><span class="f_Text" style="font-style: italic;">CNeuronMHAttention</span><span class="f_Text"> class does not contain a separate buffer for the weight matrix. All trained parameters are encapsulated in internal neural layers. Therefore, the only thing we need to do in the method for propagating the error gradient to the </span><span class="f_Text" style="font-style: italic;">CalcDeltaWeights</span><span class="f_Text"> weight matrix is to consistently call the same method for all inner layers. At the same time, we should check the results of the operations.</span></p>
<p class="p_Text"><span class="f_Text">Recall that in the previous method, we have already passed the error gradient to the weight matrix of the </span><span class="f_Text" style="font-style: italic;">m_cW0</span><span class="f_Text"> inner layer. It is necessary to exclude it from this iteration.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronMHAttention</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">read</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;call&nbsp;the&nbsp;same&nbsp;method&nbsp;for&nbsp;all&nbsp;inner&nbsp;layers</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">read</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After propagating the error gradients to the weight matrices, the only remaining step is to update the weights of our internal neural layers. This functionality is assigned to the </span><span class="f_Text" style="font-style: italic;">UpdateWeights</span><span class="f_Text"> method. Despite the complexity of the class itself, the method for updating the weight matrices turns out to be very concise and straightforward. It was object inheritance that helped us with this.</span></p>
<p class="p_Text"><span class="f_Text">We created our </span><span class="f_Text" style="font-style: italic;">CNeuronMHAttention</span><span class="f_Text"> class as a descendant of the </span><span class="f_Text" style="font-style: italic;">CNeuronAttention</span><span class="f_Text"> class. We added only one object of the inner </span><span class="f_Text" style="font-style: italic;">m_cW0</span><span class="f_Text"> neural layer. During the operations of the </span><span class="f_Text" style="font-style: italic;">UpdateWeights</span><span class="f_Text"> method of the convolutional neural layers used, all operations are performed only on elements within the object, without accessing data from other objects. That's why we can call a similar method from the parent class, where this process is already implemented for inherited objects. After successfully executing the method of the parent class, we only need to update the coefficient matrix of the </span><span class="f_Text" style="font-style: italic;">m_cW0</span><span class="f_Text"> internal neural layer.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronMHAttention</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">batch_size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,&nbsp;</span>
<br><span class="f_Definition">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VECTOR</span><span class="f_CodeExample">&nbsp;&amp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;&amp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;call&nbsp;the&nbsp;method&nbsp;of&nbsp;the&nbsp;parent&nbsp;class</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">CNeuronAttention</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">batch_size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;call&nbsp;the&nbsp;same&nbsp;method&nbsp;for&nbsp;all&nbsp;inner&nbsp;layers</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">batch_size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Of course, we verify the result of all operations and return a boolean value indicating their execution to the caller.</span></p>
<p class="p_Text"><span class="f_Text">Thus, we are nearing the completion of the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> technology implementation class. We have already implemented the whole algorithm using standard MQL5 tools. You can even create a script and test how it works. However, we still need to supplement our class with file handling methods. </span></p>

</div>

</body>
</html>
