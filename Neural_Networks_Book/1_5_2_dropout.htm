<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>1.5.2 Dropout</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="1_about_ai.htm"> 1. Basic principles of artificial intelligence construction </a> / <a class="h_m" href="1_5_improvement.htm"> 1.5 Techniques for improving the convergence of neural networks </a>/ 1.5.2 Dropout
          </td>
          <td width="70" align="right">
          <a href="1_5_1_regularization.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="1_5_3_normalization.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H2"><span class="f_H2">1.5.2 Dropout</span></p>
<p class="p_Text"><span class="f_Text">We continue studying methods for improving the convergence of neural networks. Let’s consider the dropout technology.</span></p>
<p class="p_Text"><span class="f_Text">When training a neural network, a large number of features are fed into each neuron, the influence of each of which is difficult to assess. As a result, errors of some neurons are smoothed out by the correct values of others, and errors accumulate at the output of the neural network. Training stops at a certain local minimum with a sufficiently large error that does not meet our requirements. This effect was called co-adaptation of features, in which the influence of each feature seemingly adjusts to the surrounding environment. It would be better for us to get the opposite effect when the environment is decomposed into individual features and evaluate separately the impact of each of them.</span></p>
<p class="p_Text"><span class="f_Text">To combat complex co-adaptation of features, in July 2012, a group of scientists from the University of Toronto, in a paper &quot;<a href="https://arxiv.org/abs/1207.0580" target="_blank" rel="external" class="weblink" title="Improving neural networks by preventing co-adaptation of feature detectors">Improving neural networks by preventing co-adaptation of feature detectors</a>&quot;, proposed randomly excluding some neurons during the training process. Reducing the number of features during training increases the significance of each one, and the constant change in the quantitative and qualitative composition of features reduces the risk of their co-adaptation. Such a method is called Dropout.</span></p>
<p class="p_Text"><span class="f_Text">Applying this method can be compared to decision trees because by excluding some neurons at random, we get a new neural network with its own weights at each training iteration. According to the rules of combinatorics, the variability of such networks is quite high.</span></p>
<p class="p_Text"><span class="f_Text">At the same time, all the features and neurons are evaluated during the operation of the neural network. Thereby, we obtain the most accurate and independent assessment of the current state of the studied environment.</span></p>
<p class="p_Text"><span class="f_Text">The authors of the solution in their paper point out that the method can also be used to improve the quality of pre-trained models. </span></p>
<div class="p_Text" style="text-align: center;"><div style="margin:0 auto 0 auto;width:600px"><img class="help" id="dout_2hlpercp" alt="Dropout implementation model for a perceptron with 2 hidden layers" title="Dropout implementation model for a perceptron with 2 hidden layers" width="600" height="400" style="width:600px;height:400px;border:none" src="dout_2hlpercp.png"/><p style="text-align:center"><span class="f_ImageCaption">Dropout implementation model for a perceptron with two hidden layers</span></p></div></div>
<p class="p_Text"><span class="f_Text">Describing the proposed solution from a mathematical point of view, we can say that each individual neuron is excluded from the process with a certain given probability </span><span class="f_Text" style="font-style: italic;">P</span><span class="f_Text">. Thus, the the neuron will participate in the neural network training process with a probability of </span><span class="f_Text" style="font-style: italic;">q=1–P</span><span class="f_Text">.</span></p>
<p class="p_Text"><span class="f_Text">To determine the list of excluded neurons, the method uses a pseudorandom number generator with a normal distribution. This approach allows for the most uniform possible exclusion of neurons. In practice, we will generate a vector of binary features of size equal to the input sequence. In the vector, we will set 1 for the features that are used and 0 for the excluded elements.</span></p>
<p class="p_Text"><span class="f_Text">However, the exclusion of the analyzed features undoubtedly leads to a decrease in the sum at the input of the neuron activation function. To compensate for this effect, we multiply the value of each feature by a factor of </span><span class="f_Text" style="font-style: italic;">1/q</span><span class="f_Text">. It's easy to notice that this coefficient will increase the values, as the probability </span><span class="f_Text" style="font-style: italic;">q</span><span class="f_Text"> is always in the range from 0 to 1.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:95px;height:44px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 380 176"><path d="M26 58 C29 58,32 58,35 59 C37 60,39 61,41 62 C43 63,44 65,45 68 C46 70,47 73,47 76 C47 80,47 83,46 86 C45 89,44 92,43 94 C41 97,40 99,38 101 C36 103,34 104,32 105 C30 106,28 107,26 107 C23 108,21 108,17 108 L2 108 L2 106 C3 106,4 106,5 106 C5 105,5 105,6 104 C6 104,6 103,7 102 C7 101,7 100,8 97 L14 69 C15 67,15 65,15 63 C15 62,15 61,14 61 C13 60,13 60,11 60 L12 58 L26 58 Z M13 105 C13 105,15 105,17 105 C21 105,24 104,27 103 C30 102,32 99,34 96 C36 94,37 90,38 87 C39 84,40 80,40 77 C40 71,39 68,36 65 C34 62,30 61,26 61 C25 61,23 61,22 61 L13 105 Z" fill="#212121"/><path d="M60 96 L58 102 L53 102 L54 96 L60 96 Z M53 124 C53 125,53 126,53 127 C53 127,53 128,53 128 C53 129,53 129,53 130 C53 130,54 130,54 130 C55 130,55 130,55 130 C56 130,56 130,56 129 C57 129,57 129,57 128 C58 128,58 127,59 127 L61 129 C60 130,59 131,58 131 C57 132,57 132,56 133 C55 133,55 134,54 134 C53 134,53 134,52 134 C51 134,51 134,50 134 C50 134,49 133,49 133 C48 132,48 132,48 131 C48 131,47 130,47 129 C47 129,48 128,48 127 C48 126,48 125,48 124 C49 123,49 122,49 121 C49 120,50 119,50 118 C50 117,50 116,50 115 C51 114,51 113,51 112 C51 112,51 112,51 111 C51 111,51 111,51 111 C51 110,51 110,50 110 C50 109,49 109,48 109 L48 107 L56 107 L57 107 L53 124 Z" fill="#212121"/><path d="M91 82 L91 77 L137 77 L137 82 L91 82 Z M91 97 L91 92 L137 92 L137 97 L91 97 Z" fill="#212121"/><path d="M189 41 C189 43,189 44,189 44 C189 45,190 46,190 46 C190 47,191 47,192 47 C193 48,193 48,195 48 C196 48,197 48,199 48 L199 51 L172 51 L172 48 C175 48,177 48,178 48 C179 47,180 47,180 47 C181 46,181 46,182 45 C182 44,182 43,182 41 L182 11 C182 10,182 10,182 9 C181 9,181 9,180 9 C179 9,178 9,177 10 C175 11,174 12,172 13 L170 10 L187 0 L189 0 C189 2,189 6,189 10 L189 41 Z" fill="#212121"/><path d="M189 150 C187 153,185 155,183 157 C181 158,178 159,176 159 C174 159,172 158,170 156 C169 154,168 151,168 147 C168 143,169 139,170 135 C172 131,174 128,177 126 C180 123,183 122,187 122 C188 122,190 122,191 123 C192 123,194 124,195 125 L199 122 L201 123 L192 165 C191 167,191 168,191 169 C191 170,191 171,192 171 C192 172,193 172,195 172 L195 174 L183 174 L190 151 L189 150 Z M192 136 C192 134,193 132,193 131 C193 129,192 127,191 126 C191 126,189 125,187 125 C185 125,183 126,181 128 C179 130,177 133,176 137 C175 140,174 144,174 147 C174 150,175 152,175 153 C176 154,177 155,179 155 C180 155,180 155,181 155 C182 154,183 154,184 153 C185 152,185 151,186 150 C187 149,188 148,188 147 C189 146,190 145,190 143 C191 141,191 140,192 137 L192 136 Z" fill="#212121"/><rect x="163" y="84" width="42" height="5" fill="#212121"/><path d="M234 82 C236 78,239 76,241 74 C243 73,245 72,248 72 C250 72,252 73,253 74 C254 75,255 77,255 80 L255 80 C255 80,255 80,255 81 C257 78,259 76,261 74 C263 73,265 72,267 72 C270 72,271 73,273 74 C274 75,275 77,275 80 C275 81,274 84,273 86 L271 96 C270 99,270 101,270 102 C270 103,270 104,271 104 C271 105,271 105,272 105 C273 105,274 105,274 104 C275 104,276 103,278 101 L280 103 C278 105,276 107,275 108 C273 109,271 109,269 109 C268 109,266 108,265 107 C264 106,264 105,264 103 C264 101,264 99,265 96 L267 89 C267 87,268 85,268 84 C268 83,268 82,268 81 C268 79,268 78,267 77 C267 76,266 76,265 76 C264 76,262 77,261 77 C260 78,259 79,258 81 C256 83,255 84,254 86 C254 87,253 89,252 92 L249 108 L243 108 L247 89 C248 87,248 85,248 84 C248 83,248 82,248 81 C248 79,248 78,247 77 C247 76,246 76,244 76 C244 76,243 77,241 77 C240 78,239 79,238 81 C236 83,235 84,234 86 C234 88,233 90,232 92 L229 108 L223 108 L228 84 C229 82,229 80,229 79 C229 78,229 77,228 77 C228 76,228 76,227 76 C226 76,225 76,224 77 C224 78,222 79,221 80 L219 78 C221 76,223 75,224 74 C226 73,228 72,230 72 C231 72,232 73,233 74 C234 75,235 76,235 77 C235 79,234 80,234 82 L234 82 Z" fill="#212121"/><path d="M293 96 L291 102 L286 102 L287 96 L293 96 Z M286 124 C286 125,286 126,286 127 C286 127,286 128,286 128 C286 129,286 129,286 130 C286 130,287 130,287 130 C288 130,288 130,288 130 C289 130,289 130,289 129 C290 129,290 129,290 128 C291 128,291 127,292 127 L294 129 C293 130,292 131,291 131 C290 132,290 132,289 133 C288 133,288 134,287 134 C286 134,286 134,285 134 C284 134,284 134,283 134 C283 134,282 133,282 133 C281 132,281 132,281 131 C281 131,280 130,280 129 C280 129,281 128,281 127 C281 126,281 125,281 124 C282 123,282 122,282 121 C282 120,283 119,283 118 C283 117,283 116,283 115 C284 114,284 113,284 112 C284 112,284 112,284 111 C284 111,284 111,284 111 C284 110,284 110,283 110 C283 109,282 109,281 109 L281 107 L289 107 L290 107 L286 124 Z" fill="#212121"/><path d="M315 91 C315 89,315 88,314 86 C314 84,314 82,313 81 C313 79,312 78,312 77 C312 77,312 77,311 76 C311 76,311 76,310 76 C310 76,309 76,309 76 C308 77,308 77,307 78 C307 78,306 79,305 80 L303 78 C305 76,306 75,307 74 C309 73,311 72,312 72 C313 72,314 72,314 72 C315 73,316 73,316 73 C316 74,317 74,317 75 C318 75,318 76,318 77 C319 78,319 79,319 81 C319 82,320 84,320 85 L320 85 C323 82,324 79,325 78 C327 76,328 75,329 74 C329 73,330 73,331 73 C332 72,333 72,334 72 C335 72,336 72,337 73 L335 80 L333 80 C333 79,332 78,332 78 C331 78,331 78,331 78 C331 78,330 78,330 79 C330 79,329 79,328 80 C328 81,327 82,326 83 C325 84,324 85,323 86 L321 89 C321 92,322 94,322 95 C323 97,323 99,323 100 C324 101,324 102,324 103 C324 103,325 104,325 104 C325 105,325 105,326 105 C326 105,326 105,327 105 C327 105,328 105,328 104 C329 104,330 103,331 101 L334 103 C332 105,330 107,329 108 C328 109,326 109,324 109 C323 109,322 109,321 108 C321 108,320 107,319 107 C319 106,318 104,318 103 C317 99,317 97,317 95 L316 95 C314 99,312 102,310 103 C309 105,308 106,307 107 C307 108,306 108,305 109 C304 109,303 109,302 109 C301 109,300 109,299 109 L301 101 L303 101 C303 102,304 103,304 103 C305 103,305 103,306 103 C306 103,306 102,307 101 C308 101,309 100,310 98 C311 97,313 94,315 91 L315 91 Z" fill="#212121"/><path d="M349 96 L347 102 L342 102 L343 96 L349 96 Z M342 124 C342 125,342 126,342 127 C342 127,342 128,342 128 C342 129,342 129,342 130 C342 130,343 130,343 130 C344 130,344 130,344 130 C345 130,345 130,345 129 C346 129,346 129,346 128 C347 128,347 127,348 127 L350 129 C349 130,348 131,347 131 C346 132,346 132,345 133 C344 133,344 134,343 134 C342 134,342 134,341 134 C340 134,340 134,339 134 C339 134,338 133,338 133 C337 132,337 132,337 131 C337 131,336 130,336 129 C336 129,337 128,337 127 C337 126,337 125,337 124 C338 123,338 122,338 121 C338 120,339 119,339 118 C339 117,339 116,339 115 C340 114,340 113,340 112 C340 112,340 112,340 111 C340 111,340 111,340 111 C340 110,340 110,339 110 C339 109,338 109,337 109 L337 107 L345 107 L346 107 L342 124 Z" fill="#212121"/><path d="M365 99 C366 101,366 103,366 104 C366 106,366 108,365 109 C365 110,364 112,362 113 C361 115,359 116,357 118 L355 116 C356 115,357 114,358 113 C358 112,358 111,359 109 C359 108,359 107,359 105 C359 103,359 101,359 99 L365 99 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">Where:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li" style="font-style: italic; font-weight: bold;">D</span><span class="f_li" style="font-size: 7pt; font-style: italic; font-weight: bold; vertical-align: sub;">i</span><span class="f_li"> = elements of the Dropout results vector</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic; font-weight: bold;">q</span><span class="f_li"> = probability of using a neuron during the learning process</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic; font-weight: bold;">m</span><span class="f_li" style="font-size: 7pt; font-style: italic; font-weight: bold; vertical-align: sub;">i</span><span class="f_li"> = the element of the masking vector</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic; font-weight: bold;">x</span><span class="f_li" style="font-size: 7pt; font-style: italic; font-weight: bold; vertical-align: sub;">i</span><span class="f_li"> = the elements of the input sequence vector</span></li>
</ul>
<p class="p_Text"><span class="f_Text">During the backward pass in the training process, the error gradient is multiplied by the derivative of the aforementioned function. As can be easily seen, in the case of Dropout, the backward pass will be similar to the forward pass which uses the masking vector from the forward pass. </span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:87px;height:46px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 348 184"><path d="M26 43 C23 47,21 49,19 51 C17 52,15 53,12 53 C10 53,8 52,6 50 C5 48,4 45,4 41 C4 37,5 33,6 29 C8 25,10 22,13 20 C16 17,19 16,23 16 C24 16,26 16,27 17 C28 17,29 17,31 18 L33 10 C33 9,33 8,33 7 C33 6,33 6,33 5 C33 4,33 4,33 3 C33 3,32 3,32 2 C31 2,30 2,29 2 L30 0 L39 0 L41 0 L32 41 C31 43,31 45,31 46 C31 47,31 48,31 48 C32 49,32 49,33 49 C34 49,35 49,35 48 C36 48,37 47,39 45 L41 47 C39 49,37 51,35 52 C34 53,32 53,30 53 C29 53,28 53,27 52 C26 51,25 49,25 48 C25 47,26 45,26 44 L26 43 Z M28 30 C28 28,29 26,29 25 C29 23,28 21,27 20 C27 20,25 19,23 19 C21 19,19 20,17 22 C15 24,13 27,12 31 C11 34,10 38,10 41 C10 44,11 46,11 47 C12 48,13 49,15 49 C16 49,16 49,17 49 C18 48,19 48,20 47 C21 46,21 45,22 44 C23 43,24 42,24 41 C25 40,26 39,26 37 C27 35,27 34,28 31 L28 30 Z" fill="#212121"/><path d="M69 2 C72 2,75 2,78 3 C80 4,82 5,84 6 C86 7,87 9,88 12 C89 14,90 17,90 20 C90 24,90 27,89 30 C88 33,87 36,86 38 C84 41,83 43,81 45 C79 47,77 48,75 49 C73 50,71 51,69 51 C66 52,64 52,60 52 L45 52 L45 50 C46 50,47 50,48 50 C48 49,48 49,49 48 C49 48,49 47,50 46 C50 45,50 44,51 41 L57 13 C58 11,58 9,58 7 C58 6,58 5,57 5 C56 4,56 4,54 4 L55 2 L69 2 Z M56 49 C56 49,58 49,60 49 C64 49,67 48,70 47 C73 46,75 43,77 40 C79 38,80 34,81 31 C82 28,83 24,83 21 C83 15,82 12,79 9 C77 6,73 5,69 5 C68 5,66 5,65 5 L56 49 Z" fill="#212121"/><path d="M103 40 L101 46 L96 46 L97 40 L103 40 Z M96 68 C96 69,96 70,96 71 C96 71,96 72,96 72 C96 73,96 73,96 74 C96 74,97 74,97 74 C98 74,98 74,98 74 C99 74,99 74,99 73 C100 73,100 73,100 72 C101 72,101 71,102 71 L104 73 C103 74,102 75,101 75 C100 76,100 76,99 77 C98 77,98 78,97 78 C96 78,96 78,95 78 C94 78,94 78,93 78 C93 78,92 77,92 77 C91 76,91 76,91 75 C91 75,90 74,90 73 C90 73,91 72,91 71 C91 70,91 69,91 68 C92 67,92 66,92 65 C92 64,93 63,93 62 C93 61,93 60,93 59 C94 58,94 57,94 56 C94 56,94 56,94 55 C94 55,94 55,94 55 C94 54,94 54,93 54 C93 53,92 53,91 53 L91 51 L99 51 L100 51 L96 68 Z" fill="#212121"/><path d="M30 153 C27 157,25 159,23 161 C21 162,19 163,16 163 C14 163,12 162,10 160 C9 158,8 155,8 151 C8 147,9 143,10 139 C12 135,14 132,17 130 C20 127,23 126,27 126 C28 126,30 126,31 127 C32 127,33 127,35 128 L37 120 C37 119,37 118,37 117 C37 116,37 116,37 115 C37 114,37 114,37 113 C37 113,36 113,36 112 C35 112,34 112,33 112 L34 110 L43 110 L45 110 L36 151 C35 153,35 155,35 156 C35 157,35 158,35 158 C36 159,36 159,37 159 C38 159,39 159,39 158 C40 158,41 157,43 155 L45 157 C43 159,41 161,39 162 C38 163,36 163,34 163 C33 163,32 163,31 162 C30 161,29 159,29 158 C29 157,30 155,30 154 L30 153 Z M32 140 C32 138,33 136,33 135 C33 133,32 131,31 130 C31 130,29 129,27 129 C25 129,23 130,21 132 C19 134,17 137,16 141 C15 144,14 148,14 151 C14 154,15 156,15 157 C16 158,17 159,19 159 C20 159,20 159,21 159 C22 158,23 158,24 157 C25 156,25 155,26 154 C27 153,28 152,28 151 C29 150,30 149,30 147 C31 145,31 144,32 141 L32 140 Z" fill="#212121"/><path d="M64 145 C64 143,64 142,63 140 C63 138,63 136,62 135 C62 133,61 132,61 131 C61 131,61 131,60 130 C60 130,60 130,59 130 C59 130,58 130,58 130 C57 131,57 131,56 132 C56 132,55 133,54 134 L52 132 C54 130,55 129,56 128 C58 127,60 126,61 126 C62 126,63 126,63 126 C64 127,65 127,65 127 C65 128,66 128,66 129 C67 129,67 130,67 131 C68 132,68 133,68 135 C68 136,69 138,69 139 L69 139 C72 136,73 133,74 132 C76 130,77 129,78 128 C78 127,79 127,80 127 C81 126,82 126,83 126 C84 126,85 126,86 127 L84 134 L82 134 C82 133,81 132,81 132 C80 132,80 132,80 132 C80 132,79 132,79 133 C79 133,78 133,77 134 C77 135,76 136,75 137 C74 138,73 139,72 140 L70 143 C70 146,71 148,71 149 C72 151,72 153,72 154 C73 155,73 156,73 157 C73 157,74 158,74 158 C74 159,74 159,75 159 C75 159,75 159,76 159 C76 159,77 159,77 158 C78 158,79 157,80 155 L83 157 C81 159,79 161,78 162 C77 163,75 163,73 163 C72 163,71 163,70 162 C70 162,69 161,68 161 C68 160,67 158,67 157 C66 153,66 151,66 149 L65 149 C63 153,61 156,59 157 C58 159,57 160,56 161 C56 162,55 162,54 163 C53 163,52 163,51 163 C50 163,49 163,48 163 L50 155 L52 155 C52 156,53 157,53 157 C54 157,54 157,55 157 C55 157,55 156,56 155 C57 155,58 154,59 152 C60 151,62 148,64 145 L64 145 Z" fill="#212121"/><path d="M98 150 L96 156 L91 156 L92 150 L98 150 Z M91 178 C91 179,91 180,91 181 C91 181,91 182,91 182 C91 183,91 183,91 184 C91 184,92 184,92 184 C93 184,93 184,93 184 C94 184,94 184,94 183 C95 183,95 183,95 182 C96 182,96 181,97 181 L99 183 C98 184,97 185,96 185 C95 186,95 186,94 187 C93 187,93 188,92 188 C91 188,91 188,90 188 C89 188,89 188,88 188 C88 188,87 187,87 187 C86 186,86 186,86 185 C86 185,85 184,85 183 C85 183,86 182,86 181 C86 180,86 179,86 178 C87 177,87 176,87 175 C87 174,88 173,88 172 C88 171,88 170,88 169 C89 168,89 167,89 166 C89 166,89 166,89 165 C89 165,89 165,89 165 C89 164,89 164,88 164 C88 163,87 163,86 163 L86 161 L94 161 L95 161 L91 178 Z" fill="#212121"/><rect x="0" y="88" width="108" height="5" fill="#212121"/><path d="M134 86 L134 81 L180 81 L180 86 L134 86 Z M134 101 L134 96 L180 96 L180 101 L134 101 Z" fill="#212121"/><path d="M232 45 C232 47,232 48,232 48 C232 49,233 50,233 50 C233 51,234 51,235 51 C236 52,236 52,238 52 C239 52,240 52,242 52 L242 55 L215 55 L215 52 C218 52,220 52,221 52 C222 51,223 51,223 51 C224 50,224 50,225 49 C225 48,225 47,225 45 L225 15 C225 14,225 14,225 13 C224 13,224 13,223 13 C222 13,221 13,220 14 C218 15,217 16,215 17 L213 14 L230 4 L232 4 C232 6,232 10,232 14 L232 45 Z" fill="#212121"/><path d="M232 154 C230 157,228 159,226 161 C224 162,221 163,219 163 C217 163,215 162,213 160 C212 158,211 155,211 151 C211 147,212 143,213 139 C215 135,217 132,220 130 C223 127,226 126,230 126 C231 126,233 126,234 127 C235 127,237 128,238 129 L242 126 L244 127 L235 169 C234 171,234 172,234 173 C234 174,234 175,235 175 C235 176,236 176,238 176 L238 178 L226 178 L233 155 L232 154 Z M235 140 C235 138,236 136,236 135 C236 133,235 131,234 130 C234 130,232 129,230 129 C228 129,226 130,224 132 C222 134,220 137,219 141 C218 144,217 148,217 151 C217 154,218 156,218 157 C219 158,220 159,222 159 C223 159,223 159,224 159 C225 158,226 158,227 157 C228 156,228 155,229 154 C230 153,231 152,231 151 C232 150,233 149,233 147 C234 145,234 144,235 141 L235 140 Z" fill="#212121"/><rect x="206" y="88" width="42" height="5" fill="#212121"/><path d="M277 86 C279 82,282 80,284 78 C286 77,288 76,291 76 C293 76,295 77,296 78 C297 79,298 81,298 84 L298 84 C298 84,298 84,298 85 C300 82,302 80,304 78 C306 77,308 76,310 76 C313 76,314 77,316 78 C317 79,318 81,318 84 C318 85,317 88,316 90 L314 100 C313 103,313 105,313 106 C313 107,313 108,314 108 C314 109,314 109,315 109 C316 109,317 109,317 108 C318 108,319 107,321 105 L323 107 C321 109,319 111,318 112 C316 113,314 113,312 113 C311 113,309 112,308 111 C307 110,307 109,307 107 C307 105,307 103,308 100 L310 93 C310 91,311 89,311 88 C311 87,311 86,311 85 C311 83,311 82,310 81 C310 80,309 80,308 80 C307 80,305 81,304 81 C303 82,302 83,301 85 C299 87,298 88,297 90 C297 91,296 93,295 96 L292 112 L286 112 L290 93 C291 91,291 89,291 88 C291 87,291 86,291 85 C291 83,291 82,290 81 C290 80,289 80,287 80 C287 80,286 81,284 81 C283 82,282 83,281 85 C279 87,278 88,277 90 C277 92,276 94,275 96 L272 112 L266 112 L271 88 C272 86,272 84,272 83 C272 82,272 81,271 81 C271 80,271 80,270 80 C269 80,268 80,267 81 C267 82,265 83,264 84 L262 82 C264 80,266 79,267 78 C269 77,271 76,273 76 C274 76,275 77,276 78 C277 79,278 80,278 81 C278 83,277 84,277 86 L277 86 Z" fill="#212121"/><path d="M336 100 L334 106 L329 106 L330 100 L336 100 Z M329 128 C329 129,329 130,329 131 C329 131,329 132,329 132 C329 133,329 133,329 134 C329 134,330 134,330 134 C331 134,331 134,331 134 C332 134,332 134,332 133 C333 133,333 133,333 132 C334 132,334 131,335 131 L337 133 C336 134,335 135,334 135 C333 136,333 136,332 137 C331 137,331 138,330 138 C329 138,329 138,328 138 C327 138,327 138,326 138 C326 138,325 137,325 137 C324 136,324 136,324 135 C324 135,323 134,323 133 C323 133,324 132,324 131 C324 130,324 129,324 128 C325 127,325 126,325 125 C325 124,326 123,326 122 C326 121,326 120,326 119 C327 118,327 117,327 116 C327 116,327 116,327 115 C327 115,327 115,327 115 C327 114,327 114,326 114 C326 113,325 113,324 113 L324 111 L332 111 L333 111 L329 128 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">During the operation of the neural network, the masking vector is filled with units, allowing values to be transmitted seamlessly in both directions.</span></p>
<p class="p_Text"><span class="f_Text">In practice, the coefficient </span><span class="f_Text" style="font-style: italic;">1/q</span><span class="f_Text"> is constant throughout training, so we can easily count this coefficient once and write it instead of units in the masking tensor. In this way, we eliminate the operations of recalculating the coefficient and multiplying it by 1 of the mask in each training iteration.</span></p>

</div>

</body>
</html>
