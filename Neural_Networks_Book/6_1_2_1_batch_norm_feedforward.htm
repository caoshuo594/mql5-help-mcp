<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>6.1.2.1 Batch normalization feed-forward methods</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="6_improvement_realization.htm"> 6. Architectural solutions for improving model convergence </a> / <a class="h_m" href="6_1_batch_norm.htm"> 6.1 Batch normalization </a> / <a class="h_m" href="6_1_2_batch_norm_mql.htm"> 6.1.2 Building a batch normalization class in MQL5 </a>/ 6.1.2.1 Batch normalization feed-forward methods
          </td>
          <td width="70" align="right">
          <a href="6_1_2_batch_norm_mql.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="6_1_2_2_batch_norm_backprop.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H3"><span class="f_H3">6.1.2.1 Batch normalization feed-forward methods</span></p>
<p class="p_Text"><span class="f_Text">We continue moving forward along the path of building the batch normalization class, and simultaneously, along the path of understanding the structure and methods of organizing neural networks. Earlier, we discussed various architectures for constructing neural layers to solve practical tasks. However, the operation of the batch normalization layer is equally important in organizing the functioning of a neural network, although its task may not be immediately apparent. Rather, it is hidden within the organization of the processes of the neural network itself and serves more for the stability of our model.</span></p>
<p class="p_Text"><span class="f_Text">We have already built the class initialization methods. Now it's time to build the algorithm of method operation directly. We begin this process with the </span><span class="f_Text" style="font-style: italic;">FeedForward</span><span class="f_Text"> method. This method is declared virtual in the </span><span class="f_Text" style="font-style: italic;">CNeuronBase</span><span class="f_Text"> neural layer base class of our library and is overridden in each new class.</span></p>
<p class="p_Text"><span class="f_Text">I would like to remind you that this approach allows us to eliminate the use of dispatch methods and functions for reallocating information flows and calling various methods depending on the class of the object being used. In practice, we can simply pass a pointer to any derived object into a local variable of the base class of the neural layer and call the method declared in the base class. At the same time, the system will perform all dispatching functions without our participation. It will call the method related to the actual type of the object.</span></p>
<p class="p_Text"><span class="f_Text">This property is exactly what we exploit when expecting to receive a pointer to an object of the base class of the neural layer in the method parameters. At the same time, a pointer to any of the neural layer objects in our library can be passed in the parameters. We can work with it through the use of overridden virtual functions.</span></p>
<p class="p_Text"><span class="f_Text">The operation of the feed-forward method itself starts with a control block for checking pointers to the objects used by the method. Here we check both the pointer to the object of the previous layer obtained in the parameters and pointers to internal objects.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBatchNorm</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;control&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">()&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cWeights</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cActivation</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Please note that along with other objects, we also check the pointer to the activation function object. Although the batch normalization algorithm does not use an activation function, we will not limit the user's capabilities and will provide them with the option to use an activation function as they see fit. Moreover, there are practical cases where applying an activation function after data normalization is beneficial. For example, the method authors recommend normalizing data immediately before applying the activation function. At first glance, applying such an approach would require modifications to every previously discussed class. However, we can implement the same functionality without modifying the existing classes. We simply need to declare the required neural layer without an activation function, followed by a normalization layer with the desired activation function. Therefore, I believe the use of the activation feature in our class is justified.</span></p>
<p class="p_Text"><span class="f_Text">Next, we will branch the algorithm for a case when the normalization batch size is equal to 1 or less. It should be understood that when the batch is equal to 1, no normalization is performed, and we simply pass the tensor of the original data to the output of the neural layer. After completing the data copy from the buffer, we call the activation method and exit the method after verifying the results of the operations.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;check&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;normalization&nbsp;batch</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iBatchSize</span><span class="f_CodeExample">&nbsp;&lt;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">&nbsp;&amp;&amp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">BufferWrite</span><span class="f_CodeExample">())</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cActivation</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Activation</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, we need to construct the algorithm of the method. Following the concept we have adopted, we will create two variants of the algorithm implementation: by standard MQL5 tools and in the multi-threaded calculations mode using OpenCL. Therefore, next, we create another branching of the algorithm depending on the user's choice of the computational device. In this section, we will consider the construction of the algorithm using MQL5. In further sections, we will return to the construction of the algorithm using OpenCL.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;branching&nbsp;of&nbsp;the&nbsp;algorithm&nbsp;over&nbsp;the&nbsp;computing&nbsp;device</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We start the block of operations using MQL5 with a small preparatory work. To simplify the process of accessing the data, we save a sequence of raw data into a local matrix.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Total</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">According to the data normalization algorithm, we find the mean value. In considering the architecture of our solution, we have decided to use an exponential moving average, which is determined by the formula. </span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:173px;height:41px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 692 164"><path d="M26 104 C24 108,22 110,20 112 C18 113,16 114,14 114 C13 114,12 114,11 114 C10 113,10 113,9 112 C8 113,8 115,8 118 C8 120,8 123,8 125 C8 126,8 126,8 127 L7 129 L2 129 L1 127 C2 123,3 119,4 114 L11 79 L18 79 L13 99 C13 100,12 102,12 103 C12 104,12 105,12 106 C12 108,12 108,13 109 C14 110,15 110,16 110 C17 110,19 109,20 108 C22 106,24 104,25 102 C26 100,27 98,28 96 L31 79 L38 79 L32 103 C32 105,32 107,32 108 C32 109,32 110,32 110 C32 111,33 111,34 111 C34 111,35 111,36 110 C36 110,37 109,38 108 L40 110 C39 111,38 112,37 113 C36 114,35 114,34 115 C33 115,32 115,31 115 C29 115,28 115,27 114 C26 113,26 112,26 110 C26 109,26 107,27 105 L26 104 Z" fill="#212121"/><path d="M53 103 L51 109 L46 109 L47 103 L53 103 Z M46 131 C46 132,46 133,46 134 C46 134,46 135,46 135 C46 136,46 136,46 137 C46 137,47 137,47 137 C48 137,48 137,48 137 C49 137,49 137,49 136 C50 136,50 136,50 135 C51 135,51 134,52 134 L54 136 C53 137,52 138,51 138 C50 139,50 139,49 140 C48 140,48 141,47 141 C46 141,46 141,45 141 C44 141,44 141,43 141 C43 141,42 140,42 140 C41 139,41 139,41 138 C41 138,40 137,40 136 C40 136,41 135,41 134 C41 133,41 132,41 131 C42 130,42 129,42 128 C42 127,43 126,43 125 C43 124,43 123,43 122 C44 121,44 120,44 119 C44 119,44 119,44 118 C44 118,44 118,44 118 C44 117,44 117,43 117 C43 116,42 116,41 116 L41 114 L49 114 L50 114 L46 131 Z" fill="#212121"/><path d="M84 88 L84 83 L130 83 L130 88 L84 88 Z M84 103 L84 98 L130 98 L130 103 L84 103 Z" fill="#212121"/><path d="M182 43 C180 47,178 49,176 51 C174 52,172 53,170 53 C169 53,168 53,167 53 C166 52,166 52,165 51 C164 52,164 54,164 57 C164 59,164 62,164 64 C164 65,164 65,164 66 L163 68 L158 68 L157 66 C158 62,159 58,160 53 L167 18 L174 18 L169 38 C169 39,168 41,168 42 C168 43,168 44,168 45 C168 47,168 47,169 48 C170 49,171 49,172 49 C173 49,175 48,176 47 C178 45,180 43,181 41 C182 39,183 37,184 35 L187 18 L194 18 L188 42 C188 44,188 46,188 47 C188 48,188 49,188 49 C188 50,189 50,190 50 C190 50,191 50,192 49 C192 49,193 48,194 47 L196 49 C195 50,194 51,193 52 C192 53,191 53,190 54 C189 54,188 54,187 54 C185 54,184 54,183 53 C182 52,182 51,182 49 C182 48,182 46,183 44 L182 43 Z" fill="#212121"/><path d="M209 42 L207 48 L202 48 L203 42 L209 42 Z M202 70 C202 71,202 72,202 73 C202 73,202 74,202 74 C202 75,202 75,202 76 C202 76,203 76,203 76 C204 76,204 76,204 76 C205 76,205 76,205 75 C206 75,206 75,206 74 C207 74,207 73,208 73 L210 75 C209 76,208 77,207 77 C206 78,206 78,205 79 C204 79,204 80,203 80 C202 80,202 80,201 80 C200 80,200 80,199 80 C199 80,198 79,198 79 C197 78,197 78,197 77 C197 77,196 76,196 75 C196 75,197 74,197 73 C197 72,197 71,197 70 C198 69,198 68,198 67 C198 66,199 65,199 64 C199 63,199 62,199 61 C200 60,200 59,200 58 C200 58,200 58,200 57 C200 57,200 57,200 57 C200 56,200 56,199 56 C199 55,198 55,197 55 L197 53 L205 53 L206 53 L202 70 Z" fill="#212121"/><path d="M218 66 L218 62 L248 62 L248 66 L218 66 Z" fill="#212121"/><path d="M273 73 C273 74,273 74,273 75 C273 75,273 76,273 76 C273 76,273 76,273 77 C273 77,274 77,274 77 C274 77,274 77,275 78 C275 78,275 78,276 78 C276 78,277 78,278 78 C279 78,280 78,281 78 L281 80 L259 80 L259 78 C260 78,261 78,262 78 C263 78,263 78,264 78 C264 78,265 78,265 78 C266 77,266 77,266 77 C266 77,267 77,267 77 C267 77,267 76,267 76 C267 76,267 75,267 75 C267 75,267 74,267 73 L267 54 C267 54,267 53,267 53 C267 53,266 52,266 52 C265 52,265 53,263 53 C262 54,261 55,259 56 C259 55,259 55,259 55 C258 54,258 54,258 53 C260 52,262 51,265 50 C267 49,269 47,271 46 L273 46 C273 47,273 48,273 49 C273 50,273 50,273 51 C273 52,273 52,273 53 L273 73 Z" fill="#212121"/><path d="M296 35 C296 43,297 51,300 56 C303 62,307 65,313 67 L312 70 C305 68,299 64,296 58 C292 52,290 44,290 35 C290 26,292 19,296 12 C299 6,305 2,312 0 L313 3 C307 5,303 9,300 14 C297 20,296 26,296 35 L296 35 Z" fill="#212121"/><path d="M330 27 C332 23,335 21,337 19 C339 18,341 17,344 17 C346 17,348 18,349 19 C350 20,351 22,351 25 L351 25 C351 25,351 25,351 26 C353 23,355 21,357 19 C359 18,361 17,363 17 C366 17,367 18,369 19 C370 20,371 22,371 25 C371 26,370 29,369 31 L367 41 C366 44,366 46,366 47 C366 48,366 49,367 49 C367 50,367 50,368 50 C369 50,370 50,370 49 C371 49,372 48,374 46 L376 48 C374 50,372 52,371 53 C369 54,367 54,365 54 C364 54,362 53,361 52 C360 51,360 50,360 48 C360 46,360 44,361 41 L363 34 C363 32,364 30,364 29 C364 28,364 27,364 26 C364 24,364 23,363 22 C363 21,362 21,361 21 C360 21,358 22,357 22 C356 23,355 24,354 26 C352 28,351 29,350 31 C350 32,349 34,348 37 L345 53 L339 53 L343 34 C344 32,344 30,344 29 C344 28,344 27,344 26 C344 24,344 23,343 22 C343 21,342 21,340 21 C340 21,339 22,337 22 C336 23,335 24,334 26 C332 28,331 29,330 31 C330 33,329 35,328 37 L325 53 L319 53 L324 29 C325 27,325 25,325 24 C325 23,325 22,324 22 C324 21,324 21,323 21 C322 21,321 21,320 22 C320 23,318 24,317 25 L315 23 C317 21,319 20,320 19 C322 18,324 17,326 17 C327 17,328 18,329 19 C330 20,331 21,331 22 C331 24,330 25,330 27 L330 27 Z" fill="#212121"/><path d="M400 34 L400 29 L446 29 L446 34 L400 34 Z" fill="#212121"/><path d="M494 43 C494 45,494 46,494 46 C494 47,495 48,495 48 C495 49,496 49,497 49 C498 50,498 50,500 50 C501 50,502 50,504 50 L504 53 L477 53 L477 50 C480 50,482 50,483 50 C484 49,485 49,485 49 C486 48,486 48,487 47 C487 46,487 45,487 43 L487 13 C487 12,487 12,487 11 C486 11,486 11,485 11 C484 11,483 11,482 12 C480 13,479 14,477 15 L475 12 L492 2 L494 2 C494 4,494 8,494 12 L494 43 Z" fill="#212121"/><path d="M528 35 C528 26,527 20,524 14 C521 9,517 5,511 3 L512 0 C519 2,525 6,528 12 C532 19,534 26,534 35 C534 44,532 52,528 58 C525 64,519 68,512 70 L511 67 C517 65,521 62,524 56 C527 51,528 43,528 35 L528 35 Z" fill="#212121"/><path d="M589 34 L589 55 L583 55 L583 34 L563 34 L563 29 L583 29 L583 8 L589 8 L589 29 L609 29 L609 34 L589 34 Z" fill="#212121"/><path d="M648 36 C648 34,648 33,647 31 C647 29,647 27,646 26 C646 24,645 23,645 22 C645 22,645 22,644 21 C644 21,644 21,643 21 C643 21,642 21,642 21 C641 22,641 22,640 23 C640 23,639 24,638 25 L636 23 C638 21,639 20,640 19 C642 18,644 17,645 17 C646 17,647 17,647 17 C648 18,649 18,649 18 C649 19,650 19,650 20 C651 20,651 21,651 22 C652 23,652 24,652 26 C652 27,653 29,653 30 L653 30 C656 27,657 24,658 23 C660 21,661 20,662 19 C662 18,663 18,664 18 C665 17,666 17,667 17 C668 17,669 17,670 18 L668 25 L666 25 C666 24,665 23,665 23 C664 23,664 23,664 23 C664 23,663 23,663 24 C663 24,662 24,661 25 C661 26,660 27,659 28 C658 29,657 30,656 31 L654 34 C654 37,655 39,655 40 C656 42,656 44,656 45 C657 46,657 47,657 48 C657 48,658 49,658 49 C658 50,658 50,659 50 C659 50,659 50,660 50 C660 50,661 50,661 49 C662 49,663 48,664 46 L667 48 C665 50,663 52,662 53 C661 54,659 54,657 54 C656 54,655 54,654 53 C654 53,653 52,652 52 C652 51,651 49,651 48 C650 44,650 42,650 40 L649 40 C647 44,645 47,643 48 C642 50,641 51,640 52 C640 53,639 53,638 54 C637 54,636 54,635 54 C634 54,633 54,632 54 L634 46 L636 46 C636 47,637 48,637 48 C638 48,638 48,639 48 C639 48,639 47,640 46 C641 46,642 45,643 43 C644 42,646 39,648 36 L648 36 Z" fill="#212121"/><path d="M682 41 L680 47 L675 47 L676 41 L682 41 Z M675 69 C675 70,675 71,675 72 C675 72,675 73,675 73 C675 74,675 74,675 75 C675 75,676 75,676 75 C677 75,677 75,677 75 C678 75,678 75,678 74 C679 74,679 74,679 73 C680 73,680 72,681 72 L683 74 C682 75,681 76,680 76 C679 77,679 77,678 78 C677 78,677 79,676 79 C675 79,675 79,674 79 C673 79,673 79,672 79 C672 79,671 78,671 78 C670 77,670 77,670 76 C670 76,669 75,669 74 C669 74,670 73,670 72 C670 71,670 70,670 69 C671 68,671 67,671 66 C671 65,672 64,672 63 C672 62,672 61,672 60 C673 59,673 58,673 57 C673 57,673 57,673 56 C673 56,673 56,673 56 C673 55,673 55,672 55 C672 54,671 54,670 54 L670 52 L678 52 L679 52 L675 69 Z" fill="#212121"/><path d="M406 138 C408 134,411 132,413 130 C415 129,417 128,420 128 C422 128,424 129,425 130 C426 131,427 133,427 136 L427 136 C427 136,427 136,427 137 C429 134,431 132,433 130 C435 129,437 128,439 128 C442 128,443 129,445 130 C446 131,447 133,447 136 C447 137,446 140,445 142 L443 152 C442 155,442 157,442 158 C442 159,442 160,443 160 C443 161,443 161,444 161 C445 161,446 161,446 160 C447 160,448 159,450 157 L452 159 C450 161,448 163,447 164 C445 165,443 165,441 165 C440 165,438 164,437 163 C436 162,436 161,436 159 C436 157,436 155,437 152 L439 145 C439 143,440 141,440 140 C440 139,440 138,440 137 C440 135,440 134,439 133 C439 132,438 132,437 132 C436 132,434 133,433 133 C432 134,431 135,430 137 C428 139,427 140,426 142 C426 143,425 145,424 148 L421 164 L415 164 L419 145 C420 143,420 141,420 140 C420 139,420 138,420 137 C420 135,420 134,419 133 C419 132,418 132,416 132 C416 132,415 133,413 133 C412 134,411 135,410 137 C408 139,407 140,406 142 C406 144,405 146,404 148 L401 164 L395 164 L400 140 C401 138,401 136,401 135 C401 134,401 133,400 133 C400 132,400 132,399 132 C398 132,397 132,396 133 C396 134,394 135,393 136 L391 134 C393 132,395 131,396 130 C398 129,400 128,402 128 C403 128,404 129,405 130 C406 131,407 132,407 133 C407 135,406 136,406 138 L406 138 Z" fill="#212121"/><rect x="156" y="90" width="531" height="5" fill="#212121"/></svg></span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">mean</span><span class="f_CodeExample">&nbsp;=&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">m_cBatchOptions</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Col</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)&nbsp;*&nbsp;((</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">)</span><span class="f_CodeExample" style="color: #333333;">m_iBatchSize</span><span class="f_CodeExample">&nbsp;-&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1.0</span><span class="f_CodeExample">)&nbsp;+&nbsp;</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;inputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">))&nbsp;/&nbsp;(</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">)</span><span class="f_CodeExample" style="color: #333333;">m_iBatchSize</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After determining the moving average, we find the average variance.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">delt</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">0</span><span class="f_CodeExample">)&nbsp;-&nbsp;</span><span class="f_CodeExample" style="color: #333333;">mean</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">variance</span><span class="f_CodeExample">&nbsp;=&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">m_cBatchOptions</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Col</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">1</span><span class="f_CodeExample">)&nbsp;*&nbsp;((</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">)</span><span class="f_CodeExample" style="color: #333333;">m_iBatchSize</span><span class="f_CodeExample">&nbsp;-&nbsp;</span><span class="f_CodeExample" style="color: #333333;">1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">0</span><span class="f_CodeExample">)&nbsp;+</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Functions">MathPow</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">delt</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">2</span><span class="f_CodeExample">))&nbsp;/&nbsp;(</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">)</span><span class="f_CodeExample" style="color: #333333;">m_iBatchSize</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Once the mean and variance values are found, we can easily compute the normalized value of the current element in the sequence.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:100px;height:47px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 400 188"><path d="M17 80 C17 78,17 77,16 75 C16 73,16 71,15 70 C15 68,14 67,14 66 C14 66,14 66,13 65 C13 65,13 65,12 65 C12 65,11 65,11 65 C10 66,10 66,9 67 C9 67,8 68,7 69 L5 67 C7 65,8 64,9 63 C11 62,13 61,14 61 C15 61,16 61,16 61 C17 62,18 62,18 62 C18 63,19 63,19 64 C20 64,20 65,20 66 C21 67,21 68,21 70 C21 71,22 73,22 74 L22 74 C25 71,26 68,27 67 C29 65,30 64,31 63 C31 62,32 62,33 62 C34 61,35 61,36 61 C37 61,38 61,39 62 L37 69 L35 69 C35 68,34 67,34 67 C33 67,33 67,33 67 C33 67,32 67,32 68 C32 68,31 68,30 69 C30 70,29 71,28 72 C27 73,26 74,25 75 L23 78 C23 81,24 83,24 84 C25 86,25 88,25 89 C26 90,26 91,26 92 C26 92,27 93,27 93 C27 94,27 94,28 94 C28 94,28 94,29 94 C29 94,30 94,30 93 C31 93,32 92,33 90 L36 92 C34 94,32 96,31 97 C30 98,28 98,26 98 C25 98,24 98,23 97 C23 97,22 96,21 96 C21 95,20 93,20 92 C19 88,19 86,19 84 L18 84 C16 88,14 91,12 92 C11 94,10 95,9 96 C9 97,8 97,7 98 C6 98,5 98,4 98 C3 98,2 98,1 98 L3 90 L5 90 C5 91,6 92,6 92 C7 92,7 92,8 92 C8 92,8 91,9 90 C10 90,11 89,12 87 C13 86,15 83,17 80 L17 80 Z" fill="#212121"/><path d="M27 44 L35 56 L32 58 L25 49 L25 49 L16 58 L14 56 L22 44 L27 44 Z" fill="#212121"/><path d="M68 71 L68 66 L114 66 L114 71 L68 71 Z M68 86 L68 81 L114 81 L114 86 L68 86 Z" fill="#212121"/><path d="M173 19 C173 17,173 16,172 14 C172 12,172 10,171 9 C171 7,170 6,170 5 C170 5,170 5,169 4 C169 4,169 4,168 4 C168 4,167 4,167 4 C166 5,166 5,165 6 C165 6,164 7,163 8 L161 6 C163 4,164 3,165 2 C167 1,169 0,170 0 C171 0,172 0,172 0 C173 1,174 1,174 1 C174 2,175 2,175 3 C176 3,176 4,176 5 C177 6,177 7,177 9 C177 10,178 12,178 13 L178 13 C181 10,182 7,183 6 C185 4,186 3,187 2 C187 1,188 1,189 1 C190 0,191 0,192 0 C193 0,194 0,195 1 L193 8 L191 8 C191 7,190 6,190 6 C189 6,189 6,189 6 C189 6,188 6,188 7 C188 7,187 7,186 8 C186 9,185 10,184 11 C183 12,182 13,181 14 L179 17 C179 20,180 22,180 23 C181 25,181 27,181 28 C182 29,182 30,182 31 C182 31,183 32,183 32 C183 33,183 33,184 33 C184 33,184 33,185 33 C185 33,186 33,186 32 C187 32,188 31,189 29 L192 31 C190 33,188 35,187 36 C186 37,184 37,182 37 C181 37,180 37,179 36 C179 36,178 35,177 35 C177 34,176 32,176 31 C175 27,175 25,175 23 L174 23 C172 27,170 30,168 31 C167 33,166 34,165 35 C165 36,164 36,163 37 C162 37,161 37,160 37 C159 37,158 37,157 37 L159 29 L161 29 C161 30,162 31,162 31 C163 31,163 31,164 31 C164 31,164 30,165 29 C166 29,167 28,168 26 C169 25,171 22,173 19 L173 19 Z" fill="#212121"/><path d="M207 24 L205 30 L200 30 L201 24 L207 24 Z M200 52 C200 53,200 54,200 55 C200 55,200 56,200 56 C200 57,200 57,200 58 C200 58,201 58,201 58 C202 58,202 58,202 58 C203 58,203 58,203 57 C204 57,204 57,204 56 C205 56,205 55,206 55 L208 57 C207 58,206 59,205 59 C204 60,204 60,203 61 C202 61,202 62,201 62 C200 62,200 62,199 62 C198 62,198 62,197 62 C197 62,196 61,196 61 C195 60,195 60,195 59 C195 59,194 58,194 57 C194 57,195 56,195 55 C195 54,195 53,195 52 C196 51,196 50,196 49 C196 48,197 47,197 46 C197 45,197 44,197 43 C198 42,198 41,198 40 C198 40,198 40,198 39 C198 39,198 39,198 39 C198 38,198 38,197 38 C197 37,196 37,195 37 L195 35 L203 35 L204 35 L200 52 Z" fill="#212121"/><path d="M234 17 L234 12 L280 12 L280 17 L234 17 Z" fill="#212121"/><path d="M328 26 C326 30,324 32,322 34 C320 35,318 36,316 36 C315 36,314 36,313 36 C312 35,312 35,311 34 C310 35,310 37,310 40 C310 42,310 45,310 47 C310 48,310 48,310 49 L309 51 L304 51 L303 49 C304 45,305 41,306 36 L313 1 L320 1 L315 21 C315 22,314 24,314 25 C314 26,314 27,314 28 C314 30,314 30,315 31 C316 32,317 32,318 32 C319 32,321 31,322 30 C324 28,326 26,327 24 C328 22,329 20,330 18 L333 1 L340 1 L334 25 C334 27,334 29,334 30 C334 31,334 32,334 32 C334 33,335 33,336 33 C336 33,337 33,338 32 C338 32,339 31,340 30 L342 32 C341 33,340 34,339 35 C338 36,337 36,336 37 C335 37,334 37,333 37 C331 37,330 37,329 36 C328 35,328 34,328 32 C328 31,328 29,329 27 L328 26 Z" fill="#212121"/><path d="M345 61 C346 61,346 61,346 61 C347 61,347 60,347 60 C348 60,348 59,348 58 C348 58,349 57,349 55 L353 36 C354 35,354 34,354 34 C354 33,354 33,354 32 C354 32,354 31,353 31 C353 30,352 30,351 30 L351 28 L365 28 C368 28,369 28,371 29 C372 29,374 29,374 30 C375 31,376 31,376 32 C377 33,377 34,377 35 C377 40,374 43,369 44 C369 45,369 45,369 45 C369 45,369 45,369 45 C370 46,372 47,373 48 C373 49,374 50,374 52 C374 54,374 55,373 57 C372 58,371 59,370 60 C369 61,367 62,365 62 C363 63,360 63,357 63 L345 63 L345 61 Z M361 43 C363 43,365 43,366 43 C367 42,368 42,369 41 C370 40,371 40,371 39 C371 38,372 37,372 36 C372 35,371 34,371 34 C371 33,371 33,370 32 C369 32,369 32,368 31 C367 31,366 31,364 31 C363 31,363 31,362 31 C361 31,360 31,360 31 L357 43 L361 43 Z M353 60 C354 60,354 60,355 60 C355 60,356 60,357 60 C359 60,361 60,362 60 C364 59,365 59,366 58 C367 57,367 56,368 55 C368 54,368 53,368 51 C368 50,368 50,368 49 C368 48,367 48,367 47 C366 47,365 47,364 46 C364 46,362 46,361 46 L356 46 L353 60 Z" fill="#212121"/><path d="M153 148 L166 176 L188 93 L198 93 L198 98 L191 98 L167 186 L164 186 L149 154 L142 156 L141 154 L153 148 Z" fill="#212121"/><path d="M238 133 L226 133 L226 133 C228 135,229 137,229 138 C230 140,230 142,230 144 C230 148,229 151,228 154 C226 157,224 160,221 161 C218 163,215 164,212 164 C208 164,205 163,202 161 C200 159,199 156,199 151 C199 149,200 146,201 144 C201 141,203 139,204 137 C206 135,208 133,210 132 C212 131,214 130,217 129 C220 128,223 128,228 128 L239 128 L238 133 Z M222 133 C219 133,216 134,213 136 C211 138,209 140,207 143 C206 146,205 150,205 153 C205 155,205 157,206 158 C207 159,207 160,208 160 C209 161,210 161,211 161 C214 161,216 160,218 159 C220 157,221 155,222 151 C223 148,224 145,224 142 C224 138,223 135,222 133 L222 133 Z" fill="#212121"/><path d="M260 136 C260 136,261 136,262 136 C262 136,262 136,263 136 C263 135,263 135,264 135 C264 134,264 134,264 133 L267 133 C267 134,267 135,267 137 C267 138,267 139,266 140 L244 140 L244 139 C244 138,245 137,245 136 C246 135,246 134,247 133 C248 132,249 131,250 130 C250 129,251 128,253 127 C254 125,256 124,257 123 C258 121,259 120,259 119 C260 118,260 118,260 117 C261 116,261 115,261 114 C261 113,261 113,260 112 C260 111,260 110,259 110 C259 109,258 109,257 109 C257 108,256 108,255 108 C253 108,252 109,251 109 C250 110,249 111,248 113 L245 113 L245 108 C247 107,249 106,251 106 C253 105,255 105,256 105 C258 105,260 105,261 106 C262 106,263 107,264 107 C265 108,265 109,266 110 C266 111,266 112,266 113 C266 114,266 115,266 115 C266 116,266 117,266 117 C265 118,265 118,265 119 C264 120,264 120,263 121 C263 122,262 122,262 123 C261 124,260 125,259 125 C258 126,257 127,257 128 C256 129,255 130,254 131 C253 132,252 133,251 134 C251 135,250 135,250 136 L260 136 Z" fill="#212121"/><path d="M237 184 C238 184,238 184,238 184 C239 184,239 183,239 183 C240 183,240 182,240 181 C240 181,241 180,241 178 L245 159 C246 158,246 157,246 157 C246 156,246 156,246 155 C246 155,246 154,245 154 C245 153,244 153,243 153 L243 151 L257 151 C260 151,261 151,263 152 C264 152,266 152,266 153 C267 154,268 154,268 155 C269 156,269 157,269 158 C269 163,266 166,261 167 C261 168,261 168,261 168 C261 168,261 168,261 168 C262 169,264 170,265 171 C265 172,266 173,266 175 C266 177,266 178,265 180 C264 181,263 182,262 183 C261 184,259 185,257 185 C255 186,252 186,249 186 L237 186 L237 184 Z M253 166 C255 166,257 166,258 166 C259 165,260 165,261 164 C262 163,263 163,263 162 C263 161,264 160,264 159 C264 158,263 157,263 157 C263 156,263 156,262 155 C261 155,261 155,260 154 C259 154,258 154,256 154 C255 154,255 154,254 154 C253 154,252 154,252 154 L249 166 L253 166 Z M245 183 C246 183,246 183,247 183 C247 183,248 183,249 183 C251 183,253 183,254 183 C256 182,257 182,258 181 C259 180,259 179,260 178 C260 177,260 176,260 174 C260 173,260 173,260 172 C260 171,259 171,259 170 C258 170,257 170,256 169 C256 169,254 169,253 169 L248 169 L245 183 Z" fill="#212121"/><path d="M321 144 L321 165 L315 165 L315 144 L295 144 L295 139 L315 139 L315 118 L321 118 L321 139 L341 139 L341 144 L321 144 Z" fill="#212121"/><path d="M395 136 L391 136 C391 132,389 130,385 130 C383 130,381 131,379 132 C378 133,377 135,377 138 C377 139,378 141,379 142 C380 143,382 143,384 143 L389 143 L389 147 L385 147 C383 147,382 147,380 147 C379 148,378 148,377 149 C376 149,375 150,375 150 C374 151,373 152,373 152 C373 153,373 154,373 155 C373 157,373 158,374 159 C376 160,377 160,380 160 C381 160,383 160,385 159 C387 159,389 158,391 156 L393 159 C391 161,388 162,386 163 C383 164,381 164,378 164 C375 164,373 164,372 163 C370 162,368 161,368 160 C367 158,366 157,366 155 C366 153,367 151,369 149 C371 147,374 146,378 145 L378 145 C373 143,371 141,371 137 C371 135,372 134,373 132 C374 131,375 130,377 129 C380 128,382 127,385 127 C389 127,392 128,396 129 L395 136 Z" fill="#212121"/><rect x="196" y="93" width="201" height="5" fill="#212121"/><rect x="140" y="73" width="257" height="5" fill="#212121"/></svg></span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">std</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">sqrt</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">variance</span><span class="f_CodeExample">)&nbsp;+&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1e-32</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">nx</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">delt</span><span class="f_CodeExample">&nbsp;/&nbsp;</span><span class="f_CodeExample" style="color: #333333;">std</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Note that we add a small constant to the variance to eliminate the potential zero division error.</span></p>
<p class="p_Text"><span class="f_Text">The next step of the batch normalization algorithm is shift and scaling.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">VECTOR</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">res</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cWeights</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Col</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">0</span><span class="f_CodeExample">)&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">nx</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cWeights</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Col</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">1</span><span class="f_CodeExample">);</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After that, we only need to save the obtained values into the respective elements of the buffers. Please note that we save not only the results of the algorithm operations in the result buffer but also our intermediate values in the normalization parameters buffer. We will need them in subsequent iterations of the algorithm. Do not forget to check the results of the operations.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">res</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cBatchOptions</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Col</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">mean</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cBatchOptions</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Col</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">variance</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cBatchOptions</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Col</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">nx</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">2</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//&nbsp;OpenCL&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">This completes the algorithm splitting depending on the computing device used. As always, we will set a temporary stub for the OpenCL block in the form of a false value return. We will return to this part later.</span></p>
<p class="p_Text"><span class="f_Text">Now, before exiting the method, we activate the values in the result buffer of our class. To do this, we call the </span><span class="f_Text" style="font-style: italic;">Activation</span><span class="f_Text"> method of our special object to work with the </span><span class="f_Text" style="font-style: italic;">m_cActivation</span><span class="f_Text"> activation function. After checking the result of the operation, we terminate the method.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cActivation</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Activation</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">With that, we conclude our work on the feed-forward method of the </span><span class="f_Text" style="font-style: italic;">CNeuronBatchNorm</span><span class="f_Text"> batch normalization class. I hope that understanding the logic behind its construction wasn't difficult for you. Now, let's move on to building the backpropagation methods.</span></p>

</div>

</body>
</html>
