<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>5.1.2.1 Self-Attention feed-forward method</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="5_transformer.htm"> 5. Attention mechanisms </a> / <a class="h_m" href="5_1_self-attention.htm"> 5.1 Self-Attention </a> / <a class="h_m" href="5_1_2_tr_mql5.htm"> 5.1.2 Building Self-Attention with MQL5 tools </a>/ 5.1.2.1 Self-Attention feed-forward method
          </td>
          <td width="70" align="right">
          <a href="5_1_2_tr_mql5.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="5_1_2_2_tr_backprop.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H3"><span class="f_H3">5.1.2.1 Self-Attention feed-forward method</span></p>
<p class="p_Text"><span class="f_Text">We have already created the structure of a class organization for implementing the attention mechanism and even created an object initialization method. In this section, we will organize the forward pass process.</span></p>
<p class="p_Text"><span class="f_Text">As you know, in the base class of the neural network, we have created a virtual method </span><span class="f_Text" style="font-style: italic;"><a href="3_6_2_pr_mql5.htm#feedforward" class="topiclink">CNeuronBase::FeedForward</a></span><span class="f_Text"> which is responsible for organizing the feed-forward pass. In each new class, we override this method to organize the relevant process according to the algorithm of the implemented architectural solution. By doing so, we kind of personalize the method for each class. At the same time, the external program does not need to know anything about the organization of the process within the class. It doesn't even need to know the type of neural layer. It simply calls the </span><span class="f_Text" style="font-style: italic;">FeedForward</span><span class="f_Text"> method of the next object and passes it a pointer to the previous layer of the neural network. In this way, we have shifted the functionality of dispatching and checking the required object type from our program to the system.</span></p>
<p class="p_Text"><span class="f_Text">Let's go back to our </span><span class="f_Text" style="font-style: italic;">CNeuronAttention::FeedForward</span><span class="f_Text"> method. Just like the method of the parent class, in parameters it receives a pointer to the object of the previous layer. This is consistent with the principles of method inheritance and overriding. Since we receive a pointer to an object, it is customary to begin the method with a block to check the validity of the received pointer. However, in this case, we will omit it. The reason is that the use of static internal objects allows us to refuse to check their pointers. Regarding the pointer to the previous neural layer, we will use it for the feed-forward pass of the internal convolutional neural layers </span><span class="f_Text" style="font-style: italic;">m_cQuerys</span><span class="f_Text">, </span><span class="f_Text" style="font-style: italic;">m_cKeys</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">m_cValues</span><span class="f_Text">. They already have the relevant controls and thus we do not need to duplicate them.</span></p>
<p class="p_Text"><span class="f_Text">In accordance with the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm, we need to define the </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text">, </span><span class="f_Text" style="font-style: italic;">Key</span><span class="f_Text">, and </span><span class="f_Text" style="font-style: italic;">Value </span><span class="f_Text">vectors for each element of the sequence. As you remember, it was for this functionality that we created the first three convolutional layers. Therefore, to solve this problem, we just need to call the </span><span class="f_Text" style="font-style: italic;">FeedForward</span><span class="f_Text"> methods for the named internal layers. With each call in the parameters, we pass a pointer to the previous neural layer obtained in the parameters of our </span><span class="f_Text" style="font-style: italic;">CNeuronAttention::FeedForward</span><span class="f_Text"> method.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next in the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm, we need to determine the dependency coefficients and fill in the Score matrix. At this point, it's essential to recall our paradigm of creating classes capable of running both on the CPU and using the GPU tools. Each time we build a new process, we create a branching of the algorithm depending on the computing device in use. This method will not be an exception, and we will continue to work in the same direction. Right now, we will create a similar branching of the process. We will start with the process using MQL5 tools and will return to the OpenCL branch a little later.</span></p>
<p class="p_Text"><span class="f_Text">For convenience, we copy the </span><span class="f_Text" style="font-style: italic;">m_cQuerys</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">m_cKeys</span><span class="f_Text"> matrices which contain the results of the convolutional layers.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;branching&nbsp;of&nbsp;the&nbsp;algorithm&nbsp;by&nbsp;the&nbsp;computing&nbsp;device</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">out</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After completing the preparatory work, we need to &quot;roll up our sleeves&quot; and build a new process. The </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> method involves line-wise normalization of the dependency matrix using the </span><span class="f_Text" style="font-style: italic;">Softmax</span><span class="f_Text"> function.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:183px;height:70px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 732 280"><path d="M6 105 C6 108,7 110,8 112 C10 113,12 114,15 114 C18 114,21 113,23 111 C24 109,25 107,25 103 C25 102,25 100,25 99 C24 98,23 97,22 96 C21 95,20 94,18 92 C16 91,14 90,13 88 C12 87,11 86,10 85 C10 83,9 81,9 80 C9 77,10 74,11 72 C13 70,15 68,17 67 C20 66,23 65,26 65 C28 65,30 65,32 66 C34 66,36 66,39 67 L37 76 L33 76 C33 74,33 73,32 71 C32 70,31 69,30 69 C29 68,27 68,26 68 C24 68,22 69,20 69 C19 70,18 71,17 73 C16 74,16 76,16 78 C16 79,16 81,17 82 C18 84,20 86,23 87 C25 89,27 91,28 92 C29 93,30 95,31 96 C32 98,32 100,32 101 C32 105,31 107,30 110 C28 112,26 114,24 115 C21 116,18 117,15 117 C13 117,10 117,8 117 C5 116,3 116,1 115 L3 105 L6 105 Z" fill="#212121"/><path d="M56 117 C52 117,49 116,47 114 C45 112,44 109,44 104 C44 103,44 100,45 98 C46 94,47 91,49 88 C51 86,53 84,55 82 C58 81,61 80,64 80 C68 80,71 81,73 83 C75 86,76 89,76 93 C76 96,75 99,74 102 C73 105,72 108,70 110 C69 113,67 114,64 115 C62 117,59 117,56 117 L56 117 Z M50 106 C50 109,51 111,52 112 C53 113,55 114,57 114 C60 114,62 113,64 111 C65 109,67 106,68 102 C69 98,70 94,70 91 C70 89,69 87,68 85 C67 84,65 83,63 83 C61 83,58 84,57 86 C55 89,53 92,52 96 C51 100,50 103,50 106 L50 106 Z" fill="#212121"/><path d="M96 117 C95 122,93 126,91 128 C88 131,85 132,82 132 C81 132,80 132,79 132 L80 129 C80 129,81 129,82 129 C83 129,84 129,85 128 C85 128,86 127,87 125 C87 124,88 122,89 120 L97 85 L90 85 L91 83 C93 83,94 83,94 83 C95 83,96 82,96 82 C97 82,97 81,97 81 C97 80,98 79,98 77 C100 73,102 70,104 67 C107 65,110 64,115 64 C117 64,119 64,121 65 L119 71 L116 71 C116 70,115 69,115 68 C114 67,113 67,112 67 C111 67,110 67,109 68 C108 69,107 70,106 71 C106 73,105 75,104 77 L103 81 L113 81 L112 85 L103 85 L96 117 Z" fill="#212121"/><path d="M146 109 C144 112,142 114,140 115 C138 116,136 117,134 117 C129 117,127 115,127 109 C127 108,127 106,127 104 L131 85 L125 85 L126 83 C127 83,128 83,129 83 C130 83,130 82,131 82 C131 82,132 81,132 80 C133 80,133 79,134 78 C134 77,135 75,135 72 L140 72 L138 81 L149 81 L149 85 L138 85 L134 99 C134 102,133 104,133 106 C133 107,133 108,133 108 C133 112,134 113,137 113 C138 113,139 113,140 112 C141 111,143 109,144 107 L146 109 Z" fill="#212121"/><path d="M201 66 L213 66 L212 68 C211 68,211 68,210 69 C210 69,210 69,209 70 C209 70,209 71,208 72 C208 73,208 75,207 77 L201 105 C200 107,200 108,200 109 C200 110,200 110,200 111 C200 112,200 113,201 113 C201 114,202 114,204 114 L203 116 L189 116 L189 114 C190 114,191 114,191 113 C192 113,192 113,192 112 C193 111,193 111,193 110 C193 109,194 107,194 105 L199 83 C200 80,201 77,201 74 L201 74 C200 75,199 78,196 83 L179 110 L175 110 L172 86 C172 83,171 79,171 73 L170 73 C169 79,168 84,167 88 L163 105 C163 108,163 109,163 111 C163 112,163 113,163 113 C164 114,165 114,166 114 L166 116 L153 116 L153 114 C154 114,155 114,156 113 C156 113,156 113,157 112 C157 111,157 111,158 110 C158 109,158 107,159 105 L165 77 C165 76,165 75,166 74 C166 73,166 72,166 71 C166 70,166 69,165 69 C164 68,163 68,162 68 L163 66 L176 66 L180 101 L201 66 Z" fill="#212121"/><path d="M246 83 L250 80 L252 81 L247 105 C246 107,246 109,246 110 C246 111,246 112,246 112 C247 113,247 113,248 113 C249 113,250 113,250 112 C251 112,252 111,254 109 L256 111 C254 113,252 115,250 116 C249 117,247 117,245 117 C244 117,243 117,242 116 C241 115,240 113,240 112 C240 111,241 109,241 108 L241 107 C238 111,236 113,234 115 C232 116,230 117,227 117 C225 117,223 116,221 114 C220 112,219 109,219 105 C219 101,220 97,221 93 C223 89,225 86,228 84 C231 81,234 80,238 80 C239 80,241 80,242 81 C244 81,245 82,246 83 L246 83 Z M243 94 C243 93,244 92,244 91 C244 90,244 90,244 89 C244 87,243 85,243 84 C242 84,240 83,238 83 C236 83,234 84,232 86 C230 88,228 91,227 95 C226 98,225 102,225 105 C225 108,226 110,226 111 C227 112,228 113,230 113 C232 113,233 112,235 111 C236 110,238 107,239 105 C241 102,242 99,243 95 L243 94 Z" fill="#212121"/><path d="M274 99 C274 97,274 96,273 94 C273 92,273 90,272 89 C272 87,271 86,271 85 C271 85,271 85,270 84 C270 84,270 84,269 84 C269 84,268 84,268 84 C267 85,267 85,266 86 C266 86,265 87,264 88 L262 86 C264 84,265 83,266 82 C268 81,270 80,271 80 C272 80,273 80,273 80 C274 81,275 81,275 81 C275 82,276 82,276 83 C277 83,277 84,277 85 C278 86,278 87,278 89 C278 90,279 92,279 93 L279 93 C282 90,283 87,284 86 C286 84,287 83,288 82 C288 81,289 81,290 81 C291 80,292 80,293 80 C294 80,295 80,296 81 L294 88 L292 88 C292 87,291 86,291 86 C290 86,290 86,290 86 C290 86,289 86,289 87 C289 87,288 87,287 88 C287 89,286 90,285 91 C284 92,283 93,282 94 L280 97 C280 100,281 102,281 103 C282 105,282 107,282 108 C283 109,283 110,283 111 C283 111,284 112,284 112 C284 113,284 113,285 113 C285 113,285 113,286 113 C286 113,287 113,287 112 C288 112,289 111,290 109 L293 111 C291 113,289 115,288 116 C287 117,285 117,283 117 C282 117,281 117,280 116 C280 116,279 115,278 115 C278 114,277 112,277 111 C276 107,276 105,276 103 L275 103 C273 107,271 110,269 111 C268 113,267 114,266 115 C266 116,265 116,264 117 C263 117,262 117,261 117 C260 117,259 117,258 117 L260 109 L262 109 C262 110,263 111,263 111 C264 111,264 111,265 111 C265 111,265 110,266 109 C267 109,268 108,269 106 C270 105,272 102,274 99 L274 99 Z" fill="#212121"/><path d="M312 98 C312 106,313 114,316 119 C319 125,323 128,329 130 L328 133 C321 131,315 127,312 121 C308 115,306 107,306 98 C306 89,308 82,312 75 C315 69,321 65,328 63 L329 66 C323 68,319 72,316 77 C313 83,312 89,312 98 L312 98 Z" fill="#212121"/><path d="M347 99 C347 97,347 96,346 94 C346 92,346 90,345 89 C345 87,344 86,344 85 C344 85,344 85,343 84 C343 84,343 84,342 84 C342 84,341 84,341 84 C340 85,340 85,339 86 C339 86,338 87,337 88 L335 86 C337 84,338 83,339 82 C341 81,343 80,344 80 C345 80,346 80,346 80 C347 81,348 81,348 81 C348 82,349 82,349 83 C350 83,350 84,350 85 C351 86,351 87,351 89 C351 90,352 92,352 93 L352 93 C355 90,356 87,357 86 C359 84,360 83,361 82 C361 81,362 81,363 81 C364 80,365 80,366 80 C367 80,368 80,369 81 L367 88 L365 88 C365 87,364 86,364 86 C363 86,363 86,363 86 C363 86,362 86,362 87 C362 87,361 87,360 88 C360 89,359 90,358 91 C357 92,356 93,355 94 L353 97 C353 100,354 102,354 103 C355 105,355 107,355 108 C356 109,356 110,356 111 C356 111,357 112,357 112 C357 113,357 113,358 113 C358 113,358 113,359 113 C359 113,360 113,360 112 C361 112,362 111,363 109 L366 111 C364 113,362 115,361 116 C360 117,358 117,356 117 C355 117,354 117,353 116 C353 116,352 115,351 115 C351 114,350 112,350 111 C349 107,349 105,349 103 L348 103 C346 107,344 110,342 111 C341 113,340 114,339 115 C339 116,338 116,337 117 C336 117,335 117,334 117 C333 117,332 117,331 117 L333 109 L335 109 C335 110,336 111,336 111 C337 111,337 111,338 111 C338 111,338 110,339 109 C340 109,341 108,342 106 C343 105,345 102,347 99 L347 99 Z" fill="#212121"/><path d="M381 104 L379 110 L374 110 L375 104 L381 104 Z M374 132 C374 133,374 134,374 135 C374 135,374 136,374 136 C374 137,374 137,374 138 C374 138,375 138,375 138 C376 138,376 138,376 138 C377 138,377 138,377 137 C378 137,378 137,378 136 C379 136,379 135,380 135 L382 137 C381 138,380 139,379 139 C378 140,378 140,377 141 C376 141,376 142,375 142 C374 142,374 142,373 142 C372 142,372 142,371 142 C371 142,370 141,370 141 C369 140,369 140,369 139 C369 139,368 138,368 137 C368 137,369 136,369 135 C369 134,369 133,369 132 C370 131,370 130,370 129 C370 128,371 127,371 126 C371 125,371 124,371 123 C372 122,372 121,372 120 C372 120,372 120,372 119 C372 119,372 119,372 119 C372 118,372 118,371 118 C371 117,370 117,369 117 L369 115 L377 115 L378 115 L374 132 Z" fill="#212121"/><path d="M404 98 C404 89,403 83,400 77 C397 72,393 68,387 66 L388 63 C395 65,401 69,404 75 C408 82,410 89,410 98 C410 107,408 115,404 121 C401 127,395 131,388 133 L387 130 C393 128,397 125,400 119 C403 114,404 106,404 98 L404 98 Z" fill="#212121"/><path d="M443 90 L443 85 L489 85 L489 90 L443 90 Z M443 105 L443 100 L489 100 L489 105 L443 105 Z" fill="#212121"/><path d="M607 52 C604 55,602 57,599 58 C597 59,594 60,591 60 C587 60,585 59,583 57 C581 55,580 52,580 48 C580 45,580 42,581 39 C582 36,584 33,586 31 C587 28,590 26,592 25 C595 24,598 23,601 23 C604 23,606 24,608 25 C609 26,610 28,610 31 C610 35,608 38,604 40 C600 42,594 43,587 43 C586 45,586 46,586 48 C586 51,587 53,588 54 C589 55,590 56,593 56 C595 56,597 56,599 55 C600 54,602 52,604 50 L607 52 Z M587 40 C591 40,594 40,596 39 C599 39,600 38,602 36 C603 35,604 33,604 31 C604 29,603 28,603 27 C602 27,601 26,600 26 C597 26,595 27,592 30 C590 32,588 36,587 40 L587 40 Z" fill="#212121"/><path d="M631 14 C631 13,631 12,630 10 C630 9,630 9,630 8 C630 7,629 7,629 6 C629 6,629 5,628 5 C628 5,628 4,628 4 C628 4,627 4,627 4 C627 4,626 4,626 4 C626 4,626 5,625 5 C625 5,625 5,624 6 C624 6,624 7,623 8 L621 6 C622 5,623 4,623 3 C624 3,624 2,625 1 C626 1,626 1,627 0 C627 0,628 0,629 0 C630 0,631 0,631 1 C632 1,633 2,633 3 C633 3,634 4,634 6 C634 7,635 8,635 9 L635 9 C636 8,637 7,638 6 C639 5,639 4,640 3 C641 3,641 2,642 2 C642 1,643 1,643 1 C644 1,644 0,645 0 C645 0,646 0,646 0 C647 0,648 0,649 1 L647 7 L645 7 C645 6,645 5,644 5 C644 5,644 5,644 5 C643 5,643 5,643 6 C642 6,642 6,642 6 C641 7,641 7,640 8 C640 8,639 9,638 10 C637 11,637 12,636 13 C636 15,637 17,637 18 C637 20,638 21,638 21 C638 22,639 23,639 23 C639 23,640 23,640 23 C640 23,641 23,641 23 C641 23,641 23,642 23 C642 22,642 22,643 21 C643 21,643 20,644 20 L646 21 C645 22,644 23,644 24 C643 25,643 26,642 26 C641 26,641 27,640 27 C640 27,639 27,638 27 C637 27,637 27,636 27 C636 27,635 26,635 26 C635 26,634 25,634 25 C634 24,634 24,633 23 C633 23,633 22,633 21 C633 21,633 20,633 20 C632 19,632 19,632 18 C632 18,632 17,632 17 L632 17 C631 19,630 20,629 21 C628 22,628 23,627 23 C626 24,626 25,625 25 C625 26,624 26,624 26 C623 27,623 27,622 27 C622 27,621 27,620 27 C619 27,619 27,618 27 L619 21 L621 21 C621 22,622 23,622 23 C623 23,623 23,623 23 C623 23,623 23,624 23 C624 23,624 22,625 22 C625 21,625 21,626 20 C627 20,627 19,628 18 C629 17,630 16,631 14 L631 14 Z" fill="#212121"/><path d="M661 18 L659 23 L655 23 L656 18 L661 18 Z M655 42 C655 43,655 44,655 44 C655 45,655 45,655 46 C655 47,655 47,656 47 C657 47,657 47,658 46 C659 46,659 45,660 44 L662 46 C660 47,659 48,658 49 C657 50,655 50,654 50 C653 50,652 50,651 49 C651 48,650 47,650 46 C650 45,650 43,651 41 L653 34 C653 33,653 32,653 32 C653 31,653 31,653 31 C653 30,653 30,652 29 C652 29,651 29,650 29 L651 27 L657 27 L659 27 L655 42 Z" fill="#212121"/><path d="M551 156 L568 183 L568 186 L549 213 L573 213 C574 213,575 213,576 213 C577 213,578 212,578 212 C579 211,579 211,580 210 C580 209,580 208,581 207 C581 206,582 204,582 202 L586 202 L585 220 L539 220 L539 218 L561 187 L540 154 L540 152 L585 152 L585 168 L581 168 C580 166,580 164,579 163 C579 161,578 160,578 159 C577 159,577 158,576 157 C575 157,575 157,574 156 C573 156,572 156,571 156 L551 156 Z" fill="#212121"/><path d="M561 114 C563 112,564 110,566 109 C568 108,570 107,571 107 C573 107,575 108,576 109 C577 110,577 111,577 113 C577 113,577 114,577 115 C577 116,576 117,576 118 C576 119,576 120,575 121 C575 122,575 124,574 124 C574 125,574 126,574 127 C574 128,574 128,574 128 C574 129,574 129,574 130 C574 130,575 130,575 130 C576 130,576 130,576 130 C577 130,577 130,577 129 C578 129,578 129,578 128 C579 128,579 127,580 127 L582 128 C581 130,580 130,579 131 C579 132,578 132,577 133 C577 133,576 134,575 134 C574 134,574 134,573 134 C572 134,572 134,571 134 C571 134,570 133,570 133 C569 132,569 132,569 131 C569 131,569 130,569 129 C569 129,569 128,569 127 C569 126,569 125,569 124 C570 123,570 122,570 121 C571 120,571 119,571 118 C571 117,571 117,572 116 C572 115,572 115,572 114 C572 113,572 112,571 112 C571 111,570 111,569 111 C569 111,568 111,567 111 C567 112,566 112,565 113 C565 113,564 114,564 114 C563 115,562 116,562 116 C561 117,561 118,561 119 C560 120,560 120,560 121 L557 134 L552 134 L556 117 C556 116,556 116,556 115 C556 114,556 113,556 113 C556 112,556 111,555 111 C554 111,554 111,554 111 C553 111,553 112,553 112 C552 112,552 113,551 113 C551 113,551 114,550 114 L548 113 C549 112,550 111,551 110 C551 109,552 109,553 108 C553 108,554 108,555 107 C555 107,556 107,557 107 C558 107,559 108,560 108 C561 109,561 110,561 112 C561 112,561 113,561 113 C561 113,561 114,561 114 L561 114 Z" fill="#212121"/><path d="M539 227 L538 233 L532 233 L533 227 L539 227 Z M531 264 C530 267,530 268,529 270 C528 271,527 272,526 273 C525 274,524 275,523 275 C521 276,520 276,518 276 C518 276,517 276,517 276 C516 276,516 276,516 276 L516 273 C517 273,517 273,517 273 C518 273,518 273,518 273 C519 273,520 273,521 273 C522 272,522 272,523 271 C523 271,524 270,524 269 C525 268,525 266,525 265 C526 262,527 259,527 257 C528 254,528 252,529 250 C529 249,529 247,529 246 C530 245,530 244,530 244 C530 243,530 243,530 243 C530 242,530 242,530 242 C530 241,530 241,529 241 C529 240,528 240,527 240 L527 238 L534 238 L536 238 L531 264 Z" fill="#212121"/><path d="M548 245 L548 241 L578 241 L578 245 L548 245 Z M548 257 L548 253 L578 253 L578 257 L548 257 Z" fill="#212121"/><path d="M603 258 C603 259,603 259,603 260 C603 260,603 261,603 261 C603 261,603 261,603 262 C603 262,604 262,604 262 C604 262,604 262,605 263 C605 263,605 263,606 263 C606 263,607 263,608 263 C609 263,610 263,611 263 L611 265 L589 265 L589 263 C590 263,591 263,592 263 C593 263,593 263,594 263 C594 263,595 263,595 263 C596 262,596 262,596 262 C596 262,597 262,597 262 C597 262,597 261,597 261 C597 261,597 260,597 260 C597 260,597 259,597 258 L597 239 C597 239,597 238,597 238 C597 238,596 237,596 237 C595 237,595 238,593 238 C592 239,591 240,589 241 C589 240,589 240,589 240 C588 239,588 239,588 238 C590 237,592 236,595 235 C597 234,599 232,601 231 L603 231 C603 232,603 233,603 234 C603 235,603 235,603 236 C603 237,603 237,603 238 L603 258 Z" fill="#212121"/><path d="M659 197 C656 200,654 202,651 203 C649 204,646 205,643 205 C639 205,637 204,635 202 C633 200,632 197,632 193 C632 190,632 187,633 184 C634 181,636 178,638 176 C639 173,642 171,644 170 C647 169,650 168,653 168 C656 168,658 169,660 170 C661 171,662 173,662 176 C662 180,660 183,656 185 C652 187,646 188,639 188 C638 190,638 191,638 193 C638 196,639 198,640 199 C641 200,642 201,645 201 C647 201,649 201,651 200 C652 199,654 197,656 195 L659 197 Z M639 185 C643 185,646 185,648 184 C651 184,652 183,654 181 C655 180,656 178,656 176 C656 174,655 173,655 172 C654 172,653 171,652 171 C649 171,647 172,644 175 C642 177,640 181,639 185 L639 185 Z" fill="#212121"/><path d="M683 149 C683 148,683 147,682 145 C682 144,682 144,682 143 C682 142,681 142,681 141 C681 141,681 140,680 140 C680 140,680 139,680 139 C680 139,679 139,679 139 C679 139,678 139,678 139 C678 139,678 140,677 140 C677 140,677 140,676 141 C676 141,676 142,675 143 L673 141 C674 140,675 139,675 138 C676 138,676 137,677 136 C678 136,678 136,679 135 C679 135,680 135,681 135 C682 135,683 135,683 136 C684 136,685 137,685 138 C685 138,686 139,686 141 C686 142,687 143,687 144 L687 144 C688 143,689 142,690 141 C691 140,691 139,692 138 C693 138,693 137,694 137 C694 136,695 136,695 136 C696 136,696 135,697 135 C697 135,698 135,698 135 C699 135,700 135,701 136 L699 142 L697 142 C697 141,697 140,696 140 C696 140,696 140,696 140 C695 140,695 140,695 141 C694 141,694 141,694 141 C693 142,693 142,692 143 C692 143,691 144,690 145 C689 146,689 147,688 148 C688 150,689 152,689 153 C689 155,690 156,690 156 C690 157,691 158,691 158 C691 158,692 158,692 158 C692 158,693 158,693 158 C693 158,693 158,694 158 C694 157,694 157,695 156 C695 156,695 155,696 155 L698 156 C697 157,696 158,696 159 C695 160,695 161,694 161 C693 161,693 162,692 162 C692 162,691 162,690 162 C689 162,689 162,688 162 C688 162,687 161,687 161 C687 161,686 160,686 160 C686 159,686 159,685 158 C685 158,685 157,685 156 C685 156,685 155,685 155 C684 154,684 154,684 153 C684 153,684 152,684 152 L684 152 C683 154,682 155,681 156 C680 157,680 158,679 158 C678 159,678 160,677 160 C677 161,676 161,676 161 C675 162,675 162,674 162 C674 162,673 162,672 162 C671 162,671 162,670 162 L671 156 L673 156 C673 157,674 158,674 158 C675 158,675 158,675 158 C675 158,675 158,676 158 C676 158,676 157,677 157 C677 156,677 156,678 155 C679 155,679 154,680 153 C681 152,682 151,683 149 L683 149 Z" fill="#212121"/><path d="M722 153 L721 158 L716 158 L718 153 L722 153 Z M715 185 C715 189,714 191,712 193 C710 194,708 195,705 195 C704 195,703 195,703 195 L703 192 C704 192,704 192,705 192 C706 192,707 192,708 191 C708 191,709 190,709 189 C710 188,710 187,711 185 L714 169 C715 167,715 166,715 166 C715 165,715 165,714 164 C714 164,713 164,712 164 L712 162 L719 162 L720 162 L715 185 Z" fill="#212121"/><rect x="515" y="92" width="212" height="5" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">The main feature of such normalization lies in obtaining a series of positive values that sum up to 1. Thus, by multiplying the normalized dependency coefficients with the values of </span><span class="f_Text" style="font-style: italic;">Value</span><span class="f_Text"> vectors of the corresponding sequence elements and then summing up these vectors within one </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text">, we expect to obtain new vectors within the same range of values.</span></p>
<p class="p_Text"><span class="f_Text">Let's look at the implementation of this process. First, we organize the process of calculating the dependency coefficients into the </span><span class="f_Text" style="font-style: italic;">Score</span><span class="f_Text"> matrix. According to the </span><span class="f_Text" style="font-style: italic;">Self</span><span class="f_Text">-</span><span class="f_Text" style="font-style: italic;">Attention</span><span class="f_Text"> algorithm, each element of the matrix represents the product of the </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">Key</span><span class="f_Text"> vectors. In this case, the matrix row indicates the position of the vector in the </span><span class="f_Text" style="font-style: italic;">Queries</span><span class="f_Text"> matrix and its column indicates the position in the </span><span class="f_Text" style="font-style: italic;">Keys</span><span class="f_Text"> matrix.</span></p>
<p class="p_Parameters"><span class="f_Parameters">Here, it is important to carefully consider the choice of elements to be multiplied. Let's recall how we organized the output of the results to the buffer of the convolutional layer. To enable the operation of the pooling layer in the context of filters, we have organized the sequential output of filters. First, in the first row of the result buffer matrix, we output all the elements of the result of one filter. Then, in the next row, we write the elements of the next filter, and so on. This organization of the buffer is convenient for the transparent operation of the pooling layer within the filters. In this case, within the vector of one element of the sequence, we need to use one value from each filter. In other words, we need a transposed matrix.</span></p>
<p class="p_Parameters"><span class="f_Text">Reorganizing the buffer data in such a way that the first elements of all filters come first, then the second elements of all filters, and so on, would require additional resources on each feed-forward pass. It would be much easier to organize a convenient record directly in the convolutional layer. However, this would disrupt the operation of both the pooling layer and subsequent convolutional layers when building convolutional models. Therefore, it was decided to introduce a flag into the operation of the convolutional layer to determine whether the values should be arranged in the result buffer. You may have already guessed this when I talked about the new </span><span class="f_Text" style="font-style: italic;">SetTransposedOutput</span><span class="f_Parameters"> convolutional layer method when describing the initialization method.</span><span class="f_Text"> I promised to return to the description of the functionality of this method. Such a solution has helped us keep the structure of the feed-forward pass method transparent and avoid additional time and resource costs for data reorganization. Let's finish working with the feed-forward pass method, and then we can revisit the changes in the convolutional layer.</span></p>
<p class="p_Text"><span class="f_Text">Taking into account the transposition of the convolutional layer results, to obtain the values of the matrix of dependency coefficients, we need to multiply the </span><span class="f_Text" style="font-style: italic;">Querys</span><span class="f_Text"> matrix by the transposed matrix </span><span class="f_Text" style="font-style: italic;">Keys</span><span class="f_Text">. It sounds a little strange to transpose the work of the convolutional layer method and then transpose the </span><span class="f_Text" style="font-style: italic;">Keys </span><span class="f_Text">matrix. However, we will use the result of transposing the work of the convolutional layer more than once. Of course, with the help of the entered flag, we could transpose the work of the convolutional layer </span><span class="f_Text" style="font-style: italic;">m_cQuerys</span><span class="f_Text">, and leave the </span><span class="f_Text" style="font-style: italic;">m_cKeys</span><span class="f_Text"> layer unchanged. But in this case, there is a possibility of confusion with the matrix dimensions. This will make the code more difficult to read and understand. Therefore, I decided to unify the dimensions of the matrices used.</span></p>
<p class="p_Text"><span class="f_Text">Please note that simultaneously with the calculation of the vector product, we will prepare data for normalization according to the </span><span class="f_Text" style="font-style: italic;">Softmax</span><span class="f_Text"> formula above. For this purpose, we will immediately divide the obtained matrix by the square root of the </span><span class="f_Text" style="font-style: italic;">Key</span><span class="f_Text"> vector size and take the exponent of the resulting value.</span></p>
<p class="p_Text"><span class="f_Text">Then we will take the row-wise sum of the matrix values and divide the values by the resulting vector of the matrix </span><span class="f_Text" style="font-style: italic;">Scores.</span><span class="f_Text"> MQL5 matrix operations do not allow you to divide a matrix by a vector. Therefore, we will organize a loop in which we will sequentially divide each row by the sum of its values.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;define&nbsp;Scores</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">scores</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">MathExp</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">querys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">())&nbsp;/&nbsp;</span><span class="f_Functions">sqrt</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">));</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;normalize&nbsp;Scores</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">summs</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">scores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Sum</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">1</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">r</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">r</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">r</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">scores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">scores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">r</span><span class="f_CodeExample">)&nbsp;/&nbsp;</span><span class="f_CodeExample" style="color: #333333;">summs</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">r</span><span class="f_CodeExample">],&nbsp;</span><span class="f_CodeExample" style="color: #333333;">r</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cScores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">scores</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After normalizing the data in the matrix containing the dependencies coefficients of the elements in the sequence, we will transfer these values to our data buffer </span><span class="f_Text" style="font-style: italic;">buffer m_cScores</span><span class="f_Text">.</span></p>
<p class="p_Text"><span class="f_Text">At this stage, we have computed and normalized the dependency coefficients between all elements of the sequence. Now, according to the algorithm of the Self-Attention method, we need to calculate the weighted sum of the </span><span class="f_Text" style="font-style: italic;">Values</span><span class="f_Text"> vectors in terms of each </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text">. To do this, we just need to multiply the matrix of dependency coefficients by the matrix of results of the convolutional layer </span><span class="f_Text" style="font-style: italic;">m_cValues</span><span class="f_Text">. Again, it is precisely because of the transposition of the work of the convolutional layer that we do not transpose the matrix of the results of the </span><span class="f_Text" style="font-style: italic;">m_cValues</span><span class="f_Text"> layer.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//</span><span class="f_CodeExample" style="color: #808080;">---&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;attention&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">values</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">out</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">scores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">values</span><span class="f_CodeExample">);</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">The product of the matrices will give us the result of the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> mechanism. But we will go a little further and build the entire Encoder block of the transformer. According to his algorithm, the results of</span><span class="f_Text" style="font-style: italic;"> Self-Attention</span><span class="f_Text"> are added to the buffer of the original data. The obtained values are normalized within the neural layer. The following formulas are used to normalize the data.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:91px;height:47px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 364 188"><path d="M31 86 L35 83 L37 84 L32 108 C31 110,31 112,31 113 C31 114,31 115,31 115 C32 116,32 116,33 116 C34 116,35 116,35 115 C36 115,37 114,39 112 L41 114 C39 116,37 118,35 119 C34 120,32 120,30 120 C29 120,28 120,27 119 C26 118,25 116,25 115 C25 114,26 112,26 111 L26 110 C23 114,21 116,19 118 C17 119,15 120,12 120 C10 120,8 119,6 117 C5 115,4 112,4 108 C4 104,5 100,6 96 C8 92,10 89,13 87 C16 84,19 83,23 83 C24 83,26 83,27 84 C29 84,30 85,31 86 L31 86 Z M28 97 C28 96,29 95,29 94 C29 93,29 93,29 92 C29 90,28 88,28 87 C27 87,25 86,23 86 C21 86,19 87,17 89 C15 91,13 94,12 98 C11 101,10 105,10 108 C10 111,11 113,11 114 C12 115,13 116,15 116 C17 116,18 115,20 114 C21 113,23 110,24 108 C26 105,27 102,28 98 L28 97 Z" fill="#212121"/><path d="M35 75 L16 75 L16 71 L35 71 L35 75 Z" fill="#212121"/><path d="M70 93 L70 88 L116 88 L116 93 L70 93 Z M70 108 L70 103 L116 103 L116 108 L70 108 Z" fill="#212121"/><path d="M168 52 C168 54,168 55,168 55 C168 56,169 57,169 57 C169 58,170 58,171 58 C172 59,172 59,174 59 C175 59,176 59,178 59 L178 62 L151 62 L151 59 C154 59,156 59,157 59 C158 58,159 58,159 58 C160 57,160 57,161 56 C161 55,161 54,161 52 L161 22 C161 21,161 21,161 20 C160 20,160 20,159 20 C158 20,157 20,156 21 C154 22,153 23,151 24 L149 21 L166 11 L168 11 C168 13,168 17,168 21 L168 52 Z" fill="#212121"/><path d="M155 127 C155 125,156 123,156 122 C156 121,155 120,155 120 C154 119,153 119,151 119 L152 117 L161 117 L163 117 L157 141 L157 142 C160 139,162 136,164 135 C166 134,168 133,170 133 C172 133,174 134,175 135 C177 136,177 138,177 141 C177 142,177 145,176 147 L174 157 C173 160,173 162,173 163 C173 164,173 165,173 165 C174 166,174 166,175 166 C176 166,176 166,177 165 C178 165,179 164,181 162 L183 164 C180 166,178 168,177 169 C175 170,174 170,172 170 C170 170,169 169,168 168 C167 167,167 166,167 164 C167 162,167 160,168 157 L169 150 C170 148,170 146,171 145 C171 144,171 143,171 142 C171 140,171 139,170 138 C170 137,169 137,167 137 C166 137,165 138,164 138 C163 139,162 141,160 142 C159 144,158 146,157 147 C156 149,156 151,155 153 L152 169 L145 169 L155 127 Z" fill="#212121"/><rect x="142" y="95" width="42" height="5" fill="#212121"/><path d="M230 59 L254 94 L254 97 L227 133 L260 133 C263 133,265 132,266 130 C268 129,269 126,269 122 L273 122 L272 140 L217 140 L217 138 L247 98 L219 57 L219 55 L273 55 L273 72 L269 72 C268 68,267 65,265 63 C263 60,260 59,256 59 L230 59 Z" fill="#212121"/><path d="M252 27 C252 28,252 28,251 29 C251 29,251 30,251 30 C251 30,251 30,251 31 C251 31,251 31,251 31 C251 32,251 32,251 33 C252 33,252 33,253 33 C253 33,253 33,254 33 C254 33,254 33,255 32 C255 32,255 32,256 31 C256 31,257 30,257 30 C258 30,258 30,258 31 C259 31,259 31,259 31 C258 33,257 33,257 34 C256 35,255 35,255 36 C254 36,253 37,253 37 C252 37,251 37,250 37 C250 37,249 37,249 37 C248 37,248 36,247 36 C247 35,247 35,246 34 C246 34,246 33,246 32 C246 32,246 31,246 30 C246 29,247 28,247 27 C247 26,247 25,248 24 C248 23,248 22,248 22 C249 21,249 20,249 19 C249 19,249 18,249 17 C249 17,249 17,249 16 C249 16,249 15,249 15 C248 15,248 15,248 14 C247 14,247 14,246 14 C246 14,245 14,244 15 C243 16,242 17,241 18 C240 19,239 20,238 21 C238 22,237 24,237 25 C236 27,236 29,236 31 C235 33,235 35,234 37 L229 37 L235 8 C236 7,236 6,236 6 C236 5,236 5,236 4 C236 4,236 3,236 3 C236 3,236 3,235 3 C235 2,235 2,234 2 C234 2,233 2,233 2 L233 0 L240 0 L242 0 L239 16 L239 16 C240 15,241 14,242 13 C242 13,243 12,244 12 C245 11,246 11,246 10 C247 10,248 10,249 10 C251 10,252 11,253 12 C254 13,254 14,254 16 C254 16,254 17,254 18 C254 18,254 19,254 20 L252 27 Z" fill="#212121"/><path d="M214 147 L212 153 L207 153 L208 147 L214 147 Z M207 175 C207 176,207 177,207 178 C207 178,207 179,207 179 C207 180,207 180,207 181 C207 181,208 181,208 181 C209 181,209 181,209 181 C210 181,210 181,210 180 C211 180,211 180,211 179 C212 179,212 178,213 178 L215 180 C214 181,213 182,212 182 C211 183,211 183,210 184 C209 184,209 185,208 185 C207 185,207 185,206 185 C205 185,205 185,204 185 C204 185,203 184,203 184 C202 183,202 183,202 182 C202 182,201 181,201 180 C201 180,202 179,202 178 C202 177,202 176,202 175 C203 174,203 173,203 172 C203 171,204 170,204 169 C204 168,204 167,204 166 C205 165,205 164,205 163 C205 163,205 163,205 162 C205 162,205 162,205 162 C205 161,205 161,204 161 C204 160,203 160,202 160 L202 158 L210 158 L211 158 L207 175 Z" fill="#212121"/><path d="M223 165 L223 161 L253 161 L253 165 L223 165 Z M223 177 L223 173 L253 173 L253 177 L223 177 Z" fill="#212121"/><path d="M278 178 C278 179,278 179,278 180 C278 180,278 181,278 181 C278 181,278 181,278 182 C278 182,279 182,279 182 C279 182,279 182,280 183 C280 183,280 183,281 183 C281 183,282 183,283 183 C284 183,285 183,286 183 L286 185 L264 185 L264 183 C265 183,266 183,267 183 C268 183,268 183,269 183 C269 183,270 183,270 183 C271 182,271 182,271 182 C271 182,272 182,272 182 C272 182,272 181,272 181 C272 181,272 180,272 180 C272 180,272 179,272 178 L272 159 C272 159,272 158,272 158 C272 158,271 157,271 157 C270 157,270 158,268 158 C267 159,266 160,264 161 C264 160,264 160,264 160 C263 159,263 159,263 158 C265 157,267 156,270 155 C272 154,274 152,276 151 L278 151 C278 152,278 153,278 154 C278 155,278 155,278 156 C278 157,278 157,278 158 L278 178 Z" fill="#212121"/><path d="M334 86 L338 83 L340 84 L335 108 C334 110,334 112,334 113 C334 114,334 115,334 115 C335 116,335 116,336 116 C337 116,338 116,338 115 C339 115,340 114,342 112 L344 114 C342 116,340 118,338 119 C337 120,335 120,333 120 C332 120,331 120,330 119 C329 118,328 116,328 115 C328 114,329 112,329 111 L329 110 C326 114,324 116,322 118 C320 119,318 120,315 120 C313 120,311 119,309 117 C308 115,307 112,307 108 C307 104,308 100,309 96 C311 92,313 89,316 87 C319 84,322 83,326 83 C327 83,329 83,330 84 C332 84,333 85,334 86 L334 86 Z M331 97 C331 96,332 95,332 94 C332 93,332 93,332 92 C332 90,331 88,331 87 C330 87,328 86,326 86 C324 86,322 87,320 89 C318 91,316 94,315 98 C314 101,313 105,313 108 C313 111,314 113,314 114 C315 115,316 116,318 116 C320 116,321 115,323 114 C324 113,326 110,327 108 C329 105,330 102,331 98 L331 97 Z" fill="#212121"/><path d="M357 107 L355 113 L350 113 L351 107 L357 107 Z M350 135 C350 136,350 137,350 138 C350 138,350 139,350 139 C350 140,350 140,350 141 C350 141,351 141,351 141 C352 141,352 141,352 141 C353 141,353 141,353 140 C354 140,354 140,354 139 C355 139,355 138,356 138 L358 140 C357 141,356 142,355 142 C354 143,354 143,353 144 C352 144,352 145,351 145 C350 145,350 145,349 145 C348 145,348 145,347 145 C347 145,346 144,346 144 C345 143,345 143,345 142 C345 142,344 141,344 140 C344 140,345 139,345 138 C345 137,345 136,345 135 C346 134,346 133,346 132 C346 131,347 130,347 129 C347 128,347 127,347 126 C348 125,348 124,348 123 C348 123,348 123,348 122 C348 122,348 122,348 122 C348 121,348 121,347 121 C347 120,346 120,345 120 L345 118 L353 118 L354 118 L350 135 Z" fill="#212121"/></svg></span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:152px;height:87px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 608 348"><path d="M31 72 L35 69 L37 70 L32 94 C31 96,31 98,31 99 C31 100,31 101,31 101 C32 102,32 102,33 102 C34 102,35 102,35 101 C36 101,37 100,39 98 L41 100 C39 102,37 104,35 105 C34 106,32 106,30 106 C29 106,28 106,27 105 C26 104,25 102,25 101 C25 100,26 98,26 97 L26 96 C23 100,21 102,19 104 C17 105,15 106,12 106 C10 106,8 105,6 103 C5 101,4 98,4 94 C4 90,5 86,6 82 C8 78,10 75,13 73 C16 70,19 69,23 69 C24 69,26 69,27 70 C29 70,30 71,31 72 L31 72 Z M28 83 C28 82,29 81,29 80 C29 79,29 79,29 78 C29 76,28 74,28 73 C27 73,25 72,23 72 C21 72,19 73,17 75 C15 77,13 80,12 84 C11 87,10 91,10 94 C10 97,11 99,11 100 C12 101,13 102,15 102 C17 102,18 101,20 100 C21 99,23 96,24 94 C26 91,27 88,28 84 L28 83 Z" fill="#212121"/><path d="M28 52 L36 64 L33 66 L26 57 L26 57 L17 66 L15 64 L23 52 L28 52 Z" fill="#212121"/><path d="M70 79 L70 74 L116 74 L116 79 L70 79 Z M70 94 L70 89 L116 89 L116 94 L70 94 Z" fill="#212121"/><path d="M317 15 L321 12 L323 13 L318 37 C317 39,317 41,317 42 C317 43,317 44,317 44 C318 45,318 45,319 45 C320 45,321 45,321 44 C322 44,323 43,325 41 L327 43 C325 45,323 47,321 48 C320 49,318 49,316 49 C315 49,314 49,313 48 C312 47,311 45,311 44 C311 43,312 41,312 40 L312 39 C309 43,307 45,305 47 C303 48,301 49,298 49 C296 49,294 48,292 46 C291 44,290 41,290 37 C290 33,291 29,292 25 C294 21,296 18,299 16 C302 13,305 12,309 12 C310 12,312 12,313 13 C315 13,316 14,317 15 L317 15 Z M314 26 C314 25,315 24,315 23 C315 22,315 22,315 21 C315 19,314 17,314 16 C313 16,311 15,309 15 C307 15,305 16,303 18 C301 20,299 23,298 27 C297 30,296 34,296 37 C296 40,297 42,297 43 C298 44,299 45,301 45 C303 45,304 44,306 43 C307 42,309 39,310 37 C312 34,313 31,314 27 L314 26 Z" fill="#212121"/><path d="M352 29 L352 24 L398 24 L398 29 L352 29 Z" fill="#212121"/><path d="M451 15 L455 12 L457 13 L452 37 C451 39,451 41,451 42 C451 43,451 44,451 44 C452 45,452 45,453 45 C454 45,455 45,455 44 C456 44,457 43,459 41 L461 43 C459 45,457 47,455 48 C454 49,452 49,450 49 C449 49,448 49,447 48 C446 47,445 45,445 44 C445 43,446 41,446 40 L446 39 C443 43,441 45,439 47 C437 48,435 49,432 49 C430 49,428 48,426 46 C425 44,424 41,424 37 C424 33,425 29,426 25 C428 21,430 18,433 16 C436 13,439 12,443 12 C444 12,446 12,447 13 C449 13,450 14,451 15 L451 15 Z M448 26 C448 25,449 24,449 23 C449 22,449 22,449 21 C449 19,448 17,448 16 C447 16,445 15,443 15 C441 15,439 16,437 18 C435 20,433 23,432 27 C431 30,430 34,430 37 C430 40,431 42,431 43 C432 44,433 45,435 45 C437 45,438 44,440 43 C441 42,443 39,444 37 C446 34,447 31,448 27 L448 26 Z" fill="#212121"/><path d="M455 4 L436 4 L436 0 L455 0 L455 4 Z" fill="#212121"/><path d="M156 302 L175 337 L191 100 L202 100 L202 105 L195 105 L178 351 L175 351 L152 308 L145 312 L143 309 L156 302 Z" fill="#212121"/><path d="M220 204 C220 205,220 205,220 206 C220 206,220 207,220 207 C220 207,220 207,220 208 C220 208,221 208,221 208 C221 208,221 208,222 209 C222 209,222 209,223 209 C223 209,224 209,225 209 C226 209,227 209,228 209 L228 211 L206 211 L206 209 C207 209,208 209,209 209 C210 209,210 209,211 209 C211 209,212 209,212 209 C213 208,213 208,213 208 C213 208,214 208,214 208 C214 208,214 207,214 207 C214 207,214 206,214 206 C214 206,214 205,214 204 L214 185 C214 185,214 184,214 184 C214 184,213 183,213 183 C212 183,212 184,210 184 C209 185,208 186,206 187 C206 186,206 186,206 186 C205 185,205 185,205 184 C207 183,209 182,212 181 C214 180,216 178,218 177 L220 177 C220 178,220 179,220 180 C220 181,220 181,220 182 C220 183,220 183,220 184 L220 204 Z" fill="#212121"/><path d="M225 283 C225 284,225 284,224 285 C224 285,224 286,224 286 C224 286,224 286,224 287 C224 287,224 287,224 287 C224 288,224 288,224 289 C225 289,225 289,226 289 C226 289,226 289,227 289 C227 289,227 289,228 288 C228 288,228 288,229 287 C229 287,230 286,230 286 C231 286,231 286,231 287 C232 287,232 287,232 287 C231 289,230 289,230 290 C229 291,228 291,228 292 C227 292,226 293,226 293 C225 293,224 293,223 293 C223 293,222 293,222 293 C221 293,221 292,220 292 C220 291,220 291,219 290 C219 290,219 289,219 288 C219 288,219 287,219 286 C219 285,220 284,220 283 C220 282,220 281,221 280 C221 279,221 278,221 278 C222 277,222 276,222 275 C222 275,222 274,222 273 C222 273,222 273,222 272 C222 272,222 271,222 271 C221 271,221 271,221 270 C220 270,220 270,219 270 C219 270,218 270,217 271 C216 272,215 273,214 274 C213 275,212 276,211 277 C211 278,210 280,210 281 C209 283,209 285,209 287 C208 289,208 291,207 293 L202 293 L208 264 C209 263,209 262,209 262 C209 261,209 261,209 260 C209 260,209 259,209 259 C209 259,209 259,208 259 C208 258,208 258,207 258 C207 258,206 258,206 258 L206 256 L213 256 L215 256 L212 272 L212 272 C213 271,214 270,215 269 C215 269,216 268,217 268 C218 267,219 267,219 266 C220 266,221 266,222 266 C224 266,225 267,226 268 C227 269,227 270,227 272 C227 272,227 273,227 274 C227 274,227 275,227 276 L225 283 Z" fill="#212121"/><rect x="199" y="231" width="34" height="5" fill="#212121"/><path d="M279 207 L296 234 L296 237 L277 264 L301 264 C302 264,303 264,304 264 C305 264,306 263,306 263 C307 262,307 262,308 261 C308 260,308 259,309 258 C309 257,310 255,310 253 L314 253 L313 271 L267 271 L267 269 L289 238 L268 205 L268 203 L313 203 L313 219 L309 219 C308 217,308 215,307 214 C307 212,306 211,306 210 C305 210,305 209,304 208 C303 208,303 208,302 207 C301 207,300 207,299 207 L279 207 Z" fill="#212121"/><path d="M301 175 C301 176,301 176,300 177 C300 177,300 178,300 178 C300 178,300 178,300 179 C300 179,300 179,300 179 C300 180,300 180,300 181 C301 181,301 181,302 181 C302 181,302 181,303 181 C303 181,303 181,304 180 C304 180,304 180,305 179 C305 179,306 178,306 178 C307 178,307 178,307 179 C308 179,308 179,308 179 C307 181,306 181,306 182 C305 183,304 183,304 184 C303 184,302 185,302 185 C301 185,300 185,299 185 C299 185,298 185,298 185 C297 185,297 184,296 184 C296 183,296 183,295 182 C295 182,295 181,295 180 C295 180,295 179,295 178 C295 177,296 176,296 175 C296 174,296 173,297 172 C297 171,297 170,297 170 C298 169,298 168,298 167 C298 167,298 166,298 165 C298 165,298 165,298 164 C298 164,298 163,298 163 C297 163,297 163,297 162 C296 162,296 162,295 162 C295 162,294 162,293 163 C292 164,291 165,290 166 C289 167,288 168,287 169 C287 170,286 172,286 173 C285 175,285 177,285 179 C284 181,284 183,283 185 L278 185 L284 156 C285 155,285 154,285 154 C285 153,285 153,285 152 C285 152,285 151,285 151 C285 151,285 151,284 151 C284 150,284 150,283 150 C283 150,282 150,282 150 L282 148 L289 148 L291 148 L288 164 L288 164 C289 163,290 162,291 161 C291 161,292 160,293 160 C294 159,295 159,295 158 C296 158,297 158,298 158 C300 158,301 159,302 160 C303 161,303 162,303 164 C303 164,303 165,303 166 C303 166,303 167,303 168 L301 175 Z" fill="#212121"/><path d="M263 278 L261 284 L256 284 L257 278 L263 278 Z M256 306 C256 307,256 308,256 309 C256 309,256 310,256 310 C256 311,256 311,256 312 C256 312,257 312,257 312 C258 312,258 312,258 312 C259 312,259 312,259 311 C260 311,260 311,260 310 C261 310,261 309,262 309 L264 311 C263 312,262 313,261 313 C260 314,260 314,259 315 C258 315,258 316,257 316 C256 316,256 316,255 316 C254 316,254 316,253 316 C253 316,252 315,252 315 C251 314,251 314,251 313 C251 313,250 312,250 311 C250 311,251 310,251 309 C251 308,251 307,251 306 C252 305,252 304,252 303 C252 302,253 301,253 300 C253 299,253 298,253 297 C254 296,254 295,254 294 C254 294,254 294,254 293 C254 293,254 293,254 293 C254 292,254 292,253 292 C253 291,252 291,251 291 L251 289 L259 289 L260 289 L256 306 Z" fill="#212121"/><path d="M272 296 L272 292 L302 292 L302 296 L272 296 Z M272 308 L272 304 L302 304 L302 308 L272 308 Z" fill="#212121"/><path d="M327 309 C327 310,327 310,327 311 C327 311,327 312,327 312 C327 312,327 312,327 313 C327 313,328 313,328 313 C328 313,328 313,329 314 C329 314,329 314,330 314 C330 314,331 314,332 314 C333 314,334 314,335 314 L335 316 L313 316 L313 314 C314 314,315 314,316 314 C317 314,317 314,318 314 C318 314,319 314,319 314 C320 313,320 313,320 313 C320 313,321 313,321 313 C321 313,321 312,321 312 C321 312,321 311,321 311 C321 311,321 310,321 309 L321 290 C321 290,321 289,321 289 C321 289,320 288,320 288 C319 288,319 289,317 289 C316 290,315 291,313 292 C313 291,313 291,313 291 C312 290,312 290,312 289 C314 288,316 287,319 286 C321 285,323 283,325 282 L327 282 C327 283,327 284,327 285 C327 286,327 286,327 287 C327 288,327 288,327 289 L327 309 Z" fill="#212121"/><path d="M352 237 C352 245,353 253,356 258 C359 264,363 267,369 269 L368 272 C361 270,355 266,352 260 C348 254,346 246,346 237 C346 228,348 221,352 214 C355 208,361 204,368 202 L369 205 C363 207,359 211,356 216 C353 222,352 228,352 237 L352 237 Z" fill="#212121"/><path d="M401 222 L405 219 L407 220 L402 244 C401 246,401 248,401 249 C401 250,401 251,401 251 C402 252,402 252,403 252 C404 252,405 252,405 251 C406 251,407 250,409 248 L411 250 C409 252,407 254,405 255 C404 256,402 256,400 256 C399 256,398 256,397 255 C396 254,395 252,395 251 C395 250,396 248,396 247 L396 246 C393 250,391 252,389 254 C387 255,385 256,382 256 C380 256,378 255,376 253 C375 251,374 248,374 244 C374 240,375 236,376 232 C378 228,380 225,383 223 C386 220,389 219,393 219 C394 219,396 219,397 220 C399 220,400 221,401 222 L401 222 Z M398 233 C398 232,399 231,399 230 C399 229,399 229,399 228 C399 226,398 224,398 223 C397 223,395 222,393 222 C391 222,389 223,387 225 C385 227,383 230,382 234 C381 237,380 241,380 244 C380 247,381 249,381 250 C382 251,383 252,385 252 C387 252,388 251,390 250 C391 249,393 246,394 244 C396 241,397 238,398 234 L398 233 Z" fill="#212121"/><path d="M436 236 L436 231 L482 231 L482 236 L436 236 Z" fill="#212121"/><path d="M535 222 L539 219 L541 220 L536 244 C535 246,535 248,535 249 C535 250,535 251,535 251 C536 252,536 252,537 252 C538 252,539 252,539 251 C540 251,541 250,543 248 L545 250 C543 252,541 254,539 255 C538 256,536 256,534 256 C533 256,532 256,531 255 C530 254,529 252,529 251 C529 250,530 248,530 247 L530 246 C527 250,525 252,523 254 C521 255,519 256,516 256 C514 256,512 255,510 253 C509 251,508 248,508 244 C508 240,509 236,510 232 C512 228,514 225,517 223 C520 220,523 219,527 219 C528 219,530 219,531 220 C533 220,534 221,535 222 L535 222 Z M532 233 C532 232,533 231,533 230 C533 229,533 229,533 228 C533 226,532 224,532 223 C531 223,529 222,527 222 C525 222,523 223,521 225 C519 227,517 230,516 234 C515 237,514 241,514 244 C514 247,515 249,515 250 C516 251,517 252,519 252 C521 252,522 251,524 250 C525 249,527 246,528 244 C530 241,531 238,532 234 L532 233 Z" fill="#212121"/><path d="M539 211 L520 211 L520 207 L539 207 L539 211 Z" fill="#212121"/><path d="M566 237 C566 228,565 222,562 216 C559 211,555 207,549 205 L550 202 C557 204,563 208,566 214 C570 221,572 228,572 237 C572 246,570 254,566 260 C563 266,557 270,550 272 L549 269 C555 267,559 264,562 258 C565 253,566 245,566 237 L566 237 Z" fill="#212121"/><path d="M597 215 C597 215,598 215,599 215 C599 215,599 215,600 215 C600 214,600 214,601 214 C601 213,601 213,601 212 L604 212 C604 213,604 214,604 216 C604 217,604 218,603 219 L581 219 L581 218 C581 217,582 216,582 215 C583 214,583 213,584 212 C585 211,586 210,587 209 C587 208,588 207,590 206 C591 204,593 203,594 202 C595 200,596 199,596 198 C597 197,597 197,597 196 C598 195,598 194,598 193 C598 192,598 192,597 191 C597 190,597 189,596 189 C596 188,595 188,594 188 C594 187,593 187,592 187 C590 187,589 188,588 188 C587 189,586 190,585 192 L582 192 L582 187 C584 186,586 185,588 185 C590 184,592 184,593 184 C595 184,597 184,598 185 C599 185,600 186,601 186 C602 187,602 188,603 189 C603 190,603 191,603 192 C603 193,603 194,603 194 C603 195,603 196,603 196 C602 197,602 197,602 198 C601 199,601 199,600 200 C600 201,599 201,599 202 C598 203,597 204,596 204 C595 205,594 206,594 207 C593 208,592 209,591 210 C590 211,589 212,588 213 C588 214,587 214,587 215 L597 215 Z" fill="#212121"/><rect x="199" y="100" width="408" height="5" fill="#212121"/><rect x="142" y="81" width="465" height="5" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">To perform this operation, we will first bring the format of the results matrix of the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> block in accordance with the format of the matrix of the initial data and add the two matrices. The result is normalized in a specially selected </span><span class="f_Text" style="font-style: italic;">NormlizeBuffer</span><span class="f_Text"> method.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;add&nbsp;to&nbsp;initial&nbsp;data&nbsp;and&nbsp;normalize</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">out</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Rows</span><span class="f_CodeExample">(),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Cols</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">out</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><br>
<span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">NormlizeBuffer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">(),&nbsp;</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cStd</span><span class="f_CodeExample">),&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">With this, the first block of operations is completed. This concludes the section on dividing the algorithm based on the execution of mathematical operations. For the block of operations using OpenCL, we will temporarily set the return of an error value and come back to it later.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//&nbsp;OpenCL&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Let's continue working with the encoder algorithm and move on to the second block of operations. Here it is necessary to conduct the signal of each element of the sequence through two fully connected layers. As you remember, we decided to organize this work through two convolutional layers. At first glance, there is nothing complicated about it - we simply call the forward pass methods for each convolutional layer sequentially.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">---&nbsp;call&nbsp;the&nbsp;feed-forward&nbsp;methods&nbsp;of&nbsp;the&nbsp;Feed&nbsp;Forward&nbsp;block&nbsp;layers</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">)))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">)))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Here, correct operation is possible only due to the transposition of the buffer of the convolutional neural layers results. Only this approach allows the aligned operation on each individual element of the sequence.</span></p>
<p class="p_Text"><span class="f_Text">After conducting a forward pass through two convolutional layers, just as after determining the attention results, it is necessary to propagate the obtained results to the data input into the first convolutional layer and normalize the resulting sums. We have already considered such a task above. Here we use the same algorithm, only the data buffers are different.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;add&nbsp;to&nbsp;the&nbsp;output&nbsp;of&nbsp;attention&nbsp;and&nbsp;normalize</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SumArray</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;normalize</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">NormlizeBuffer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cStd</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">1</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">It should be noted that</span><span class="f_li"> thanks to the buffer substitution organized in the initialization method, we obtain the results of the second convolutional layer from the result buffer of the current layer. In the same buffer, we will save the results of data normalization.</span></p>
<p class="p_Text"><span class="f_Text">After the completion of the operations, we exit the feed-forward method with a positive result.</span></p>
<p class="p_Text"><span class="f_Text">Now let's take a look at the changes made to the convolutional layer class. First, we'll add a variable to store the flag of the </span><span class="f_Text" style="font-style: italic;">m_bTransposedOutput </span><span class="f_Text">output structure. This will be a Boolean flag indicating the need to transpose the result matrix for output to the buffer. By default, we will set the value to </span><span class="f_Text" style="font-style: italic;">false,</span><span class="f_Text"> which means working in normal mode.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">class</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronConv</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">public</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronProof</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #0000ff;">protected</span><span class="f_CodeExample">:</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_bTransposedOutput</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;</span>
<br><span class="f_CodeExample" style="color: #0000ff;">public</span><span class="f_CodeExample">:</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">SetTransposedOutput</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">value</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;....</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">To control the value of the flag, let's create the </span><span class="f_Text" style="font-style: italic;">SetTransposedOutput</span><span class="f_Text"> method. The functionality of the method is quite simple. We resize the result matrices and error gradients.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronConv</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">SetTransposedOutput</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">value</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_bTransposedOutput</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">value</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">value</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">BufferInit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iNeurons</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iWindowOut</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">BufferInit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iNeurons</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iWindowOut</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">BufferInit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iWindowOut</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iNeurons</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">BufferInit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iWindowOut</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iNeurons</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">However, as you understand, the presence of a flag and even a method that changes it will not affect the results of data output to the buffer. To do this, we have to make some changes to the forward pass method. We are not changing the algorithm or the calculation logic at all; our changes will only involve rearranging matrices when multiplying the input data by the weight matrix, depending on the state of the </span><span class="f_Text" style="font-style: italic;">m_bTransposedOutput</span><span class="f_Text"> flag.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronConv</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;control&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;....</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;branching&nbsp;the&nbsp;algorithm&nbsp;depending&nbsp;on&nbsp;the&nbsp;execution&nbsp;device</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;....</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;Calculating&nbsp;the&nbsp;weighted&nbsp;sum&nbsp;of&nbsp;the&nbsp;elements&nbsp;of&nbsp;the&nbsp;input&nbsp;window</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_bTransposedOutput</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cWeights</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">());</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cWeights</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">());</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//&nbsp;OpenCL&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;....</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cActivation</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Activation</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After making changes to the feed-forward method, we need to make similar adjustments to the backpropagation methods because the error gradient should be propagated back to the point of error occurrence. Otherwise, the results of training the neural network will be unpredictable. First, we make changes to the gradient distribution method in the hidden layer </span><span class="f_Text" style="font-style: italic;">CNeuronConv::CalcHiddenGradient</span><span class="f_Text">.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronConv</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;control&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;....</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;correction&nbsp;of&nbsp;error&nbsp;gradients&nbsp;to&nbsp;the&nbsp;derivative&nbsp;of&nbsp;the&nbsp;activation&nbsp;function</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;....</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;branching&nbsp;the&nbsp;algorithm&nbsp;depending&nbsp;on&nbsp;the&nbsp;execution&nbsp;device</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">input_gradient</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_bTransposedOutput</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iNeurons</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iWindowOut</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iWindowOut</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iNeurons</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;....</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//&nbsp;OpenCL&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;....</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Then we make the relevant changes in the </span><span class="f_Text" style="font-style: italic;">CNeuronConv::CalcDeltaWeights</span><span class="f_Text"> method for distributing the gradient to the weight matrix level.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronConv</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;control&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;....</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;branching&nbsp;the&nbsp;algorithm&nbsp;depending&nbsp;on&nbsp;the&nbsp;execution&nbsp;device</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">input_data</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;....</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_bTransposedOutput</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iNeurons</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iWindowOut</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iWindowOut</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iNeurons</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cDeltaWeights</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;+=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">inp</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//&nbsp;OpenCL&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;....</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">As you can see, the changes are not so crucial, but they provide enhanced flexibility in settings.</span></p>

</div>

</body>
</html>
