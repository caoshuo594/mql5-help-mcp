<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>4.2.2.2 Backpropagation methods</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="4_main_layer_types.htm"> 4. Basic types of neural layers </a> / <a class="h_m" href="4_2_rnn.htm"> 4.2 Recurrent neural networks </a> / <a class="h_m" href="4_2_2_rnn_mql5.htm"> 4.2.2. Building an LSTM block in MQL5 </a>/ 4.2.2.2 Backpropagation methods
          </td>
          <td width="70" align="right">
          <a href="4_2_2_1_rnn_mql5_feedforward.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="4_2_2_3_rnn_mql5_save_load.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H3"><span class="f_H3">4.2.2.2 Backpropagation methods for LSTM block</span></p>
<p class="p_Text"><span class="f_Text">The feed-forward pass represents the standard mode of operation of a neural network. However, before it can be used in real-life operations, we need to train our model. Recurrent neural networks are trained using the familiar backpropagation method with a slight addition. The reason is that, unlike the neural layer types we've discussed before, only recurrent layers use their own output as their input on future iterations. Also, they all have their own weights that need to be learned as well. In the learning process, we have to unfold the recurrent layers chronologically as a multilayer perceptron. The only difference is that all layers will use the same weight matrix. Precisely for this purpose, during the feed-forward pass, we kept a record of the state history of all objects. Now it's time to put them to good use.</span></p>
<p class="p_Text"><span class="f_Text">We have three methods responsible for the backward pass in the base class of the neural layer:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li" style="font-style: italic;">CalcHiddenGradient</span><span class="f_li"> – a gradient distribution through a hidden layer.</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">CalcDeltaWeights</span><span class="f_li"> – a distribution of the gradient to the weighting matrix.</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">UpdateWeights</span><span class="f_li"> – the method of updating the weights.</span></li>
</ul>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">class</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronLSTM</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">public</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #0000ff;">protected</span><span class="f_CodeExample">:</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;....&nbsp;&nbsp;&nbsp;</span>
<br><span class="f_CodeExample" style="color: #0000ff;">public</span><span class="f_CodeExample">:</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;....&nbsp;&nbsp;&nbsp;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">override</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">override</span><span class="f_CodeExample">&nbsp;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">batch_size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;&amp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;&amp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">)&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">override</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We have to redefine them.</span></p>
<p class="p_Text"><span class="f_Text">First, we will override the </span><span class="f_Text" style="font-style: italic;">CalcHiddenGradient</span><span class="f_Text"> method for distributing the gradient through the hidden layer. Here we will need to unwrap the entire historical chain and run the error gradient through all states. Additionally, let's not forget that besides distributing gradients within the LSTM block, we must also perform the second function of this method: propagating the gradient of the error back to the previous layer.</span></p>
<p class="p_Text"><span class="f_Text">The method receives a pointer to the object of the previous layer and returns a boolean result indicating the success of the operations.</span></p>
<p class="p_Text"><span class="f_Text">At the beginning of the method, we check all the objects used. We check both pointers to objects of the previous layer and internal objects received in the parameters.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronLSTM</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;check&nbsp;the&nbsp;relevance&nbsp;of&nbsp;all&nbsp;objects</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">()&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cForgetGate</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cForgetGateOuts</span><span class="f_CodeExample">&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cInputGate</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cInputGateOuts</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputGate</span><span class="f_CodeExample">&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputGateOuts</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cNewContent</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cNewContentOuts</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Let's not forget that a backpropagation pass is only possible after a feed-forward pass. The foundation of source data for the backpropagation pass is established exactly during the feed-forward pass. Therefore, the next step is to check for the presence of information in the memory stacks and hidden states. In addition, the stack filling indicates the depth of gradient propagation in the story.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;check&nbsp;the&nbsp;presence&nbsp;of&nbsp;forward&nbsp;pass&nbsp;data</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">total</span><span class="f_CodeExample">&nbsp;=&nbsp;(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">)</span><span class="f_CodeExample" style="color: #0000ff;">fmin</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cMemorys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Total</span><span class="f_CodeExample">(),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cHiddenStates</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Total</span><span class="f_CodeExample">())&nbsp;-&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">total</span><span class="f_CodeExample">&nbsp;&lt;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Continuing the preparatory work, let's create pointers to the result and gradient buffers of the internal layers. I think the need for pointers to gradient buffers is obvious. We will need to write error gradients to them, propagating them through the </span><span class="f_Text" style="font-style: italic;">LSTM</span><span class="f_Text"> block. The need for result buffers, on the other hand, is not so obvious. As you know, every neuron has an activation function. Our inner layers are activated by the <a href="1_2_activation.htm#sigmoid" class="topiclink">logistic</a> function and by the <a href="1_2_activation.htm#tanh" class="topiclink">hyperbolic tangent</a>. The error gradient obtained at the input of the neural layer must be adjusted to the derivative of the activation function. The derivative of the above activation functions can be easily recalculated based on the result of the function itself. Thus, we need the appropriate input data to perform a correct backpropagation pass. For the previously considered neural layers, such an issue was not raised because the correct data were written to the result buffer in a forward pass. In the case of a recurrent block, only the result of the last forward pass will be stored in the result buffer. To work out the depth of the history, we will have to overwrite the values of the result buffer with the values of the corresponding time step. </span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;make&nbsp;pointers&nbsp;to&nbsp;buffers&nbsp;of&nbsp;gradients&nbsp;and&nbsp;results&nbsp;of&nbsp;internal&nbsp;layers</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">fg_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cForgetGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">fg_grad</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">fg_out</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cForgetGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">fg_out</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">ig_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cInputGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">ig_grad</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">ig_out</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cInputGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">ig_out</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">og_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cOutputGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">og_grad</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">og_out</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cOutputGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">og_out</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">nc_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cNewContent</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">nc_grad</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">nc_out</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cNewContent</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">nc_out</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">At the end of the preparatory process, we will store the size of the internal thread buffers into a local variable.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">uint</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">out_total</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Total</span><span class="f_CodeExample">();</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, we create a loop through historical data. The main operations of our method will be performed in the body of this loop. At the beginning of the loop, we will load information from the corresponding historical step in our stacks. Note that all buffers are loaded for the analyzed chronological step, while the memory buffer is taken from the preceding step. I will explain the reasons for this below. </span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;loop&nbsp;through&nbsp;the&nbsp;accumulated&nbsp;history</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">total</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;get&nbsp;pointers&nbsp;to&nbsp;buffers&nbsp;from&nbsp;the&nbsp;stack</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">fg</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cForgetGateOuts</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">fg</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">ig</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cInputGateOuts</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">ig</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">og</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cOutputGateOuts</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">og</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">nc</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cNewContentOuts</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">nc</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">memory</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cMemorys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">memory</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">hidden</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cHiddenStates</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">hidden</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cInputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, we have to distribute the error gradient received at the input of the </span><span class="f_Text" style="font-style: italic;">LSTM</span><span class="f_Text"> block between the internal neural layers. This is where we build a new process. Following our class construction concept, we create a branching of the algorithm based on the execution device for mathematical operations.</span></p>
<p class="p_Text"><span class="f_Text">The error gradient distribution is performed in reverse order of the forward flow of information. Hence, we will construct its propagation algorithm from output to input. Let's look at the result node of our </span><span class="f_Text" style="font-style: italic;">LSTM</span><span class="f_Text"> block. During the feed-forward pass, the updated memory state is activated by the hyperbolic tangent and multiplied by the output gate state. Thus, we have two components affecting the result of the block: the memory value and the gate.</span></p>
<div class="p_Text" style="text-align: center;"><div style="margin:0 auto 0 auto;width:330px"><img class="help" alt="LSTM block result node" title="LSTM block result node" width="330" height="400" style="width:330px;height:400px;border:none" src="lstm_bp1.png"/><p style="text-align:center"><span class="f_ImageCaption">LSTM block result node</span></p></div></div>
<p class="p_Text"><span class="f_Text">In order to reduce the error at the block output, we need to adjust the values of both components. To do this, we need to distribute the overall error gradient through a multiplication function that combines the two threads of information. That is, multiply the error gradient we know by the derivative of the function along each direction. We know from our high school math course that the derivative of the product of a constant over a variable is a constant. We apply the following approach: when determining the influence of one of the factors, we assume that all other components have constant values. Hence, we can write the following mathematical formulas.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:202px;height:17px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 808 68"><path d="M31 2 C36 2,39 4,42 6 C45 9,46 13,46 18 C46 22,45 27,44 31 C43 36,41 40,38 44 C36 47,33 50,30 52 C27 53,23 54,20 54 C15 54,11 53,9 50 C6 47,5 43,5 38 C5 34,6 30,7 25 C8 21,10 17,12 13 C15 9,17 7,21 5 C24 3,27 2,31 2 L31 2 Z M39 17 C39 9,36 5,31 5 C28 5,26 6,23 8 C21 10,19 13,17 16 C16 20,14 24,13 28 C12 32,12 36,12 39 C12 47,15 51,20 51 C23 51,27 50,29 46 C32 43,34 39,36 33 C38 27,39 22,39 17 L39 17 Z" fill="#212121"/><path d="M60 5 L60 3 L75 3 L74 5 C74 5,73 6,72 6 C72 6,72 6,71 7 C71 7,71 8,70 9 C70 10,70 12,69 14 L65 35 C64 36,64 38,64 39 C64 40,63 42,63 43 C63 46,64 48,65 49 C67 50,69 51,71 51 C74 51,76 51,78 50 C79 49,81 47,82 45 C83 43,83 41,84 37 L89 14 C90 12,90 10,90 8 C90 7,90 6,89 6 C89 5,88 5,86 5 L87 3 L101 3 L100 5 C99 5,99 6,98 6 C98 6,98 6,97 7 C97 7,97 8,96 9 C96 10,96 12,95 14 L90 36 C89 41,88 44,86 47 C84 49,82 51,80 52 C77 53,74 54,70 54 C66 54,63 53,60 51 C58 49,57 46,57 42 C57 41,57 39,58 37 C58 36,58 34,59 31 L63 14 C63 12,63 10,63 8 C63 7,63 6,63 6 C62 5,61 5,60 5 L60 5 Z" fill="#212121"/><path d="M125 53 L109 53 L110 51 C111 51,112 51,112 51 C112 50,113 50,113 49 C114 49,114 48,114 47 C115 46,115 44,116 42 L123 6 L119 6 C117 6,116 6,115 7 C114 7,113 8,112 9 C112 10,110 12,109 15 L106 15 L108 3 L147 3 L144 16 L140 16 C140 14,140 12,140 11 C140 9,140 8,139 8 C139 7,139 7,138 6 C138 6,137 6,135 6 L130 6 L122 42 C122 43,122 44,122 44 C121 45,121 46,121 46 C121 47,121 47,121 48 C121 49,121 49,122 50 C122 50,122 51,123 51 C123 51,124 51,125 51 L125 53 Z" fill="#212121"/><path d="M176 27 L176 22 L222 22 L222 27 L176 27 Z M176 42 L176 37 L222 37 L222 42 L176 42 Z" fill="#212121"/><path d="M279 2 C284 2,287 4,290 6 C293 9,294 13,294 18 C294 22,293 27,292 31 C291 36,289 40,286 44 C284 47,281 50,278 52 C275 53,271 54,268 54 C263 54,259 53,257 50 C254 47,253 43,253 38 C253 34,254 30,255 25 C256 21,258 17,260 13 C263 9,265 7,269 5 C272 3,275 2,279 2 L279 2 Z M287 17 C287 9,284 5,279 5 C276 5,274 6,271 8 C269 10,267 13,265 16 C264 20,262 24,261 28 C260 32,260 36,260 39 C260 47,263 51,268 51 C271 51,275 50,277 46 C280 43,282 39,284 33 C286 27,287 22,287 17 L287 17 Z" fill="#212121"/><path d="M329 42 C330 40,330 38,330 36 C330 35,330 34,329 34 C329 33,328 33,327 33 L327 31 L342 31 L341 33 C340 33,340 34,339 34 C339 34,338 34,338 35 C338 35,337 36,337 37 C337 38,336 40,336 42 L333 53 L330 54 L328 51 C326 53,324 53,322 54 C321 54,320 54,318 54 C313 54,310 53,308 50 C305 47,304 44,304 38 C304 35,305 30,306 26 C307 21,309 17,311 14 C313 10,316 7,320 5 C323 3,327 2,331 2 C333 2,335 2,337 3 C339 3,341 3,343 4 L341 14 L337 14 C337 11,337 9,335 7 C334 6,333 5,330 5 C328 5,326 6,324 7 C323 8,321 9,319 12 C318 14,316 16,315 20 C314 23,313 26,312 30 C311 33,311 37,311 40 C311 44,311 47,313 48 C314 50,316 51,319 51 C320 51,321 51,322 51 C323 51,324 50,325 50 C326 49,327 48,327 47 C328 46,329 45,329 44 L329 42 Z" fill="#212121"/><path d="M397 37 L395 41 L385 34 L387 46 L382 46 L383 34 L373 41 L371 37 L381 32 L371 27 L373 23 L383 30 L382 18 L387 18 L385 30 L395 23 L397 27 L387 32 L397 37 Z" fill="#212121"/><path d="M444 46 C442 49,440 51,438 52 C436 53,434 54,432 54 C427 54,425 52,425 46 C425 45,425 43,425 41 L429 22 L423 22 L424 20 C425 20,426 20,427 20 C428 20,428 19,429 19 C429 19,430 18,430 17 C431 17,431 16,432 15 C432 14,433 12,433 9 L438 9 L436 18 L447 18 L447 22 L436 22 L432 36 C432 39,431 41,431 43 C431 44,431 45,431 45 C431 49,432 50,435 50 C436 50,437 50,438 49 C439 48,441 46,442 44 L444 46 Z" fill="#212121"/><path d="M480 20 L484 17 L486 18 L481 42 C480 44,480 46,480 47 C480 48,480 49,480 49 C481 50,481 50,482 50 C483 50,484 50,484 49 C485 49,486 48,488 46 L490 48 C488 50,486 52,484 53 C483 54,481 54,479 54 C478 54,477 54,476 53 C475 52,474 50,474 49 C474 48,475 46,475 45 L475 44 C472 48,470 50,468 52 C466 53,464 54,461 54 C459 54,457 53,455 51 C454 49,453 46,453 42 C453 38,454 34,455 30 C457 26,459 23,462 21 C465 18,468 17,472 17 C473 17,475 17,476 18 C478 18,479 19,480 20 L480 20 Z M477 31 C477 30,478 29,478 28 C478 27,478 27,478 26 C478 24,477 22,477 21 C476 21,474 20,472 20 C470 20,468 21,466 23 C464 25,462 28,461 32 C460 35,459 39,459 42 C459 45,460 47,460 48 C461 49,462 50,464 50 C466 50,467 49,469 48 C470 47,472 44,473 42 C475 39,476 36,477 32 L477 31 Z" fill="#212121"/><path d="M501 29 C502 27,502 25,502 24 C502 23,502 22,501 22 C501 21,501 21,500 21 C499 21,498 21,497 22 C497 23,495 24,494 25 L492 23 C494 21,496 19,498 18 C499 18,501 17,503 17 C504 17,505 18,506 19 C507 20,508 21,508 22 C508 24,507 25,507 27 L507 27 C509 23,512 21,514 19 C516 18,518 17,521 17 C523 17,525 18,526 19 C527 20,528 22,528 25 C528 26,527 29,527 31 L524 41 C524 44,523 46,523 47 C523 48,523 49,524 49 C524 50,525 50,525 50 C526 50,527 50,528 49 C528 49,530 48,531 46 L533 48 C531 50,529 52,528 53 C526 54,524 54,522 54 C521 54,520 53,519 52 C518 51,517 50,517 48 C517 46,518 44,518 41 L520 34 C521 32,521 30,521 29 C521 28,521 27,521 26 C521 24,521 23,521 22 C520 21,519 21,518 21 C517 21,516 22,514 22 C513 23,512 24,511 26 C509 28,508 29,507 31 C507 33,506 35,505 37 L502 53 L496 53 L501 29 Z" fill="#212121"/><path d="M547 11 C547 9,548 7,548 6 C548 5,547 4,547 4 C546 3,545 3,543 3 L544 1 L553 1 L555 1 L549 25 L549 26 C552 23,554 20,556 19 C558 18,560 17,562 17 C564 17,566 18,567 19 C569 20,569 22,569 25 C569 26,569 29,568 31 L566 41 C565 44,565 46,565 47 C565 48,565 49,565 49 C566 50,566 50,567 50 C568 50,568 50,569 49 C570 49,571 48,573 46 L575 48 C572 50,570 52,569 53 C567 54,566 54,564 54 C562 54,561 53,560 52 C559 51,559 50,559 48 C559 46,559 44,560 41 L561 34 C562 32,562 30,563 29 C563 28,563 27,563 26 C563 24,563 23,562 22 C562 21,561 21,559 21 C558 21,557 22,556 22 C555 23,554 25,552 26 C551 28,550 30,549 31 C548 33,548 35,547 37 L544 53 L537 53 L547 11 Z" fill="#212121"/><path d="M590 35 C590 43,591 51,594 56 C597 62,601 65,607 67 L606 70 C599 68,593 64,590 58 C586 52,584 44,584 35 C584 26,586 19,590 12 C593 6,599 2,606 0 L607 3 C601 5,597 9,594 14 C591 20,590 26,590 35 L590 35 Z" fill="#212121"/><path d="M658 3 L670 3 L669 5 C668 5,668 5,667 6 C667 6,667 6,666 7 C666 7,666 8,665 9 C665 10,665 12,664 14 L658 42 C657 44,657 45,657 46 C657 47,657 47,657 48 C657 49,657 50,658 50 C658 51,659 51,661 51 L660 53 L646 53 L646 51 C647 51,648 51,648 50 C649 50,649 50,649 49 C650 48,650 48,650 47 C650 46,651 44,651 42 L656 20 C657 17,658 14,658 11 L658 11 C657 12,656 15,653 20 L636 47 L632 47 L629 23 C629 20,628 16,628 10 L627 10 C626 16,625 21,624 25 L620 42 C620 45,620 46,620 48 C620 49,620 50,620 50 C621 51,622 51,623 51 L623 53 L610 53 L610 51 C611 51,612 51,613 50 C613 50,613 50,614 49 C614 48,614 48,615 47 C615 46,615 44,616 42 L622 14 C622 13,622 12,623 11 C623 10,623 9,623 8 C623 7,623 6,622 6 C621 5,620 5,619 5 L620 3 L633 3 L637 38 L658 3 Z" fill="#212121"/><path d="M703 46 C700 49,698 51,695 52 C693 53,690 54,687 54 C683 54,681 53,679 51 C677 49,676 46,676 42 C676 39,676 36,677 33 C678 30,680 27,682 25 C683 22,686 20,688 19 C691 18,694 17,697 17 C700 17,702 18,704 19 C705 20,706 22,706 25 C706 29,704 32,700 34 C696 36,690 37,683 37 C682 39,682 40,682 42 C682 45,683 47,684 48 C685 49,686 50,689 50 C691 50,693 50,695 49 C696 48,698 46,700 44 L703 46 Z M683 34 C687 34,690 34,692 33 C695 33,696 32,698 30 C699 29,700 27,700 25 C700 23,699 22,699 21 C698 21,697 20,696 20 C693 20,691 21,688 24 C686 26,684 30,683 34 L683 34 Z" fill="#212121"/><path d="M725 27 C727 23,730 21,732 19 C734 18,736 17,739 17 C741 17,743 18,744 19 C745 20,746 22,746 25 L746 25 C746 25,746 25,746 26 C748 23,750 21,752 19 C754 18,756 17,758 17 C761 17,762 18,764 19 C765 20,766 22,766 25 C766 26,765 29,764 31 L762 41 C761 44,761 46,761 47 C761 48,761 49,762 49 C762 50,762 50,763 50 C764 50,765 50,765 49 C766 49,767 48,769 46 L771 48 C769 50,767 52,766 53 C764 54,762 54,760 54 C759 54,757 53,756 52 C755 51,755 50,755 48 C755 46,755 44,756 41 L758 34 C758 32,759 30,759 29 C759 28,759 27,759 26 C759 24,759 23,758 22 C758 21,757 21,756 21 C755 21,753 22,752 22 C751 23,750 24,749 26 C747 28,746 29,745 31 C745 32,744 34,743 37 L740 53 L734 53 L738 34 C739 32,739 30,739 29 C739 28,739 27,739 26 C739 24,739 23,738 22 C738 21,737 21,735 21 C735 21,734 22,732 22 C731 23,730 24,729 26 C727 28,726 29,725 31 C725 33,724 35,723 37 L720 53 L714 53 L719 29 C720 27,720 25,720 24 C720 23,720 22,719 22 C719 21,719 21,718 21 C717 21,716 21,715 22 C715 23,713 24,712 25 L710 23 C712 21,714 20,715 19 C717 18,719 17,721 17 C722 17,723 18,724 19 C725 20,726 21,726 22 C726 24,725 25,725 27 L725 27 Z" fill="#212121"/><path d="M791 35 C791 26,790 20,787 14 C784 9,780 5,774 3 L775 0 C782 2,788 6,791 12 C795 19,797 26,797 35 C797 44,795 52,791 58 C788 64,782 68,775 70 L774 67 C780 65,784 62,787 56 C790 51,791 43,791 35 L791 35 Z" fill="#212121"/></svg></span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:164px;height:40px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 656 160"><path d="M31 20 L31 19 C31 18,31 17,31 16 C31 14,31 12,31 11 C30 9,30 8,29 7 C29 6,28 5,27 4 C26 3,25 3,23 3 C22 3,20 4,19 4 C17 5,16 6,14 8 L11 8 L11 5 C13 4,15 3,17 2 C18 1,19 1,21 1 C22 0,23 0,25 0 C29 0,32 1,34 4 C36 7,37 11,37 17 C37 19,37 22,36 25 C36 28,35 31,34 34 C33 38,32 40,31 43 C30 45,28 47,27 48 C25 50,24 51,22 52 C20 53,17 53,15 53 C12 53,10 53,8 52 C7 51,5 49,4 47 C4 45,3 43,3 40 C3 37,3 34,4 32 C5 29,6 27,8 24 C10 22,12 20,14 19 C17 18,19 17,22 17 C23 17,25 17,26 18 C28 18,29 19,31 20 L31 20 Z M29 27 C29 25,28 23,27 22 C26 21,25 20,23 20 C21 20,19 21,18 21 C17 22,15 23,14 25 C13 26,12 28,11 30 C11 32,10 34,10 35 C9 37,9 40,9 42 C9 44,9 46,10 47 C10 48,11 49,12 49 C13 50,14 50,15 50 C18 50,20 49,22 47 C24 45,26 42,27 37 C28 33,29 30,29 27 L29 27 Z" fill="#212121"/><path d="M72 1 C77 1,80 3,83 5 C86 8,87 12,87 17 C87 21,86 26,85 30 C84 35,82 39,79 43 C77 46,74 49,71 51 C68 52,64 53,61 53 C56 53,52 52,50 49 C47 46,46 42,46 37 C46 33,47 29,48 24 C49 20,51 16,53 12 C56 8,58 6,62 4 C65 2,68 1,72 1 L72 1 Z M80 16 C80 8,77 4,72 4 C69 4,67 5,64 7 C62 9,60 12,58 15 C57 19,55 23,54 27 C53 31,53 35,53 38 C53 46,56 50,61 50 C64 50,68 49,70 45 C73 42,75 38,77 32 C79 26,80 21,80 16 L80 16 Z" fill="#212121"/><path d="M125 41 C125 43,124 45,124 46 C124 47,124 48,125 48 C125 49,126 49,126 49 C127 49,128 49,129 48 C129 48,131 47,132 45 L134 47 C132 49,130 51,129 52 C127 53,125 53,124 53 C122 53,121 53,120 52 C119 51,119 49,119 48 C119 47,119 45,119 44 L119 43 C117 47,114 49,112 51 C110 52,108 53,105 53 C103 53,101 52,100 51 C99 50,98 48,98 45 C98 44,99 42,99 39 L102 29 C102 26,103 24,103 23 C103 22,103 21,102 21 C102 20,102 20,101 20 C100 20,99 20,98 21 C98 22,96 23,95 24 L93 22 C95 20,97 19,98 18 C99 17,100 17,101 17 C102 16,103 16,104 16 C105 16,107 17,108 18 C108 19,109 20,109 22 C109 24,109 27,108 30 L106 36 C106 38,105 39,105 40 C105 41,105 42,105 42 C105 43,105 44,105 44 C105 46,105 47,106 48 C106 49,107 49,108 49 C109 49,110 49,112 48 C113 47,114 46,115 44 C117 42,118 41,119 39 C119 38,120 36,121 33 L124 17 L130 17 L125 41 Z" fill="#212121"/><path d="M160 45 C158 48,156 50,154 51 C152 52,150 53,148 53 C143 53,141 51,141 45 C141 44,141 42,141 40 L145 21 L139 21 L140 19 C141 19,142 19,143 19 C144 19,144 18,145 18 C145 18,146 17,146 16 C147 16,147 15,148 14 C148 13,149 11,149 8 L154 8 L152 17 L163 17 L163 21 L152 21 L148 35 C148 38,147 40,147 42 C147 43,147 44,147 44 C147 48,148 49,151 49 C152 49,153 49,154 48 C155 47,157 45,158 43 L160 45 Z" fill="#212121"/><path d="M44 127 L44 126 C44 125,44 124,44 123 C44 121,44 119,44 118 C43 116,43 115,42 114 C42 113,41 112,40 111 C39 110,38 110,36 110 C35 110,33 111,32 111 C30 112,29 113,27 115 L24 115 L24 112 C26 111,28 110,30 109 C31 108,32 108,34 108 C35 107,36 107,38 107 C42 107,45 108,47 111 C49 114,50 118,50 124 C50 126,50 129,49 132 C49 135,48 138,47 141 C46 145,45 147,44 150 C43 152,41 154,40 155 C38 157,37 158,35 159 C33 160,30 160,28 160 C25 160,23 160,21 159 C20 158,18 156,17 154 C17 152,16 150,16 147 C16 144,16 141,17 139 C18 136,19 134,21 131 C23 129,25 127,27 126 C30 125,32 124,35 124 C36 124,38 124,39 125 C41 125,42 126,44 127 L44 127 Z M42 134 C42 132,41 130,40 129 C39 128,38 127,36 127 C34 127,32 128,31 128 C30 129,28 130,27 132 C26 133,25 135,24 137 C24 139,23 141,23 142 C22 144,22 147,22 149 C22 151,22 153,23 154 C23 155,24 156,25 156 C26 157,27 157,28 157 C31 157,33 156,35 154 C37 152,39 149,40 144 C41 140,42 137,42 134 L42 134 Z" fill="#212121"/><path d="M85 108 C90 108,93 110,96 112 C99 115,100 119,100 124 C100 128,99 133,98 137 C97 142,95 146,92 150 C90 153,87 156,84 158 C81 159,77 160,74 160 C69 160,65 159,63 156 C60 153,59 149,59 144 C59 140,60 136,61 131 C62 127,64 123,66 119 C69 115,71 113,75 111 C78 109,81 108,85 108 L85 108 Z M93 123 C93 115,90 111,85 111 C82 111,80 112,77 114 C75 116,73 119,71 122 C70 126,68 130,67 134 C66 138,66 142,66 145 C66 153,69 157,74 157 C77 157,81 156,83 152 C86 149,88 145,90 139 C92 133,93 128,93 123 L93 123 Z" fill="#212121"/><path d="M135 148 C136 146,136 144,136 142 C136 141,136 140,135 140 C135 139,134 139,133 139 L133 137 L148 137 L147 139 C146 139,146 140,145 140 C145 140,144 140,144 141 C144 141,143 142,143 143 C143 144,142 146,142 148 L139 159 L136 160 L134 157 C132 159,130 159,128 160 C127 160,126 160,124 160 C119 160,116 159,114 156 C111 153,110 150,110 144 C110 141,111 136,112 132 C113 127,115 123,117 120 C119 116,122 113,126 111 C129 109,133 108,137 108 C139 108,141 108,143 109 C145 109,147 109,149 110 L147 120 L143 120 C143 117,143 115,141 113 C140 112,139 111,136 111 C134 111,132 112,130 113 C129 114,127 115,125 118 C124 120,122 122,121 126 C120 129,119 132,118 136 C117 139,117 143,117 146 C117 150,117 153,119 154 C120 156,122 157,125 157 C126 157,127 157,128 157 C129 157,130 156,131 156 C132 155,133 154,133 153 C134 152,135 151,135 150 L135 148 Z" fill="#212121"/><rect x="0" y="85" width="165" height="5" fill="#212121"/><path d="M191 83 L191 78 L237 78 L237 83 L191 83 Z M191 98 L191 93 L237 93 L237 98 L191 98 Z" fill="#212121"/><path d="M288 102 C286 105,284 107,282 108 C280 109,278 110,276 110 C271 110,269 108,269 102 C269 101,269 99,269 97 L273 78 L267 78 L268 76 C269 76,270 76,271 76 C272 76,272 75,273 75 C273 75,274 74,274 73 C275 73,275 72,276 71 C276 70,277 68,277 65 L282 65 L280 74 L291 74 L291 78 L280 78 L276 92 C276 95,275 97,275 99 C275 100,275 101,275 101 C275 105,276 106,279 106 C280 106,281 106,282 105 C283 104,285 102,286 100 L288 102 Z" fill="#212121"/><path d="M324 76 L328 73 L330 74 L325 98 C324 100,324 102,324 103 C324 104,324 105,324 105 C325 106,325 106,326 106 C327 106,328 106,328 105 C329 105,330 104,332 102 L334 104 C332 106,330 108,328 109 C327 110,325 110,323 110 C322 110,321 110,320 109 C319 108,318 106,318 105 C318 104,319 102,319 101 L319 100 C316 104,314 106,312 108 C310 109,308 110,305 110 C303 110,301 109,299 107 C298 105,297 102,297 98 C297 94,298 90,299 86 C301 82,303 79,306 77 C309 74,312 73,316 73 C317 73,319 73,320 74 C322 74,323 75,324 76 L324 76 Z M321 87 C321 86,322 85,322 84 C322 83,322 83,322 82 C322 80,321 78,321 77 C320 77,318 76,316 76 C314 76,312 77,310 79 C308 81,306 84,305 88 C304 91,303 95,303 98 C303 101,304 103,304 104 C305 105,306 106,308 106 C310 106,311 105,313 104 C314 103,316 100,317 98 C319 95,320 92,321 88 L321 87 Z" fill="#212121"/><path d="M345 85 C346 83,346 81,346 80 C346 79,346 78,345 78 C345 77,345 77,344 77 C343 77,342 77,341 78 C341 79,339 80,338 81 L336 79 C338 77,340 75,342 74 C343 74,345 73,347 73 C348 73,349 74,350 75 C351 76,352 77,352 78 C352 80,351 81,351 83 L351 83 C353 79,356 77,358 75 C360 74,362 73,365 73 C367 73,369 74,370 75 C371 76,372 78,372 81 C372 82,371 85,371 87 L368 97 C368 100,367 102,367 103 C367 104,367 105,368 105 C368 106,369 106,369 106 C370 106,371 106,372 105 C372 105,374 104,375 102 L377 104 C375 106,373 108,372 109 C370 110,368 110,366 110 C365 110,364 109,363 108 C362 107,361 106,361 104 C361 102,362 100,362 97 L364 90 C365 88,365 86,365 85 C365 84,365 83,365 82 C365 80,365 79,365 78 C364 77,363 77,362 77 C361 77,360 78,358 78 C357 79,356 80,355 82 C353 84,352 85,351 87 C351 89,350 91,349 93 L346 109 L340 109 L345 85 Z" fill="#212121"/><path d="M391 67 C391 65,392 63,392 62 C392 61,391 60,391 60 C390 59,389 59,387 59 L388 57 L397 57 L399 57 L393 81 L393 82 C396 79,398 76,400 75 C402 74,404 73,406 73 C408 73,410 74,411 75 C413 76,413 78,413 81 C413 82,413 85,412 87 L410 97 C409 100,409 102,409 103 C409 104,409 105,409 105 C410 106,410 106,411 106 C412 106,412 106,413 105 C414 105,415 104,417 102 L419 104 C416 106,414 108,413 109 C411 110,410 110,408 110 C406 110,405 109,404 108 C403 107,403 106,403 104 C403 102,403 100,404 97 L405 90 C406 88,406 86,407 85 C407 84,407 83,407 82 C407 80,407 79,406 78 C406 77,405 77,403 77 C402 77,401 78,400 78 C399 79,398 81,396 82 C395 84,394 86,393 87 C392 89,392 91,391 93 L388 109 L381 109 L391 67 Z" fill="#212121"/><path d="M434 91 C434 99,435 107,438 112 C441 118,445 121,451 123 L450 126 C443 124,437 120,434 114 C430 108,428 100,428 91 C428 82,430 75,434 68 C437 62,443 58,450 56 L451 59 C445 61,441 65,438 70 C435 76,434 82,434 91 L434 91 Z" fill="#212121"/><path d="M502 59 L514 59 L513 61 C512 61,512 61,511 62 C511 62,511 62,510 63 C510 63,510 64,509 65 C509 66,509 68,508 70 L502 98 C501 100,501 101,501 102 C501 103,501 103,501 104 C501 105,501 106,502 106 C502 107,503 107,505 107 L504 109 L490 109 L490 107 C491 107,492 107,492 106 C493 106,493 106,493 105 C494 104,494 104,494 103 C494 102,495 100,495 98 L500 76 C501 73,502 70,502 67 L502 67 C501 68,500 71,497 76 L480 103 L476 103 L473 79 C473 76,472 72,472 66 L471 66 C470 72,469 77,468 81 L464 98 C464 101,464 102,464 104 C464 105,464 106,464 106 C465 107,466 107,467 107 L467 109 L454 109 L454 107 C455 107,456 107,457 106 C457 106,457 106,458 105 C458 104,458 104,459 103 C459 102,459 100,460 98 L466 70 C466 69,466 68,467 67 C467 66,467 65,467 64 C467 63,467 62,466 62 C465 61,464 61,463 61 L464 59 L477 59 L481 94 L502 59 Z" fill="#212121"/><path d="M547 102 C544 105,542 107,539 108 C537 109,534 110,531 110 C527 110,525 109,523 107 C521 105,520 102,520 98 C520 95,520 92,521 89 C522 86,524 83,526 81 C527 78,530 76,532 75 C535 74,538 73,541 73 C544 73,546 74,548 75 C549 76,550 78,550 81 C550 85,548 88,544 90 C540 92,534 93,527 93 C526 95,526 96,526 98 C526 101,527 103,528 104 C529 105,530 106,533 106 C535 106,537 106,539 105 C540 104,542 102,544 100 L547 102 Z M527 90 C531 90,534 90,536 89 C539 89,540 88,542 86 C543 85,544 83,544 81 C544 79,543 78,543 77 C542 77,541 76,540 76 C537 76,535 77,532 80 C530 82,528 86,527 90 L527 90 Z" fill="#212121"/><path d="M569 83 C571 79,574 77,576 75 C578 74,580 73,583 73 C585 73,587 74,588 75 C589 76,590 78,590 81 L590 81 C590 81,590 81,590 82 C592 79,594 77,596 75 C598 74,600 73,602 73 C605 73,606 74,608 75 C609 76,610 78,610 81 C610 82,609 85,608 87 L606 97 C605 100,605 102,605 103 C605 104,605 105,606 105 C606 106,606 106,607 106 C608 106,609 106,609 105 C610 105,611 104,613 102 L615 104 C613 106,611 108,610 109 C608 110,606 110,604 110 C603 110,601 109,600 108 C599 107,599 106,599 104 C599 102,599 100,600 97 L602 90 C602 88,603 86,603 85 C603 84,603 83,603 82 C603 80,603 79,602 78 C602 77,601 77,600 77 C599 77,597 78,596 78 C595 79,594 80,593 82 C591 84,590 85,589 87 C589 88,588 90,587 93 L584 109 L578 109 L582 90 C583 88,583 86,583 85 C583 84,583 83,583 82 C583 80,583 79,582 78 C582 77,581 77,579 77 C579 77,578 78,576 78 C575 79,574 80,573 82 C571 84,570 85,569 87 C569 89,568 91,567 93 L564 109 L558 109 L563 85 C564 83,564 81,564 80 C564 79,564 78,563 78 C563 77,563 77,562 77 C561 77,560 77,559 78 C559 79,557 80,556 81 L554 79 C556 77,558 76,559 75 C561 74,563 73,565 73 C566 73,567 74,568 75 C569 76,570 77,570 78 C570 80,569 81,569 83 L569 83 Z" fill="#212121"/><path d="M635 91 C635 82,634 76,631 70 C628 65,624 61,618 59 L619 56 C626 58,632 62,635 68 C639 75,641 82,641 91 C641 100,639 108,635 114 C632 120,626 124,619 126 L618 123 C624 121,628 118,631 112 C634 107,635 99,635 91 L635 91 Z" fill="#212121"/></svg></span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:157px;height:44px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 628 176"><path d="M161 20 L161 19 C161 18,161 17,161 16 C161 14,161 12,161 11 C160 9,160 8,159 7 C159 6,158 5,157 4 C156 3,155 3,153 3 C152 3,150 4,149 4 C147 5,146 6,144 8 L141 8 L141 5 C143 4,145 3,147 2 C148 1,149 1,151 1 C152 0,153 0,155 0 C159 0,162 1,164 4 C166 7,167 11,167 17 C167 19,167 22,166 25 C166 28,165 31,164 34 C163 38,162 40,161 43 C160 45,158 47,157 48 C155 50,154 51,152 52 C150 53,147 53,145 53 C142 53,140 53,138 52 C137 51,135 49,134 47 C134 45,133 43,133 40 C133 37,133 34,134 32 C135 29,136 27,138 24 C140 22,142 20,144 19 C147 18,149 17,152 17 C153 17,155 17,156 18 C158 18,159 19,161 20 L161 20 Z M159 27 C159 25,158 23,157 22 C156 21,155 20,153 20 C151 20,149 21,148 21 C147 22,145 23,144 25 C143 26,142 28,141 30 C141 32,140 34,140 35 C139 37,139 40,139 42 C139 44,139 46,140 47 C140 48,141 49,142 49 C143 50,144 50,145 50 C148 50,150 49,152 47 C154 45,156 42,157 37 C158 33,159 30,159 27 L159 27 Z" fill="#212121"/><path d="M202 1 C207 1,210 3,213 5 C216 8,217 12,217 17 C217 21,216 26,215 30 C214 35,212 39,209 43 C207 46,204 49,201 51 C198 52,194 53,191 53 C186 53,182 52,180 49 C177 46,176 42,176 37 C176 33,177 29,178 24 C179 20,181 16,183 12 C186 8,188 6,192 4 C195 2,198 1,202 1 L202 1 Z M210 16 C210 8,207 4,202 4 C199 4,197 5,194 7 C192 9,190 12,188 15 C187 19,185 23,184 27 C183 31,183 35,183 38 C183 46,186 50,191 50 C194 50,198 49,200 45 C203 42,205 38,207 32 C209 26,210 21,210 16 L210 16 Z" fill="#212121"/><path d="M255 41 C255 43,254 45,254 46 C254 47,254 48,255 48 C255 49,256 49,256 49 C257 49,258 49,259 48 C259 48,261 47,262 45 L264 47 C262 49,260 51,259 52 C257 53,255 53,254 53 C252 53,251 53,250 52 C249 51,249 49,249 48 C249 47,249 45,249 44 L249 43 C247 47,244 49,242 51 C240 52,238 53,235 53 C233 53,231 52,230 51 C229 50,228 48,228 45 C228 44,229 42,229 39 L232 29 C232 26,233 24,233 23 C233 22,233 21,232 21 C232 20,232 20,231 20 C230 20,229 20,228 21 C228 22,226 23,225 24 L223 22 C225 20,227 19,228 18 C229 17,230 17,231 17 C232 16,233 16,234 16 C235 16,237 17,238 18 C238 19,239 20,239 22 C239 24,239 27,238 30 L236 36 C236 38,235 39,235 40 C235 41,235 42,235 42 C235 43,235 44,235 44 C235 46,235 47,236 48 C236 49,237 49,238 49 C239 49,240 49,242 48 C243 47,244 46,245 44 C247 42,248 41,249 39 C249 38,250 36,251 33 L254 17 L260 17 L255 41 Z" fill="#212121"/><path d="M290 45 C288 48,286 50,284 51 C282 52,280 53,278 53 C273 53,271 51,271 45 C271 44,271 42,271 40 L275 21 L269 21 L270 19 C271 19,272 19,273 19 C274 19,274 18,275 18 C275 18,276 17,276 16 C277 16,277 15,278 14 C278 13,279 11,279 8 L284 8 L282 17 L293 17 L293 21 L282 21 L278 35 C278 38,277 40,277 42 C277 43,277 44,277 44 C277 48,278 49,281 49 C282 49,283 49,284 48 C285 47,287 45,288 43 L290 45 Z" fill="#212121"/><path d="M31 127 L31 126 C31 125,31 124,31 123 C31 121,31 119,31 118 C30 116,30 115,29 114 C29 113,28 112,27 111 C26 110,25 110,23 110 C22 110,20 111,19 111 C17 112,16 113,14 115 L11 115 L11 112 C13 111,15 110,17 109 C18 108,19 108,21 108 C22 107,23 107,25 107 C29 107,32 108,34 111 C36 114,37 118,37 124 C37 126,37 129,36 132 C36 135,35 138,34 141 C33 145,32 147,31 150 C30 152,28 154,27 155 C25 157,24 158,22 159 C20 160,17 160,15 160 C12 160,10 160,8 159 C7 158,5 156,4 154 C4 152,3 150,3 147 C3 144,3 141,4 139 C5 136,6 134,8 131 C10 129,12 127,14 126 C17 125,19 124,22 124 C23 124,25 124,26 125 C28 125,29 126,31 127 L31 127 Z M29 134 C29 132,28 130,27 129 C26 128,25 127,23 127 C21 127,19 128,18 128 C17 129,15 130,14 132 C13 133,12 135,11 137 C11 139,10 141,10 142 C9 144,9 147,9 149 C9 151,9 153,10 154 C10 155,11 156,12 156 C13 157,14 157,15 157 C18 157,20 156,22 154 C24 152,26 149,27 144 C28 140,29 137,29 134 L29 134 Z" fill="#212121"/><path d="M66 152 C64 155,62 157,60 158 C58 159,56 160,54 160 C49 160,47 158,47 152 C47 151,47 149,47 147 L51 128 L45 128 L46 126 C47 126,48 126,49 126 C50 126,50 125,51 125 C51 125,52 124,52 123 C53 123,53 122,54 121 C54 120,55 118,55 115 L60 115 L58 124 L69 124 L69 128 L58 128 L54 142 C54 145,53 147,53 149 C53 150,53 151,53 151 C53 155,54 156,57 156 C58 156,59 156,60 155 C61 154,63 152,64 150 L66 152 Z" fill="#212121"/><path d="M102 126 L106 123 L108 124 L103 148 C102 150,102 152,102 153 C102 154,102 155,102 155 C103 156,103 156,104 156 C105 156,106 156,106 155 C107 155,108 154,110 152 L112 154 C110 156,108 158,106 159 C105 160,103 160,101 160 C100 160,99 160,98 159 C97 158,96 156,96 155 C96 154,97 152,97 151 L97 150 C94 154,92 156,90 158 C88 159,86 160,83 160 C81 160,79 159,77 157 C76 155,75 152,75 148 C75 144,76 140,77 136 C79 132,81 129,84 127 C87 124,90 123,94 123 C95 123,97 123,98 124 C100 124,101 125,102 126 L102 126 Z M99 137 C99 136,100 135,100 134 C100 133,100 133,100 132 C100 130,99 128,99 127 C98 127,96 126,94 126 C92 126,90 127,88 129 C86 131,84 134,83 138 C82 141,81 145,81 148 C81 151,82 153,82 154 C83 155,84 156,86 156 C88 156,89 155,91 154 C92 153,94 150,95 148 C97 145,98 142,99 138 L99 137 Z" fill="#212121"/><path d="M123 135 C124 133,124 131,124 130 C124 129,124 128,123 128 C123 127,123 127,122 127 C121 127,120 127,119 128 C119 129,117 130,116 131 L114 129 C116 127,118 125,120 124 C121 124,123 123,125 123 C126 123,127 124,128 125 C129 126,130 127,130 128 C130 130,129 131,129 133 L129 133 C131 129,134 127,136 125 C138 124,140 123,143 123 C145 123,147 124,148 125 C149 126,150 128,150 131 C150 132,149 135,149 137 L146 147 C146 150,145 152,145 153 C145 154,145 155,146 155 C146 156,147 156,147 156 C148 156,149 156,150 155 C150 155,152 154,153 152 L155 154 C153 156,151 158,150 159 C148 160,146 160,144 160 C143 160,142 159,141 158 C140 157,139 156,139 154 C139 152,140 150,140 147 L142 140 C143 138,143 136,143 135 C143 134,143 133,143 132 C143 130,143 129,143 128 C142 127,141 127,140 127 C139 127,138 128,136 128 C135 129,134 130,133 132 C131 134,130 135,129 137 C129 139,128 141,127 143 L124 159 L118 159 L123 135 Z" fill="#212121"/><path d="M169 117 C169 115,170 113,170 112 C170 111,169 110,169 110 C168 109,167 109,165 109 L166 107 L175 107 L177 107 L171 131 L171 132 C174 129,176 126,178 125 C180 124,182 123,184 123 C186 123,188 124,189 125 C191 126,191 128,191 131 C191 132,191 135,190 137 L188 147 C187 150,187 152,187 153 C187 154,187 155,187 155 C188 156,188 156,189 156 C190 156,190 156,191 155 C192 155,193 154,195 152 L197 154 C194 156,192 158,191 159 C189 160,188 160,186 160 C184 160,183 159,182 158 C181 157,181 156,181 154 C181 152,181 150,182 147 L183 140 C184 138,184 136,185 135 C185 134,185 133,185 132 C185 130,185 129,184 128 C184 127,183 127,181 127 C180 127,179 128,178 128 C177 129,176 131,174 132 C173 134,172 136,171 137 C170 139,170 141,169 143 L166 159 L159 159 L169 117 Z" fill="#212121"/><path d="M212 141 C212 149,213 157,216 162 C219 168,223 171,229 173 L228 176 C221 174,215 170,212 164 C208 158,206 150,206 141 C206 132,208 125,212 118 C215 112,221 108,228 106 L229 109 C223 111,219 115,216 120 C213 126,212 132,212 141 L212 141 Z" fill="#212121"/><path d="M280 109 L292 109 L291 111 C290 111,290 111,289 112 C289 112,289 112,288 113 C288 113,288 114,287 115 C287 116,287 118,286 120 L280 148 C279 150,279 151,279 152 C279 153,279 153,279 154 C279 155,279 156,280 156 C280 157,281 157,283 157 L282 159 L268 159 L268 157 C269 157,270 157,270 156 C271 156,271 156,271 155 C272 154,272 154,272 153 C272 152,273 150,273 148 L278 126 C279 123,280 120,280 117 L280 117 C279 118,278 121,275 126 L258 153 L254 153 L251 129 C251 126,250 122,250 116 L249 116 C248 122,247 127,246 131 L242 148 C242 151,242 152,242 154 C242 155,242 156,242 156 C243 157,244 157,245 157 L245 159 L232 159 L232 157 C233 157,234 157,235 156 C235 156,235 156,236 155 C236 154,236 154,237 153 C237 152,237 150,238 148 L244 120 C244 119,244 118,245 117 C245 116,245 115,245 114 C245 113,245 112,244 112 C243 111,242 111,241 111 L242 109 L255 109 L259 144 L280 109 Z" fill="#212121"/><path d="M325 152 C322 155,320 157,317 158 C315 159,312 160,309 160 C305 160,303 159,301 157 C299 155,298 152,298 148 C298 145,298 142,299 139 C300 136,302 133,304 131 C305 128,308 126,310 125 C313 124,316 123,319 123 C322 123,324 124,326 125 C327 126,328 128,328 131 C328 135,326 138,322 140 C318 142,312 143,305 143 C304 145,304 146,304 148 C304 151,305 153,306 154 C307 155,308 156,311 156 C313 156,315 156,317 155 C318 154,320 152,322 150 L325 152 Z M305 140 C309 140,312 140,314 139 C317 139,318 138,320 136 C321 135,322 133,322 131 C322 129,321 128,321 127 C320 127,319 126,318 126 C315 126,313 127,310 130 C308 132,306 136,305 140 L305 140 Z" fill="#212121"/><path d="M347 133 C349 129,352 127,354 125 C356 124,358 123,361 123 C363 123,365 124,366 125 C367 126,368 128,368 131 L368 131 C368 131,368 131,368 132 C370 129,372 127,374 125 C376 124,378 123,380 123 C383 123,384 124,386 125 C387 126,388 128,388 131 C388 132,387 135,386 137 L384 147 C383 150,383 152,383 153 C383 154,383 155,384 155 C384 156,384 156,385 156 C386 156,387 156,387 155 C388 155,389 154,391 152 L393 154 C391 156,389 158,388 159 C386 160,384 160,382 160 C381 160,379 159,378 158 C377 157,377 156,377 154 C377 152,377 150,378 147 L380 140 C380 138,381 136,381 135 C381 134,381 133,381 132 C381 130,381 129,380 128 C380 127,379 127,378 127 C377 127,375 128,374 128 C373 129,372 130,371 132 C369 134,368 135,367 137 C367 138,366 140,365 143 L362 159 L356 159 L360 140 C361 138,361 136,361 135 C361 134,361 133,361 132 C361 130,361 129,360 128 C360 127,359 127,357 127 C357 127,356 128,354 128 C353 129,352 130,351 132 C349 134,348 135,347 137 C347 139,346 141,345 143 L342 159 L336 159 L341 135 C342 133,342 131,342 130 C342 129,342 128,341 128 C341 127,341 127,340 127 C339 127,338 127,337 128 C337 129,335 130,334 131 L332 129 C334 127,336 126,337 125 C339 124,341 123,343 123 C344 123,345 124,346 125 C347 126,348 127,348 128 C348 130,347 131,347 133 L347 133 Z" fill="#212121"/><path d="M413 141 C413 132,412 126,409 120 C406 115,402 111,396 109 L397 106 C404 108,410 112,413 118 C417 125,419 132,419 141 C419 150,417 158,413 164 C410 170,404 174,397 176 L396 173 C402 171,406 168,409 162 C412 157,413 149,413 141 L413 141 Z" fill="#212121"/><rect x="0" y="85" width="426" height="5" fill="#212121"/><path d="M452 83 L452 78 L498 78 L498 83 L452 83 Z M452 98 L452 93 L498 93 L498 98 L452 98 Z" fill="#212121"/><path d="M555 58 C560 58,563 60,566 62 C569 65,570 69,570 74 C570 78,569 83,568 87 C567 92,565 96,562 100 C560 103,557 106,554 108 C551 109,547 110,544 110 C539 110,535 109,533 106 C530 103,529 99,529 94 C529 90,530 86,531 81 C532 77,534 73,536 69 C539 65,541 63,545 61 C548 59,551 58,555 58 L555 58 Z M563 73 C563 65,560 61,555 61 C552 61,550 62,547 64 C545 66,543 69,541 72 C540 76,538 80,537 84 C536 88,536 92,536 95 C536 103,539 107,544 107 C547 107,551 106,553 102 C556 99,558 95,560 89 C562 83,563 78,563 73 L563 73 Z" fill="#212121"/><path d="M605 98 C606 96,606 94,606 92 C606 91,606 90,605 90 C605 89,604 89,603 89 L603 87 L618 87 L617 89 C616 89,616 90,615 90 C615 90,614 90,614 91 C614 91,613 92,613 93 C613 94,612 96,612 98 L609 109 L606 110 L604 107 C602 109,600 109,598 110 C597 110,596 110,594 110 C589 110,586 109,584 106 C581 103,580 100,580 94 C580 91,581 86,582 82 C583 77,585 73,587 70 C589 66,592 63,596 61 C599 59,603 58,607 58 C609 58,611 58,613 59 C615 59,617 59,619 60 L617 70 L613 70 C613 67,613 65,611 63 C610 62,609 61,606 61 C604 61,602 62,600 63 C599 64,597 65,595 68 C594 70,592 72,591 76 C590 79,589 82,588 86 C587 89,587 93,587 96 C587 100,587 103,589 104 C590 106,592 107,595 107 C596 107,597 107,598 107 C599 107,600 106,601 106 C602 105,603 104,603 103 C604 102,605 101,605 100 L605 98 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">Then we can easily distribute the derivative in both directions using the following mathematical formulas.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:268px;height:19px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 1072 76"><path d="M30 42 C31 40,31 38,31 36 C31 35,31 34,30 34 C30 33,29 33,28 33 L28 31 L43 31 L42 33 C41 33,41 34,40 34 C40 34,39 34,39 35 C39 35,38 36,38 37 C38 38,37 40,37 42 L34 53 L31 54 L29 51 C27 53,25 53,23 54 C22 54,21 54,19 54 C14 54,11 53,9 50 C6 47,5 44,5 38 C5 35,6 30,7 26 C8 21,10 17,12 14 C14 10,17 7,21 5 C24 3,28 2,32 2 C34 2,36 2,38 3 C40 3,42 3,44 4 L42 14 L38 14 C38 11,38 9,36 7 C35 6,34 5,31 5 C29 5,27 6,25 7 C24 8,22 9,20 12 C19 14,17 16,16 20 C15 23,14 26,13 30 C12 33,12 37,12 40 C12 44,12 47,14 48 C15 50,17 51,20 51 C21 51,22 51,23 51 C24 51,25 50,26 50 C27 49,28 48,28 47 C29 46,30 45,30 44 L30 42 Z" fill="#212121"/><path d="M63 27 C66 23,68 21,70 19 C72 18,75 17,77 17 C79 17,80 17,81 17 L80 26 L76 26 C76 25,75 24,75 24 C75 23,75 23,74 22 C74 22,73 22,73 22 C72 22,71 22,70 23 C69 24,68 25,66 27 C65 28,64 30,63 31 C63 33,62 35,61 37 L58 53 L52 53 L57 29 C57 28,58 27,58 26 C58 25,58 25,58 24 C58 23,58 22,57 22 C57 21,57 21,56 21 C55 21,54 21,53 22 C53 23,51 24,50 25 L48 23 C50 21,52 19,54 18 C55 18,57 17,59 17 C60 17,61 18,62 19 C63 20,63 21,63 22 C63 24,63 25,63 26 L63 27 Z" fill="#212121"/><path d="M114 20 L118 17 L120 18 L115 42 C114 44,114 46,114 47 C114 48,114 49,114 49 C115 50,115 50,116 50 C117 50,118 50,118 49 C119 49,120 48,122 46 L124 48 C122 50,120 52,118 53 C117 54,115 54,113 54 C112 54,111 54,110 53 C109 52,108 50,108 49 C108 48,109 46,109 45 L109 44 C106 48,104 50,102 52 C100 53,98 54,95 54 C93 54,91 53,89 51 C88 49,87 46,87 42 C87 38,88 34,89 30 C91 26,93 23,96 21 C99 18,102 17,106 17 C107 17,109 17,110 18 C112 18,113 19,114 20 L114 20 Z M111 31 C111 30,112 29,112 28 C112 27,112 27,112 26 C112 24,111 22,111 21 C110 21,108 20,106 20 C104 20,102 21,100 23 C98 25,96 28,95 32 C94 35,93 39,93 42 C93 45,94 47,94 48 C95 49,96 50,98 50 C100 50,101 49,103 48 C104 47,106 44,107 42 C109 39,110 36,111 32 L111 31 Z" fill="#212121"/><path d="M151 44 C148 48,146 50,144 52 C142 53,140 54,137 54 C135 54,133 53,131 51 C130 49,129 46,129 42 C129 38,130 34,131 30 C133 26,135 23,138 21 C141 18,144 17,148 17 C149 17,151 17,152 18 C153 18,154 18,156 19 L158 11 C158 10,158 9,158 8 C158 7,158 7,158 6 C158 5,158 5,158 4 C158 4,157 4,157 3 C156 3,155 3,154 3 L155 1 L164 1 L166 1 L157 42 C156 44,156 46,156 47 C156 48,156 49,156 49 C157 50,157 50,158 50 C159 50,160 50,160 49 C161 49,162 48,164 46 L166 48 C164 50,162 52,160 53 C159 54,157 54,155 54 C154 54,153 54,152 53 C151 52,150 50,150 49 C150 48,151 46,151 45 L151 44 Z M153 31 C153 29,154 27,154 26 C154 24,153 22,152 21 C152 21,150 20,148 20 C146 20,144 21,142 23 C140 25,138 28,137 32 C136 35,135 39,135 42 C135 45,136 47,136 48 C137 49,138 50,140 50 C141 50,141 50,142 50 C143 49,144 49,145 48 C146 47,146 46,147 45 C148 44,149 43,149 42 C150 41,151 40,151 38 C152 36,152 35,153 32 L153 31 Z" fill="#212121"/><path d="M181 77 C180 77,178 77,177 76 C175 76,174 75,173 74 C172 73,171 72,171 70 C170 69,170 67,170 65 C170 64,170 62,170 60 C171 59,171 57,172 55 C172 53,173 51,174 50 C175 48,176 47,178 45 C179 44,181 43,183 42 C185 41,187 41,189 41 C191 41,193 41,194 42 C196 43,197 43,198 44 C199 45,199 47,200 48 C200 50,201 51,201 53 C201 54,200 56,200 58 C200 59,199 61,199 63 C198 65,197 67,196 68 C195 70,194 71,193 73 C191 74,190 75,188 76 C186 77,184 77,181 77 L181 77 Z M175 66 C175 69,176 71,177 72 C178 73,179 74,182 74 C183 74,184 74,186 73 C187 72,188 71,189 70 C190 69,191 68,192 66 C192 65,193 63,194 61 C194 60,194 58,195 56 C195 55,195 53,195 52 C195 49,195 47,194 46 C193 45,191 44,189 44 C187 44,186 44,185 45 C183 46,182 47,181 48 C180 49,179 51,178 52 C178 54,177 56,177 57 C176 59,176 61,176 62 C175 64,175 65,175 66 L175 66 Z" fill="#212121"/><path d="M225 60 L236 60 L236 62 C235 62,235 62,235 63 C234 63,234 63,234 63 C234 64,233 64,233 65 C233 66,233 67,232 68 C232 69,232 71,231 72 C231 74,231 75,230 76 C230 77,230 77,229 77 C229 77,229 77,228 77 C228 77,228 76,227 76 C227 76,227 75,227 75 C226 76,225 76,224 76 C224 76,223 77,222 77 C222 77,221 77,221 77 C220 77,219 77,219 77 C217 77,215 77,214 76 C212 76,211 75,210 74 C210 73,209 72,208 70 C208 69,208 67,208 65 C208 63,208 62,208 60 C209 58,209 56,210 54 C211 52,212 51,213 49 C214 48,215 46,217 45 C218 44,220 43,221 42 C223 41,225 41,227 41 C229 41,231 41,233 41 C234 42,236 42,237 43 L236 50 L233 50 C233 48,232 47,231 46 C230 45,229 44,227 44 C224 44,222 45,220 48 C219 49,218 50,217 51 C216 53,216 54,215 56 C214 58,214 59,214 61 C213 63,213 65,213 67 C213 69,214 71,215 72 C216 74,217 74,219 74 C220 74,221 74,222 74 C222 74,223 74,224 73 C224 73,225 72,225 72 C226 71,226 71,226 70 C227 70,227 69,227 69 C227 68,227 67,227 67 C227 66,228 66,228 65 C228 65,228 65,228 64 C228 64,227 63,227 63 C227 62,226 62,225 62 L225 60 Z" fill="#212121"/><path d="M267 27 L267 22 L313 22 L313 27 L267 27 Z M267 42 L267 37 L313 37 L313 42 L267 42 Z" fill="#212121"/><path d="M369 42 C370 40,370 38,370 36 C370 35,370 34,369 34 C369 33,368 33,367 33 L367 31 L382 31 L381 33 C380 33,380 34,379 34 C379 34,378 34,378 35 C378 35,377 36,377 37 C377 38,376 40,376 42 L373 53 L370 54 L368 51 C366 53,364 53,362 54 C361 54,360 54,358 54 C353 54,350 53,348 50 C345 47,344 44,344 38 C344 35,345 30,346 26 C347 21,349 17,351 14 C353 10,356 7,360 5 C363 3,367 2,371 2 C373 2,375 2,377 3 C379 3,381 3,383 4 L381 14 L377 14 C377 11,377 9,375 7 C374 6,373 5,370 5 C368 5,366 6,364 7 C363 8,361 9,359 12 C358 14,356 16,355 20 C354 23,353 26,352 30 C351 33,351 37,351 40 C351 44,351 47,353 48 C354 50,356 51,359 51 C360 51,361 51,362 51 C363 51,364 50,365 50 C366 49,367 48,367 47 C368 46,369 45,369 44 L369 42 Z" fill="#212121"/><path d="M402 27 C405 23,407 21,409 19 C411 18,414 17,416 17 C418 17,419 17,420 17 L419 26 L415 26 C415 25,414 24,414 24 C414 23,414 23,413 22 C413 22,412 22,412 22 C411 22,410 22,409 23 C408 24,407 25,405 27 C404 28,403 30,402 31 C402 33,401 35,400 37 L397 53 L391 53 L396 29 C396 28,397 27,397 26 C397 25,397 25,397 24 C397 23,397 22,396 22 C396 21,396 21,395 21 C394 21,393 21,392 22 C392 23,390 24,389 25 L387 23 C389 21,391 19,393 18 C394 18,396 17,398 17 C399 17,400 18,401 19 C402 20,402 21,402 22 C402 24,402 25,402 26 L402 27 Z" fill="#212121"/><path d="M453 20 L457 17 L459 18 L454 42 C453 44,453 46,453 47 C453 48,453 49,453 49 C454 50,454 50,455 50 C456 50,457 50,457 49 C458 49,459 48,461 46 L463 48 C461 50,459 52,457 53 C456 54,454 54,452 54 C451 54,450 54,449 53 C448 52,447 50,447 49 C447 48,448 46,448 45 L448 44 C445 48,443 50,441 52 C439 53,437 54,434 54 C432 54,430 53,428 51 C427 49,426 46,426 42 C426 38,427 34,428 30 C430 26,432 23,435 21 C438 18,441 17,445 17 C446 17,448 17,449 18 C451 18,452 19,453 20 L453 20 Z M450 31 C450 30,451 29,451 28 C451 27,451 27,451 26 C451 24,450 22,450 21 C449 21,447 20,445 20 C443 20,441 21,439 23 C437 25,435 28,434 32 C433 35,432 39,432 42 C432 45,433 47,433 48 C434 49,435 50,437 50 C439 50,440 49,442 48 C443 47,445 44,446 42 C448 39,449 36,450 32 L450 31 Z" fill="#212121"/><path d="M490 44 C487 48,485 50,483 52 C481 53,479 54,476 54 C474 54,472 53,470 51 C469 49,468 46,468 42 C468 38,469 34,470 30 C472 26,474 23,477 21 C480 18,483 17,487 17 C488 17,490 17,491 18 C492 18,493 18,495 19 L497 11 C497 10,497 9,497 8 C497 7,497 7,497 6 C497 5,497 5,497 4 C497 4,496 4,496 3 C495 3,494 3,493 3 L494 1 L503 1 L505 1 L496 42 C495 44,495 46,495 47 C495 48,495 49,495 49 C496 50,496 50,497 50 C498 50,499 50,499 49 C500 49,501 48,503 46 L505 48 C503 50,501 52,499 53 C498 54,496 54,494 54 C493 54,492 54,491 53 C490 52,489 50,489 49 C489 48,490 46,490 45 L490 44 Z M492 31 C492 29,493 27,493 26 C493 24,492 22,491 21 C491 21,489 20,487 20 C485 20,483 21,481 23 C479 25,477 28,476 32 C475 35,474 39,474 42 C474 45,475 47,475 48 C476 49,477 50,479 50 C480 50,480 50,481 50 C482 49,483 49,484 48 C485 47,485 46,486 45 C487 44,488 43,488 42 C489 41,490 40,490 38 C491 36,491 35,492 32 L492 31 Z" fill="#212121"/><path d="M520 77 C519 77,517 77,516 76 C514 76,513 75,512 74 C511 73,510 72,510 70 C509 69,509 67,509 65 C509 64,509 62,509 60 C510 59,510 57,511 55 C511 53,512 51,513 50 C514 48,515 47,517 45 C518 44,520 43,522 42 C524 41,526 41,528 41 C530 41,532 41,533 42 C535 43,536 43,537 44 C538 45,538 47,539 48 C539 50,540 51,540 53 C540 54,539 56,539 58 C539 59,538 61,538 63 C537 65,536 67,535 68 C534 70,533 71,532 73 C530 74,529 75,527 76 C525 77,523 77,520 77 L520 77 Z M514 66 C514 69,515 71,516 72 C517 73,518 74,521 74 C522 74,523 74,525 73 C526 72,527 71,528 70 C529 69,530 68,531 66 C531 65,532 63,533 61 C533 60,533 58,534 56 C534 55,534 53,534 52 C534 49,534 47,533 46 C532 45,530 44,528 44 C526 44,525 44,524 45 C522 46,521 47,520 48 C519 49,518 51,517 52 C517 54,516 56,516 57 C515 59,515 61,515 62 C514 64,514 65,514 66 L514 66 Z" fill="#212121"/><path d="M565 69 C563 72,561 73,560 74 C558 76,556 76,554 76 C553 76,551 76,550 75 C549 74,549 72,549 71 C549 70,549 70,549 69 C549 68,549 67,550 66 C550 65,550 64,550 63 C551 62,551 61,551 60 C551 59,552 58,552 57 C552 56,552 55,552 55 C552 54,552 53,551 53 C550 53,550 53,550 53 C549 53,549 54,548 54 C548 54,548 55,547 55 C547 55,546 56,546 56 L544 55 C545 54,546 53,547 52 C547 51,548 51,549 50 C549 50,550 50,551 49 C551 49,552 49,553 49 C554 49,555 50,556 50 C557 51,557 52,557 54 C557 54,557 55,557 56 C557 57,557 58,556 59 C556 61,555 64,555 65 C554 67,554 68,554 69 C554 70,554 71,555 71 C555 72,556 72,557 72 C558 72,558 72,559 71 C560 71,561 70,562 69 C563 68,564 67,565 66 C565 64,566 63,566 62 L569 49 L574 49 L570 66 C570 67,570 68,570 68 C570 69,570 70,570 70 C570 71,570 71,570 72 C570 72,571 72,571 72 C571 72,572 72,572 72 C572 72,573 72,573 71 C574 71,574 71,574 70 C575 70,575 69,576 69 L578 70 C577 72,576 72,575 73 C574 74,574 74,573 75 C572 75,572 76,571 76 C570 76,570 76,569 76 C568 76,567 76,566 75 C565 74,565 73,565 72 C565 71,565 71,565 70 C565 70,565 70,565 69 L565 69 Z" fill="#212121"/><path d="M600 70 C599 71,598 72,597 73 C596 74,596 74,595 75 C594 75,593 76,592 76 C592 76,591 76,590 76 C588 76,586 76,585 75 C584 74,584 72,584 70 C584 70,584 69,584 68 C584 67,584 67,584 66 L587 52 L583 52 L583 50 C584 50,585 50,586 50 C587 50,587 49,587 49 C588 49,588 48,588 48 C588 48,589 47,589 47 C589 46,589 45,590 45 C590 44,590 43,591 42 L595 42 L593 49 L602 49 L602 52 L593 52 L590 63 C590 64,590 64,589 65 C589 66,589 66,589 66 C589 67,589 67,589 68 C589 68,589 68,589 69 C589 70,589 71,590 71 C590 72,591 72,592 72 C593 72,594 72,595 71 C596 71,597 70,598 68 L600 70 Z" fill="#212121"/><path d="M654 37 L652 41 L642 34 L644 46 L639 46 L640 34 L630 41 L628 37 L638 32 L628 27 L630 23 L640 30 L639 18 L644 18 L642 30 L652 23 L654 27 L644 32 L654 37 Z" fill="#212121"/><path d="M701 46 C699 49,697 51,695 52 C693 53,691 54,689 54 C684 54,682 52,682 46 C682 45,682 43,682 41 L686 22 L680 22 L681 20 C682 20,683 20,684 20 C685 20,685 19,686 19 C686 19,687 18,687 17 C688 17,688 16,689 15 C689 14,690 12,690 9 L695 9 L693 18 L704 18 L704 22 L693 22 L689 36 C689 39,688 41,688 43 C688 44,688 45,688 45 C688 49,689 50,692 50 C693 50,694 50,695 49 C696 48,698 46,699 44 L701 46 Z" fill="#212121"/><path d="M737 20 L741 17 L743 18 L738 42 C737 44,737 46,737 47 C737 48,737 49,737 49 C738 50,738 50,739 50 C740 50,741 50,741 49 C742 49,743 48,745 46 L747 48 C745 50,743 52,741 53 C740 54,738 54,736 54 C735 54,734 54,733 53 C732 52,731 50,731 49 C731 48,732 46,732 45 L732 44 C729 48,727 50,725 52 C723 53,721 54,718 54 C716 54,714 53,712 51 C711 49,710 46,710 42 C710 38,711 34,712 30 C714 26,716 23,719 21 C722 18,725 17,729 17 C730 17,732 17,733 18 C735 18,736 19,737 20 L737 20 Z M734 31 C734 30,735 29,735 28 C735 27,735 27,735 26 C735 24,734 22,734 21 C733 21,731 20,729 20 C727 20,725 21,723 23 C721 25,719 28,718 32 C717 35,716 39,716 42 C716 45,717 47,717 48 C718 49,719 50,721 50 C723 50,724 49,726 48 C727 47,729 44,730 42 C732 39,733 36,734 32 L734 31 Z" fill="#212121"/><path d="M758 29 C759 27,759 25,759 24 C759 23,759 22,758 22 C758 21,758 21,757 21 C756 21,755 21,754 22 C754 23,752 24,751 25 L749 23 C751 21,753 19,755 18 C756 18,758 17,760 17 C761 17,762 18,763 19 C764 20,765 21,765 22 C765 24,764 25,764 27 L764 27 C766 23,769 21,771 19 C773 18,775 17,778 17 C780 17,782 18,783 19 C784 20,785 22,785 25 C785 26,784 29,784 31 L781 41 C781 44,780 46,780 47 C780 48,780 49,781 49 C781 50,782 50,782 50 C783 50,784 50,785 49 C785 49,787 48,788 46 L790 48 C788 50,786 52,785 53 C783 54,781 54,779 54 C778 54,777 53,776 52 C775 51,774 50,774 48 C774 46,775 44,775 41 L777 34 C778 32,778 30,778 29 C778 28,778 27,778 26 C778 24,778 23,778 22 C777 21,776 21,775 21 C774 21,773 22,771 22 C770 23,769 24,768 26 C766 28,765 29,764 31 C764 33,763 35,762 37 L759 53 L753 53 L758 29 Z" fill="#212121"/><path d="M804 11 C804 9,805 7,805 6 C805 5,804 4,804 4 C803 3,802 3,800 3 L801 1 L810 1 L812 1 L806 25 L806 26 C809 23,811 20,813 19 C815 18,817 17,819 17 C821 17,823 18,824 19 C826 20,826 22,826 25 C826 26,826 29,825 31 L823 41 C822 44,822 46,822 47 C822 48,822 49,822 49 C823 50,823 50,824 50 C825 50,825 50,826 49 C827 49,828 48,830 46 L832 48 C829 50,827 52,826 53 C824 54,823 54,821 54 C819 54,818 53,817 52 C816 51,816 50,816 48 C816 46,816 44,817 41 L818 34 C819 32,819 30,820 29 C820 28,820 27,820 26 C820 24,820 23,819 22 C819 21,818 21,816 21 C815 21,814 22,813 22 C812 23,811 25,809 26 C808 28,807 30,806 31 C805 33,805 35,804 37 L801 53 L794 53 L804 11 Z" fill="#212121"/><path d="M847 35 C847 43,848 51,851 56 C854 62,858 65,864 67 L863 70 C856 68,850 64,847 58 C843 52,841 44,841 35 C841 26,843 19,847 12 C850 6,856 2,863 0 L864 3 C858 5,854 9,851 14 C848 20,847 26,847 35 L847 35 Z" fill="#212121"/><path d="M915 3 L927 3 L926 5 C925 5,925 5,924 6 C924 6,924 6,923 7 C923 7,923 8,922 9 C922 10,922 12,921 14 L915 42 C914 44,914 45,914 46 C914 47,914 47,914 48 C914 49,914 50,915 50 C915 51,916 51,918 51 L917 53 L903 53 L903 51 C904 51,905 51,905 50 C906 50,906 50,906 49 C907 48,907 48,907 47 C907 46,908 44,908 42 L913 20 C914 17,915 14,915 11 L915 11 C914 12,913 15,910 20 L893 47 L889 47 L886 23 C886 20,885 16,885 10 L884 10 C883 16,882 21,881 25 L877 42 C877 45,877 46,877 48 C877 49,877 50,877 50 C878 51,879 51,880 51 L880 53 L867 53 L867 51 C868 51,869 51,870 50 C870 50,870 50,871 49 C871 48,871 48,872 47 C872 46,872 44,873 42 L879 14 C879 13,879 12,880 11 C880 10,880 9,880 8 C880 7,880 6,879 6 C878 5,877 5,876 5 L877 3 L890 3 L894 38 L915 3 Z" fill="#212121"/><path d="M960 46 C957 49,955 51,952 52 C950 53,947 54,944 54 C940 54,938 53,936 51 C934 49,933 46,933 42 C933 39,933 36,934 33 C935 30,937 27,939 25 C940 22,943 20,945 19 C948 18,951 17,954 17 C957 17,959 18,961 19 C962 20,963 22,963 25 C963 29,961 32,957 34 C953 36,947 37,940 37 C939 39,939 40,939 42 C939 45,940 47,941 48 C942 49,943 50,946 50 C948 50,950 50,952 49 C953 48,955 46,957 44 L960 46 Z M940 34 C944 34,947 34,949 33 C952 33,953 32,955 30 C956 29,957 27,957 25 C957 23,956 22,956 21 C955 21,954 20,953 20 C950 20,948 21,945 24 C943 26,941 30,940 34 L940 34 Z" fill="#212121"/><path d="M982 27 C984 23,987 21,989 19 C991 18,993 17,996 17 C998 17,1000 18,1001 19 C1002 20,1003 22,1003 25 L1003 25 C1003 25,1003 25,1003 26 C1005 23,1007 21,1009 19 C1011 18,1013 17,1015 17 C1018 17,1019 18,1021 19 C1022 20,1023 22,1023 25 C1023 26,1022 29,1021 31 L1019 41 C1018 44,1018 46,1018 47 C1018 48,1018 49,1019 49 C1019 50,1019 50,1020 50 C1021 50,1022 50,1022 49 C1023 49,1024 48,1026 46 L1028 48 C1026 50,1024 52,1023 53 C1021 54,1019 54,1017 54 C1016 54,1014 53,1013 52 C1012 51,1012 50,1012 48 C1012 46,1012 44,1013 41 L1015 34 C1015 32,1016 30,1016 29 C1016 28,1016 27,1016 26 C1016 24,1016 23,1015 22 C1015 21,1014 21,1013 21 C1012 21,1010 22,1009 22 C1008 23,1007 24,1006 26 C1004 28,1003 29,1002 31 C1002 32,1001 34,1000 37 L997 53 L991 53 L995 34 C996 32,996 30,996 29 C996 28,996 27,996 26 C996 24,996 23,995 22 C995 21,994 21,992 21 C992 21,991 22,989 22 C988 23,987 24,986 26 C984 28,983 29,982 31 C982 33,981 35,980 37 L977 53 L971 53 L976 29 C977 27,977 25,977 24 C977 23,977 22,976 22 C976 21,976 21,975 21 C974 21,973 21,972 22 C972 23,970 24,969 25 L967 23 C969 21,971 20,972 19 C974 18,976 17,978 17 C979 17,980 18,981 19 C982 20,983 21,983 22 C983 24,982 25,982 27 L982 27 Z" fill="#212121"/><path d="M1048 35 C1048 26,1047 20,1044 14 C1041 9,1037 5,1031 3 L1032 0 C1039 2,1045 6,1048 12 C1052 19,1054 26,1054 35 C1054 44,1052 52,1048 58 C1045 64,1039 68,1032 70 L1031 67 C1037 65,1041 62,1044 56 C1047 51,1048 43,1048 35 L1048 35 Z" fill="#212121"/></svg></span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:265px;height:23px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 1060 92"><path d="M30 41 C31 39,31 37,31 35 C31 34,31 33,30 33 C30 32,29 32,28 32 L28 30 L43 30 L42 32 C41 32,41 33,40 33 C40 33,39 33,39 34 C39 34,38 35,38 36 C38 37,37 39,37 41 L34 52 L31 53 L29 50 C27 52,25 52,23 53 C22 53,21 53,19 53 C14 53,11 52,9 49 C6 46,5 43,5 37 C5 34,6 29,7 25 C8 20,10 16,12 13 C14 9,17 6,21 4 C24 2,28 1,32 1 C34 1,36 1,38 2 C40 2,42 2,44 3 L42 13 L38 13 C38 10,38 8,36 6 C35 5,34 4,31 4 C29 4,27 5,25 6 C24 7,22 8,20 11 C19 13,17 15,16 19 C15 22,14 25,13 29 C12 32,12 36,12 39 C12 43,12 46,14 47 C15 49,17 50,20 50 C21 50,22 50,23 50 C24 50,25 49,26 49 C27 48,28 47,28 46 C29 45,30 44,30 43 L30 41 Z" fill="#212121"/><path d="M63 26 C66 22,68 20,70 18 C72 17,75 16,77 16 C79 16,80 16,81 16 L80 25 L76 25 C76 24,75 23,75 23 C75 22,75 22,74 21 C74 21,73 21,73 21 C72 21,71 21,70 22 C69 23,68 24,66 26 C65 27,64 29,63 30 C63 32,62 34,61 36 L58 52 L52 52 L57 28 C57 27,58 26,58 25 C58 24,58 24,58 23 C58 22,58 21,57 21 C57 20,57 20,56 20 C55 20,54 20,53 21 C53 22,51 23,50 24 L48 22 C50 20,52 18,54 17 C55 17,57 16,59 16 C60 16,61 17,62 18 C63 19,63 20,63 21 C63 23,63 24,63 25 L63 26 Z" fill="#212121"/><path d="M114 19 L118 16 L120 17 L115 41 C114 43,114 45,114 46 C114 47,114 48,114 48 C115 49,115 49,116 49 C117 49,118 49,118 48 C119 48,120 47,122 45 L124 47 C122 49,120 51,118 52 C117 53,115 53,113 53 C112 53,111 53,110 52 C109 51,108 49,108 48 C108 47,109 45,109 44 L109 43 C106 47,104 49,102 51 C100 52,98 53,95 53 C93 53,91 52,89 50 C88 48,87 45,87 41 C87 37,88 33,89 29 C91 25,93 22,96 20 C99 17,102 16,106 16 C107 16,109 16,110 17 C112 17,113 18,114 19 L114 19 Z M111 30 C111 29,112 28,112 27 C112 26,112 26,112 25 C112 23,111 21,111 20 C110 20,108 19,106 19 C104 19,102 20,100 22 C98 24,96 27,95 31 C94 34,93 38,93 41 C93 44,94 46,94 47 C95 48,96 49,98 49 C100 49,101 48,103 47 C104 46,106 43,107 41 C109 38,110 35,111 31 L111 30 Z" fill="#212121"/><path d="M151 43 C148 47,146 49,144 51 C142 52,140 53,137 53 C135 53,133 52,131 50 C130 48,129 45,129 41 C129 37,130 33,131 29 C133 25,135 22,138 20 C141 17,144 16,148 16 C149 16,151 16,152 17 C153 17,154 17,156 18 L158 10 C158 9,158 8,158 7 C158 6,158 6,158 5 C158 4,158 4,158 3 C158 3,157 3,157 2 C156 2,155 2,154 2 L155 0 L164 0 L166 0 L157 41 C156 43,156 45,156 46 C156 47,156 48,156 48 C157 49,157 49,158 49 C159 49,160 49,160 48 C161 48,162 47,164 45 L166 47 C164 49,162 51,160 52 C159 53,157 53,155 53 C154 53,153 53,152 52 C151 51,150 49,150 48 C150 47,151 45,151 44 L151 43 Z M153 30 C153 28,154 26,154 25 C154 23,153 21,152 20 C152 20,150 19,148 19 C146 19,144 20,142 22 C140 24,138 27,137 31 C136 34,135 38,135 41 C135 44,136 46,136 47 C137 48,138 49,140 49 C141 49,141 49,142 49 C143 48,144 48,145 47 C146 46,146 45,147 44 C148 43,149 42,149 41 C150 40,151 39,151 37 C152 35,152 34,153 31 L153 30 Z" fill="#212121"/><path d="M187 73 C186 74,185 75,184 76 C183 77,183 77,182 78 C181 78,180 79,179 79 C179 79,178 79,177 79 C175 79,173 79,172 78 C171 77,171 75,171 73 C171 73,171 72,171 71 C171 70,171 70,171 69 L174 55 L170 55 L170 53 C171 53,172 53,173 53 C174 53,174 52,174 52 C175 52,175 51,175 51 C175 51,176 50,176 50 C176 49,176 48,177 48 C177 47,177 46,178 45 L182 45 L180 52 L189 52 L189 55 L180 55 L177 66 C177 67,177 67,176 68 C176 69,176 69,176 69 C176 70,176 70,176 71 C176 71,176 71,176 72 C176 73,176 74,177 74 C177 75,178 75,179 75 C180 75,181 75,182 74 C183 74,184 73,185 71 L187 73 Z" fill="#212121"/><path d="M211 72 C210 73,209 74,208 75 C207 76,206 77,205 77 C205 78,204 78,203 79 C202 79,201 79,200 79 C198 79,196 78,195 77 C194 75,193 73,193 70 C193 69,194 67,194 66 C194 64,195 63,196 61 C196 60,197 59,198 57 C199 56,200 55,201 55 C202 54,203 53,205 53 C206 52,207 52,208 52 C210 52,211 52,212 53 C213 53,214 53,215 54 L218 52 L220 53 L216 69 C216 70,216 71,215 72 C215 72,215 73,215 73 C215 74,215 74,216 75 C216 75,216 75,217 75 C217 75,218 75,218 75 C218 75,219 75,219 74 C219 74,220 74,220 73 C221 73,221 72,222 72 C222 72,222 72,222 73 C223 73,223 73,223 74 C222 75,222 75,221 76 C220 77,219 77,219 78 C218 78,217 79,217 79 C216 79,215 79,215 79 C213 79,212 79,212 78 C211 77,211 76,211 75 C211 74,211 73,211 72 L211 72 Z M212 64 C213 63,213 62,213 61 C213 60,213 60,213 59 C213 57,213 57,212 56 C211 55,210 55,209 55 C208 55,207 55,206 56 C205 56,204 57,203 58 C203 58,202 59,201 60 C201 61,200 62,200 63 C200 64,199 65,199 67 C199 68,199 69,199 70 C199 72,199 73,200 74 C200 75,201 75,202 75 C203 75,203 75,204 75 C205 75,205 74,206 74 C207 73,209 71,210 70 C211 68,212 66,212 64 L212 64 Z" fill="#212121"/><path d="M238 59 C240 57,241 55,243 54 C245 53,247 52,248 52 C250 52,252 53,253 54 C254 55,254 56,254 58 C254 58,254 59,254 60 C254 61,253 62,253 63 C253 64,253 65,252 66 C252 67,252 69,251 69 C251 70,251 71,251 72 C251 73,251 73,251 73 C251 74,251 74,251 75 C251 75,252 75,252 75 C253 75,253 75,253 75 C254 75,254 75,254 74 C255 74,255 74,255 73 C256 73,256 72,257 72 L259 73 C258 75,257 75,256 76 C256 77,255 77,254 78 C254 78,253 79,252 79 C251 79,251 79,250 79 C249 79,249 79,248 79 C248 79,247 78,247 78 C246 77,246 77,246 76 C246 76,246 75,246 74 C246 74,246 73,246 72 C246 71,246 70,246 69 C247 68,247 67,247 66 C248 65,248 64,248 63 C248 62,248 62,249 61 C249 60,249 60,249 59 C249 58,249 57,248 57 C248 56,247 56,246 56 C246 56,245 56,244 56 C244 57,243 57,242 58 C242 58,241 59,241 59 C240 60,239 61,239 61 C238 62,238 63,238 64 C237 65,237 65,237 66 L234 79 L229 79 L233 62 C233 61,233 61,233 60 C233 59,233 58,233 58 C233 57,233 56,232 56 C231 56,231 56,231 56 C230 56,230 57,230 57 C229 57,229 58,228 58 C228 58,228 59,227 59 L225 58 C226 57,227 56,228 55 C228 54,229 54,230 53 C230 53,231 53,232 52 C232 52,233 52,234 52 C235 52,236 53,237 53 C238 54,238 55,238 57 C238 57,238 58,238 58 C238 58,238 59,238 59 L238 59 Z" fill="#212121"/><path d="M286 69 C286 70,286 70,285 71 C285 71,285 72,285 72 C285 72,285 72,285 73 C285 73,285 73,285 73 C285 74,285 74,285 75 C286 75,286 75,287 75 C287 75,287 75,288 75 C288 75,288 75,289 74 C289 74,289 74,290 73 C290 73,291 72,291 72 C292 72,292 72,292 73 C293 73,293 73,293 73 C292 75,291 75,291 76 C290 77,289 77,289 78 C288 78,287 79,287 79 C286 79,285 79,284 79 C284 79,283 79,283 79 C282 79,282 78,281 78 C281 77,281 77,280 76 C280 76,280 75,280 74 C280 74,280 73,280 72 C280 71,281 70,281 69 C281 68,281 67,282 66 C282 65,282 64,282 64 C283 63,283 62,283 61 C283 61,283 60,283 59 C283 59,283 59,283 58 C283 58,283 57,283 57 C282 57,282 57,282 56 C281 56,281 56,280 56 C280 56,279 56,278 57 C277 58,276 59,275 60 C274 61,273 62,272 63 C272 64,271 66,271 67 C270 69,270 71,270 73 C269 75,269 77,268 79 L263 79 L269 50 C270 49,270 48,270 48 C270 47,270 47,270 46 C270 46,270 45,270 45 C270 45,270 45,269 45 C269 44,269 44,268 44 C268 44,267 44,267 44 L267 42 L274 42 L276 42 L273 58 L273 58 C274 57,275 56,276 55 C276 55,277 54,278 54 C279 53,280 53,280 52 C281 52,282 52,283 52 C285 52,286 53,287 54 C288 55,288 56,288 58 C288 58,288 59,288 60 C288 60,288 61,288 62 L286 69 Z" fill="#212121"/><path d="M305 65 C305 72,306 77,308 81 C310 85,313 88,317 89 L317 91 C311 90,307 87,304 82 C302 78,300 72,300 66 C300 59,302 54,304 49 C307 44,311 41,317 40 L317 42 C313 44,310 46,308 50 C306 54,305 59,305 65 L305 65 Z" fill="#212121"/><path d="M364 46 C364 46,363 46,363 47 C363 47,362 47,362 47 C362 48,362 48,361 49 C361 50,361 51,360 52 L356 71 C356 72,356 73,356 73 C356 74,356 75,356 75 C356 76,356 76,356 77 C357 77,357 77,358 77 L358 79 L347 79 L347 77 C348 77,348 77,348 77 C349 77,349 76,349 76 C349 76,349 76,350 75 C350 74,350 74,350 73 C351 72,351 71,351 70 C351 69,352 68,352 67 C352 65,352 64,353 63 C353 62,353 61,353 59 C354 59,354 58,354 57 C354 56,354 55,355 55 C355 54,355 53,355 52 C355 51,356 51,356 50 L355 50 L340 75 L336 75 L333 50 L332 50 C332 51,332 52,332 54 C331 56,331 58,330 60 C330 61,330 62,330 63 C329 63,329 64,329 65 C329 66,329 67,328 68 C328 69,328 70,328 71 C328 72,327 73,327 73 C327 74,327 75,327 75 C327 76,327 76,328 77 C328 77,329 77,330 77 L330 79 L320 79 L320 77 C321 77,321 77,321 77 C322 77,322 76,322 76 C323 76,323 75,323 74 C323 74,324 73,324 71 L328 52 C329 51,329 51,329 50 C329 49,329 49,329 48 C329 47,329 47,328 47 C328 46,327 46,326 46 L326 44 L337 44 L340 68 L356 44 L365 44 L364 46 Z" fill="#212121"/><path d="M391 73 C389 75,387 77,385 78 C383 79,381 79,378 79 C377 79,376 79,374 78 C373 78,372 78,372 77 C371 76,370 75,370 74 C370 73,369 72,369 70 C369 69,370 68,370 67 C370 66,370 65,371 64 C371 63,371 62,372 61 C373 60,373 59,374 58 C375 56,377 55,379 54 C381 53,384 52,386 52 C389 52,390 53,392 54 C393 55,393 56,393 58 C393 64,387 67,375 67 C375 68,375 68,375 69 C375 69,375 70,375 70 C375 72,375 74,376 75 C377 76,378 76,380 76 C380 76,381 76,382 76 C383 76,384 75,384 75 C385 75,386 74,387 73 C387 73,388 72,389 71 L391 73 Z M375 64 C378 64,380 64,381 64 C383 63,384 63,385 62 C386 62,387 61,388 61 C388 60,388 59,388 58 C388 57,388 57,388 56 C387 55,386 55,385 55 C384 55,383 55,382 56 C381 56,380 57,380 58 C379 58,378 59,377 60 C377 61,376 63,375 64 L375 64 Z" fill="#212121"/><path d="M422 79 L416 79 C417 78,417 77,417 76 C418 74,418 73,418 72 C419 70,419 69,419 68 C419 67,420 65,420 64 C420 63,420 62,421 61 C421 60,421 60,421 59 C421 58,421 57,420 57 C420 56,419 56,418 56 C417 56,416 56,415 57 C414 58,413 58,412 59 C412 61,411 62,410 63 C409 64,409 65,409 67 L406 79 L401 79 L405 62 C405 61,405 60,405 60 C405 59,405 58,405 58 C405 57,405 56,404 56 C403 56,403 56,403 56 C402 56,402 57,402 57 C401 57,401 58,400 58 C400 58,400 59,399 59 L397 58 C398 57,399 56,400 55 C400 54,401 54,402 53 C402 53,403 53,404 52 C404 52,405 52,406 52 C407 52,408 53,409 53 C410 54,410 55,410 57 C410 57,410 58,410 58 C410 58,410 59,410 59 L410 59 C412 57,413 55,415 54 C417 53,418 52,420 52 C422 52,424 53,425 54 C425 55,426 56,426 58 C426 58,426 58,426 58 C426 58,426 59,426 59 L426 59 C428 56,429 55,431 54 C433 53,434 52,436 52 C438 52,439 53,440 54 C441 55,442 56,442 58 C442 58,442 59,441 60 C441 61,441 62,441 63 C441 64,440 65,440 66 C440 67,439 68,439 69 C439 70,439 70,439 71 C439 71,439 72,439 72 C438 72,438 72,438 73 C438 73,438 73,438 73 C438 74,438 74,439 75 C439 75,439 75,440 75 C440 75,441 75,441 75 C441 75,442 75,442 74 C442 74,443 74,443 73 C444 73,444 72,445 72 L446 73 C446 75,445 75,444 76 C443 77,443 77,442 78 C441 78,441 79,440 79 C439 79,438 79,438 79 C437 79,436 79,436 79 C435 79,435 78,434 78 C434 77,434 77,434 76 C433 76,433 75,433 74 C433 73,434 71,434 69 C435 68,435 67,435 66 C435 65,436 64,436 63 C436 62,436 62,436 61 C436 60,436 60,436 59 C436 58,436 57,436 57 C435 56,435 56,434 56 C433 56,433 56,432 57 C431 57,431 57,430 58 C430 58,429 59,428 59 C428 60,427 61,427 62 C426 62,426 63,425 64 C425 65,425 66,424 67 L422 79 Z" fill="#212121"/><path d="M461 65 C461 59,460 54,458 50 C456 46,453 44,449 42 L449 40 C455 41,459 44,462 49 C465 54,466 59,466 66 C466 72,465 78,462 82 C459 87,455 90,449 91 L449 89 C453 88,456 85,458 81 C460 77,461 72,461 65 L461 65 Z" fill="#212121"/><path d="M495 26 L495 21 L541 21 L541 26 L495 26 Z M495 41 L495 36 L541 36 L541 41 L495 41 Z" fill="#212121"/><path d="M597 41 C598 39,598 37,598 35 C598 34,598 33,597 33 C597 32,596 32,595 32 L595 30 L610 30 L609 32 C608 32,608 33,607 33 C607 33,606 33,606 34 C606 34,605 35,605 36 C605 37,604 39,604 41 L601 52 L598 53 L596 50 C594 52,592 52,590 53 C589 53,588 53,586 53 C581 53,578 52,576 49 C573 46,572 43,572 37 C572 34,573 29,574 25 C575 20,577 16,579 13 C581 9,584 6,588 4 C591 2,595 1,599 1 C601 1,603 1,605 2 C607 2,609 2,611 3 L609 13 L605 13 C605 10,605 8,603 6 C602 5,601 4,598 4 C596 4,594 5,592 6 C591 7,589 8,587 11 C586 13,584 15,583 19 C582 22,581 25,580 29 C579 32,579 36,579 39 C579 43,579 46,581 47 C582 49,584 50,587 50 C588 50,589 50,590 50 C591 50,592 49,593 49 C594 48,595 47,595 46 C596 45,597 44,597 43 L597 41 Z" fill="#212121"/><path d="M623 55 C623 56,622 56,622 57 C622 57,621 58,621 58 C621 59,621 59,621 60 C621 62,622 63,623 64 C624 65,626 65,629 65 C632 65,634 64,636 63 C638 62,640 60,641 57 C642 54,643 50,645 44 L644 44 C642 47,639 50,637 51 C635 52,633 53,631 53 C628 53,626 52,625 50 C623 48,622 45,622 41 C622 37,623 33,625 29 C627 25,629 22,632 20 C635 17,638 16,641 16 C643 16,644 16,646 17 C647 17,649 18,650 19 L653 16 L656 17 L652 34 C652 35,651 38,650 43 C649 48,648 52,648 54 C647 56,646 58,645 59 C644 61,643 63,642 64 C640 65,638 66,636 67 C634 68,632 68,629 68 C620 68,615 65,615 60 C615 59,616 58,616 57 C617 55,619 54,620 53 L623 55 Z M633 49 C635 49,636 49,637 48 C638 47,640 46,641 44 C643 42,644 40,644 38 C645 36,646 34,646 32 C647 29,647 27,647 25 C647 23,647 21,646 20 C645 20,644 19,642 19 C640 19,637 20,635 22 C633 24,632 27,631 31 C630 34,629 38,629 41 C629 44,629 46,630 47 C631 48,632 49,633 49 L633 49 Z" fill="#212121"/><path d="M676 26 C679 22,681 20,683 18 C685 17,688 16,690 16 C692 16,693 16,694 16 L693 25 L689 25 C689 24,688 23,688 23 C688 22,688 22,687 21 C687 21,686 21,686 21 C685 21,684 21,683 22 C682 23,681 24,679 26 C678 27,677 29,676 30 C676 32,675 34,674 36 L671 52 L665 52 L670 28 C670 27,671 26,671 25 C671 24,671 24,671 23 C671 22,671 21,670 21 C670 20,670 20,669 20 C668 20,667 20,666 21 C666 22,664 23,663 24 L661 22 C663 20,665 18,667 17 C668 17,670 16,672 16 C673 16,674 17,675 18 C676 19,676 20,676 21 C676 23,676 24,676 25 L676 26 Z" fill="#212121"/><path d="M727 19 L731 16 L733 17 L728 41 C727 43,727 45,727 46 C727 47,727 48,727 48 C728 49,728 49,729 49 C730 49,731 49,731 48 C732 48,733 47,735 45 L737 47 C735 49,733 51,731 52 C730 53,728 53,726 53 C725 53,724 53,723 52 C722 51,721 49,721 48 C721 47,722 45,722 44 L722 43 C719 47,717 49,715 51 C713 52,711 53,708 53 C706 53,704 52,702 50 C701 48,700 45,700 41 C700 37,701 33,702 29 C704 25,706 22,709 20 C712 17,715 16,719 16 C720 16,722 16,723 17 C725 17,726 18,727 19 L727 19 Z M724 30 C724 29,725 28,725 27 C725 26,725 26,725 25 C725 23,724 21,724 20 C723 20,721 19,719 19 C717 19,715 20,713 22 C711 24,709 27,708 31 C707 34,706 38,706 41 C706 44,707 46,707 47 C708 48,709 49,711 49 C713 49,714 48,716 47 C717 46,719 43,720 41 C722 38,723 35,724 31 L724 30 Z" fill="#212121"/><path d="M764 43 C761 47,759 49,757 51 C755 52,753 53,750 53 C748 53,746 52,744 50 C743 48,742 45,742 41 C742 37,743 33,744 29 C746 25,748 22,751 20 C754 17,757 16,761 16 C762 16,764 16,765 17 C766 17,767 17,769 18 L771 10 C771 9,771 8,771 7 C771 6,771 6,771 5 C771 4,771 4,771 3 C771 3,770 3,770 2 C769 2,768 2,767 2 L768 0 L777 0 L779 0 L770 41 C769 43,769 45,769 46 C769 47,769 48,769 48 C770 49,770 49,771 49 C772 49,773 49,773 48 C774 48,775 47,777 45 L779 47 C777 49,775 51,773 52 C772 53,770 53,768 53 C767 53,766 53,765 52 C764 51,763 49,763 48 C763 47,764 45,764 44 L764 43 Z M766 30 C766 28,767 26,767 25 C767 23,766 21,765 20 C765 20,763 19,761 19 C759 19,757 20,755 22 C753 24,751 27,750 31 C749 34,748 38,748 41 C748 44,749 46,749 47 C750 48,751 49,753 49 C754 49,754 49,755 49 C756 48,757 48,758 47 C759 46,759 45,760 44 C761 43,762 42,762 41 C763 40,764 39,764 37 C765 35,765 34,766 31 L766 30 Z" fill="#212121"/><path d="M794 76 C793 76,791 76,790 75 C788 75,787 74,786 73 C785 72,784 71,784 69 C783 68,783 66,783 64 C783 63,783 61,783 59 C784 58,784 56,785 54 C785 52,786 50,787 49 C788 47,789 46,791 44 C792 43,794 42,796 41 C798 40,800 40,802 40 C804 40,806 40,807 41 C809 42,810 42,811 43 C812 44,812 46,813 47 C813 49,814 50,814 52 C814 53,813 55,813 57 C813 58,812 60,812 62 C811 64,810 66,809 67 C808 69,807 70,806 72 C804 73,803 74,801 75 C799 76,797 76,794 76 L794 76 Z M788 65 C788 68,789 70,790 71 C791 72,792 73,795 73 C796 73,797 73,799 72 C800 71,801 70,802 69 C803 68,804 67,805 65 C805 64,806 62,807 60 C807 59,807 57,808 55 C808 54,808 52,808 51 C808 48,808 46,807 45 C806 44,804 43,802 43 C800 43,799 43,798 44 C796 45,795 46,794 47 C793 48,792 50,791 51 C791 53,790 55,790 56 C789 58,789 60,789 61 C788 63,788 64,788 65 L788 65 Z" fill="#212121"/><path d="M839 68 C837 71,835 72,834 73 C832 75,830 75,828 75 C827 75,825 75,824 74 C823 73,823 71,823 70 C823 69,823 69,823 68 C823 67,823 66,824 65 C824 64,824 63,824 62 C825 61,825 60,825 59 C825 58,826 57,826 56 C826 55,826 54,826 54 C826 53,826 52,825 52 C824 52,824 52,824 52 C823 52,823 53,822 53 C822 53,822 54,821 54 C821 54,820 55,820 55 L818 54 C819 53,820 52,821 51 C821 50,822 50,823 49 C823 49,824 49,825 48 C825 48,826 48,827 48 C828 48,829 49,830 49 C831 50,831 51,831 53 C831 53,831 54,831 55 C831 56,831 57,830 58 C830 60,829 63,829 64 C828 66,828 67,828 68 C828 69,828 70,829 70 C829 71,830 71,831 71 C832 71,832 71,833 70 C834 70,835 69,836 68 C837 67,838 66,839 65 C839 63,840 62,840 61 L843 48 L848 48 L844 65 C844 66,844 67,844 67 C844 68,844 69,844 69 C844 70,844 70,844 71 C844 71,845 71,845 71 C845 71,846 71,846 71 C846 71,847 71,847 70 C848 70,848 70,848 69 C849 69,849 68,850 68 L852 69 C851 71,850 71,849 72 C848 73,848 73,847 74 C846 74,846 75,845 75 C844 75,844 75,843 75 C842 75,841 75,840 74 C839 73,839 72,839 71 C839 70,839 70,839 69 C839 69,839 69,839 68 L839 68 Z" fill="#212121"/><path d="M874 69 C873 70,872 71,871 72 C870 73,870 73,869 74 C868 74,867 75,866 75 C866 75,865 75,864 75 C862 75,860 75,859 74 C858 73,858 71,858 69 C858 69,858 68,858 67 C858 66,858 66,858 65 L861 51 L857 51 L857 49 C858 49,859 49,860 49 C861 49,861 48,861 48 C862 48,862 47,862 47 C862 47,863 46,863 46 C863 45,863 44,864 44 C864 43,864 42,865 41 L869 41 L867 48 L876 48 L876 51 L867 51 L864 62 C864 63,864 63,863 64 C863 65,863 65,863 65 C863 66,863 66,863 67 C863 67,863 67,863 68 C863 69,863 70,864 70 C864 71,865 71,866 71 C867 71,868 71,869 70 C870 70,871 69,872 67 L874 69 Z" fill="#212121"/><path d="M928 36 L926 40 L916 33 L918 45 L913 45 L914 33 L904 40 L902 36 L912 31 L902 26 L904 22 L914 29 L913 17 L918 17 L916 29 L926 22 L928 26 L918 31 L928 36 Z" fill="#212121"/><path d="M981 1 C986 1,989 3,992 5 C995 8,996 12,996 17 C996 21,995 26,994 30 C993 35,991 39,988 43 C986 46,983 49,980 51 C977 52,973 53,970 53 C965 53,961 52,959 49 C956 46,955 42,955 37 C955 33,956 29,957 24 C958 20,960 16,962 12 C965 8,967 6,971 4 C974 2,977 1,981 1 L981 1 Z M989 16 C989 8,986 4,981 4 C978 4,976 5,973 7 C971 9,969 12,967 15 C966 19,964 23,963 27 C962 31,962 35,962 38 C962 46,965 50,970 50 C973 50,977 49,979 45 C982 42,984 38,986 32 C988 26,989 21,989 16 L989 16 Z" fill="#212121"/><path d="M1031 41 C1032 39,1032 37,1032 35 C1032 34,1032 33,1031 33 C1031 32,1030 32,1029 32 L1029 30 L1044 30 L1043 32 C1042 32,1042 33,1041 33 C1041 33,1040 33,1040 34 C1040 34,1039 35,1039 36 C1039 37,1038 39,1038 41 L1035 52 L1032 53 L1030 50 C1028 52,1026 52,1024 53 C1023 53,1022 53,1020 53 C1015 53,1012 52,1010 49 C1007 46,1006 43,1006 37 C1006 34,1007 29,1008 25 C1009 20,1011 16,1013 13 C1015 9,1018 6,1022 4 C1025 2,1029 1,1033 1 C1035 1,1037 1,1039 2 C1041 2,1043 2,1045 3 L1043 13 L1039 13 C1039 10,1039 8,1037 6 C1036 5,1035 4,1032 4 C1030 4,1028 5,1026 6 C1025 7,1023 8,1021 11 C1020 13,1018 15,1017 19 C1016 22,1015 25,1014 29 C1013 32,1013 36,1013 39 C1013 43,1013 46,1015 47 C1016 49,1018 50,1021 50 C1022 50,1023 50,1024 50 C1025 50,1026 49,1027 49 C1028 48,1029 47,1029 46 C1030 45,1031 44,1031 43 L1031 41 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">We haven't created a separate buffer for the activated memory state. However, we can easily count it by re-activating the corresponding state or by dividing the hidden state by the output gate value. I chose the second path, and the entire algorithm for distributing the error gradient at this site is expressed in the following code.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;branching&nbsp;of&nbsp;the&nbsp;algorithm&nbsp;across&nbsp;the&nbsp;computing&nbsp;device</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;calculate&nbsp;the&nbsp;gradient&nbsp;at&nbsp;the&nbsp;output&nbsp;of&nbsp;each&nbsp;internal&nbsp;layer</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">hidden</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;/&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">og</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1e-8</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;OutputGate&nbsp;gradient</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">og_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;memory&nbsp;gradient</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">&nbsp;*=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">og</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Before distributing the memory gradient to the rest of the internal layers, we must correct the resulting gradient by the derivative of the activation function.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;adjust&nbsp;the&nbsp;gradient&nbsp;to&nbsp;the&nbsp;derivative</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">&nbsp;*=&nbsp;</span><span class="f_Functions">MathPow</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">2</span><span class="f_CodeExample">)&nbsp;*&nbsp;(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">-1</span><span class="f_CodeExample">)&nbsp;+&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We continue to distribute the error gradient between the internal layers. We need to distribute the error gradient from the memory flow to three more internal layers. Moving along the information flow inside the </span><span class="f_Text" style="font-style: italic;">LSTM</span><span class="f_Text"> block in reverse, the first function we encounter is summation. The derivative of the sum is 1. Therefore, we pass the error gradient in both directions unchanged.</span></p>
<div class="p_Text" style="text-align: center;"><div style="margin:0 auto 0 auto;width:330px"><img class="help" alt="Error gradient distribution inside the LSTM block" title="Error gradient distribution inside the LSTM block" width="330" height="400" style="width:330px;height:400px;border:none" src="lstm_bp2.png"/><p style="text-align:center"><span class="f_ImageCaption">Error gradient distribution inside the LSTM block</span></p></div></div>
<p class="p_Text"><span class="f_Text">Next, in both directions, we encounter the product. The principles of propagating the gradient through the multiplication of two numbers have been explained in detail above, so there is no need to repeat them. I just want to remind you that, unlike all buffers from the stack, only the memory buffer was taken one step further back in history. I promised to clarify this point, and now is the most suitable time to do so. Take a look at the </span><span class="f_Text" style="font-style: italic;">LSTM</span><span class="f_Text"> block diagram. To refresh memory, we multiply the output of the Forget gate by the memory state transferred from the previous iteration. Hence, to determine the error gradient at the output of the Forget gate, we need to multiply the error gradient in the memory thread by the memory state of the previous iteration. It is the buffer of this state that we loaded at the start of the loop</span></p>
<p class="p_Text"><span class="f_Text">The MQL5 code of the described operations is presented below.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;InputGate&nbsp;gradient</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">ig_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">nc</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;NewContent&nbsp;gradient</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">nc_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">ig</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;ForgetGates&nbsp;gradient</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">fg_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">memory</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">This completes the thread separation block by computational operation unit, and we merge the threads of the algorithm. We set a stub for the OpenCL branch and move on.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We have already discussed the need to use the historical states of the inner layer result buffers. Now we need to put this into practice and fill the result buffers with relevant historical data.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;copy&nbsp;the&nbsp;corresponding&nbsp;historical&nbsp;data&nbsp;to&nbsp;the&nbsp;buffers&nbsp;of&nbsp;the&nbsp;internal&nbsp;layers</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cForgetGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetOutputs</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">fg</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cInputGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetOutputs</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">ig</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetOutputs</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">og</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cNewContent</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetOutputs</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">nc</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, we need to propagate the gradient from the output to the input of the internal neural layers. This functionality is easily implemented by the base class method. However, please note the following. All four internal neural layers use the same input data. We also need to put the error gradient together in the same buffer. The neural layer base class methods we developed earlier are constructed in such a way that they overwrite values. Therefore, we need to organize the process of summing the error gradients from each internal neural layer.</span></p>
<p class="p_Text"><span class="f_Text">First, we'll run a gradient through the Forget gate. Recall that in order to transfer the source data to the internal neural layers, we created a base layer of source data and after performing forward pass operations, we stored a pointer to it in the source data stack. This type of object already contains buffers for writing data and error gradients. So, now we just take this pointer and pass it in the parameters of the </span><span class="f_Text" style="font-style: italic;">CNeuronBase::CalcHiddenGradient</span><span class="f_Text"> method. After this, our base class method will execute and fill the error gradient buffer at the source data level for the forget gates. But it's only one gate, and we need to gather information from all of them. To avoid losing the computed error gradient when calling a similar method for other internal layers, we will copy the data into the </span><span class="f_Text" style="font-style: italic;">m_cInputGradient</span><span class="f_Text"> buffer which we created in advance for accumulating error gradients.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;propagate&nbsp;a&nbsp;gradient&nbsp;through&nbsp;the&nbsp;inner&nbsp;layers</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cForgetGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cInputGradient</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cInputGradient</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">new</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cInputGradient</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cInputGradient</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cInputGradient</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">BufferCreate</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cInputGradient</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Scaling</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cInputGradient</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SumArray</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We repeat the operations for the remaining internal layers. However, now we add the new values of the error gradient to the previously obtained values.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cInputGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cInputGradient</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SumArray</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cInputGradient</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SumArray</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cNewContent</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">SumArray</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cInputGradient</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Please note the following. While processing the first three internal layers we move values into the temporary buffer </span><span class="f_Text" style="font-style: italic;">m_cInputGradient</span><span class="f_Text">. However, while processing the last layer, we transfer the previously accumulated error gradient into the source data layer buffer. Thus, we keep the overall error gradient at the initial data layer along with the initial data itself in the same initial data layer. It will also be automatically saved in our stack. Recall what I wrote about objects and pointers to them.</span></p>
<p class="p_Text"><span class="f_Text">Here comes an interesting moment. Remember, why we did all this? Propagation of the error gradient across all elements of the neural network is necessary to have a reference for determining the direction and extent of weight matrix element adjustments to reduce the overall error of our neural network performance. Consequently, as a result of the operations of this method, we must:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li">Bring the error gradient to the previous layer, and</span></li>
<li class="p_li"><span class="f_li">Bring the error gradient to the weight matrices of the internal neural layers.</span></li>
</ul>
<p class="p_Text"><span class="f_Text">If we run the next iteration cycle in this state with new data for recalculating the error gradients of internal layers, we will simply replace the just-calculated values. However, we need to propagate the error gradients all the way to the weight matrices of the internal neural layers. Therefore, without waiting for a call from an external program, we call the </span><span class="f_Text" style="font-style: italic;">CNeuronBase::CalcDeltaWeights</span><span class="f_Text"> method for all internal layers, which will recalculate the gradient at the weight matrix level.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;project&nbsp;the&nbsp;gradient&nbsp;onto&nbsp;the&nbsp;weight&nbsp;matrices&nbsp;of&nbsp;the&nbsp;internal&nbsp;layers</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cForgetGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cInputGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cNewContent</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We pass the error gradient only from the current state to the previous neural layer. Historical data remains only for the internal user of the </span><span class="f_Text" style="font-style: italic;">LSTM</span><span class="f_Text"> block. Therefore, we check the iteration index and only then pass the error gradient to the buffer of the previous layer. Do not forget that our error gradient buffer at the source data level contains more data than the buffer of the previous layer. This is because it also contains the error gradient of the hidden state. Hence, we will transfer only the necessary part of the data to the previous layer.</span></p>
<p class="p_Text"><span class="f_Text">We transfer the remainder to the error gradient buffer of our </span><span class="f_Text" style="font-style: italic;">LSTM</span><span class="f_Text"> block. Remember, at the beginning of the loop, it was from this buffer that we took the error gradient to propagate throughout the LSTM block? It's time to prepare the initial data for the next iteration of our loop through the chronological iterations of the feed-forward pass and error gradient propagation.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;if&nbsp;the&nbsp;gradient&nbsp;of&nbsp;the&nbsp;current&nbsp;state&nbsp;is&nbsp;calculated,&nbsp;then&nbsp;transfer&nbsp;it&nbsp;to&nbsp;the&nbsp;previous&nbsp;layer</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;and&nbsp;write&nbsp;the&nbsp;hidden&nbsp;state&nbsp;gradient&nbsp;to&nbsp;the&nbsp;gradient&nbsp;buffer&nbsp;for&nbsp;a&nbsp;new&nbsp;iteration</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">inputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Split</span><span class="f_CodeExample">((</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;==&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">&nbsp;?&nbsp;</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">()&nbsp;:</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;inputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">()),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Total</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After the successful execution of all iterations, we exit the method with a positive result.</span></p>
<p class="p_Text"><span class="f_Text">We have gone through two of the most complex and intricate methods for constructing a recurrent </span><span class="f_Text" style="font-style: italic;">LSTM</span><span class="f_Text"> block algorithm. The rest of it will be much easier. For example, the </span><span class="f_Text" style="font-style: italic;">CalcDeltaWeights</span><span class="f_Text"> method. The functionality of this method involves passing the error gradient to the level of the weight matrix. The </span><span class="f_Text" style="font-style: italic;">LSTM</span><span class="f_Text"> block does not have any separate weight matrix. All parameters are located within the nested neural layers, and we have already brought the error gradient to the level of their weight matrices in the previous method. Therefore, we rewrite the method with an empty stub with a positive result.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)&nbsp;{&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Another backward pass method, </span><span class="f_Text" style="font-style: italic;">UpdateWeights</span><span class="f_Text">, is a method for updating the weights matrix. The method is also inherited from the neural layer base class and overridden as needed. </span><span class="f_Text" style="font-style: italic;">LSTM</span><span class="f_Text"> block unlike the previously discussed types of neural layers does not have a single weight matrix. Instead, internal neural layers with their own weight matrices are used. So we can't just use the method of the parent class and have to override it.</span></p>
<p class="p_Text"><span class="f_Text">The </span><span class="f_Text" style="font-style: italic;">CNeuronLSTM::UpdateWeights</span><span class="f_Text"> method from an external program receives the parameters required to execute the algorithm for updating the weight matrix and returns the logical value of the result of the method operations.</span></p>
<p class="p_Text"><span class="f_Text">Even though the method parameters do not include any object pointers, we still set up control structures at the beginning of the method. Here, we check the validity of pointers to internal neural layers and the value of the parameter indicating the depth of history analysis, which should be greater than 0.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronLSTM</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">batch_size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;&amp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;&amp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;check&nbsp;the&nbsp;state&nbsp;of&nbsp;objects</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cForgetGate</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cInputGate</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputGate</span><span class="f_CodeExample">&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cNewContent</span><span class="f_CodeExample">&nbsp;||&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iDepth</span><span class="f_CodeExample">&nbsp;&lt;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Please note the</span><span class="f_Text" style="font-style: italic;"> batch_size</span><span class="f_Text"> parameter. This parameter indicates the number of backpropagation iterations between weight updates. It is tracked by an external program and passed to the method in parameters. For an external program and for the neural network types considered earlier, the number of feed-forward and backpropagation passes is equal, as each feed-forward pass is followed by a backpropagation pass, in which the deviation of the estimated neural network result from the expected result is determined and the error gradient is propagated throughout the neural network. In the case of a recurrent block, the situation is slightly different: for each feed-forward pass, a recurrent block undergoes multiple iterations of backward passes, determined by the depth of the analyzed history. Consequently, we must adjust the batch size received from the external program to the depth of the historical data.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">batch</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">batch_size</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iDepth</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We can then use the methods to update the weight matrix by passing them the correct data in the parameters.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;update&nbsp;the&nbsp;weight&nbsp;matrices&nbsp;of&nbsp;the&nbsp;internal&nbsp;layers</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cForgetGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">batch</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cInputGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">batch</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputGate</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">batch</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cNewContent</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">batch</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After successfully updating the weight matrices of all internal neural layers, we exit the method with a positive result.</span></p>
<p class="p_Text"><span class="f_Text">This concludes our review of LSTM block backpropagation methods. We can move forward in building our system.</span></p>

</div>

</body>
</html>
