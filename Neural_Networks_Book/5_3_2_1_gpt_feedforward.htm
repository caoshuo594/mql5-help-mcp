<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>5.3.2.1 GPT feed-forward method</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="5_transformer.htm"> 5. Attention mechanisms </a> / <a class="h_m" href="5_3_gpt.htm"> 5.3 GPT architecture </a> / <a class="h_m" href="5_3_2_gpt_mql5.htm"> 5.3.2 Building a GPT model in MQL5 </a>/ 5.3.2.1 GPT feed-forward method
          </td>
          <td width="70" align="right">
          <a href="5_3_2_gpt_mql5.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="5_3_2_2_gpt_backprop.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H3"><span class="f_H3">5.3.2.1 GPT feed-forward method</span></p>
<p class="p_Text"><span class="f_Text">We continue our work on implementing the </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> algorithm proposed by the </span><span class="f_Text" style="font-style: italic;">OpenAI</span><span class="f_Text"> team. We have already created the basic skeleton of the class with objects to implement the algorithm. Now we are proceeding directly to its implementation. Yes, the class will utilize the familiar </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm, but with some implementation specifics.</span></p>
<p class="p_Text"><span class="f_Text">As in all the previously discussed classes, all the feed-forward functionality is implemented in the </span><span class="f_Text" style="font-style: italic;">CNeuronGPT::FeedForward</span><span class="f_Text"> method. As you know, this method is virtual, is inherited from the base neural network class, and is overridden in each class to implement a specific algorithm. In the method parameters, it receives a pointer to the object of the previous neural layer, which contains the initial data in its buffer for executing the algorithm.</span></p>
<p class="p_Text"><span class="f_Text">As in all previous implementations, we start the method with the control block. In this block, we check the validity of pointers to the objects involved in the method. This operation allows us to prevent many critical errors when accessing invalid objects.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronGPT</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;check&nbsp;the&nbsp;relevance&nbsp;of&nbsp;all&nbsp;objects</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">())</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, we increment the </span><span class="f_Text" style="font-style: italic;">m_iCurrentPosition</span><span class="f_Text"> index of the current object in the </span><span class="f_Text" style="font-style: italic;">Key </span><span class="f_Text">and </span><span class="f_Text" style="font-style: italic;">Value</span><span class="f_Text"> buffers. We need this pointer for organizing the stack in these buffers. In fact, the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm performs a weighted summation of different contexts into a single vector. According to the mathematical rules, rearranging the places of the summands does not change the sum. That is, it is absolutely irrelevant at which position of the data buffer the element is located. What's important is its presence. This is the disadvantage of this algorithm for handling timeseries, but also a plus for our implementation. When organizing the data stack in the </span><span class="f_Text" style="font-style: italic;">Key</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">Value</span><span class="f_Text"> buffers, we will not perform a costly full data shift. Instead, we will move the pointer along the stack and overwrite the data in the corresponding data buffer elements.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;increment&nbsp;the&nbsp;pointer&nbsp;to&nbsp;the&nbsp;current&nbsp;object&nbsp;in&nbsp;the&nbsp;data&nbsp;stack</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iCurrentPosition</span><span class="f_CodeExample">++;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iCurrentPosition</span><span class="f_CodeExample">&nbsp;&gt;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iCurrentPosition</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">The next straightforward step is taken to organize the correct functioning of our internal multi-layered architecture. The pointer to the previous neuron layer obtained in the parameters is needed only for the first internal layer. Further internal neural layers will use the output from the preceding internal neural layer as their input data. Therefore, for internal use, we introduce a local variable to store a pointer to the previous neural layer. Now we will assign it the pointer obtained from the method parameters, but after the iterations of each internal neural layer, we will write a new pointer into it. So, we can organize the loop operation through all internal neural layers. In this case, we will work with one object pointer variable in the loop body. In reality, each neural layer will access a buffer of its own input data.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevL</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">As mentioned before, the main functionality of our feed-forward method will be implemented within the body of the loop iterating through the internal neural layers. Therefore, the next step is to create such a loop. Right within the loop, we extract from the collection the pointer to the </span><span class="f_Text" style="font-style: italic;">Querys</span><span class="f_Text"> object corresponding to the current internal neural layer. We check the validity of the extracted pointer and then execute the feed-forward method of the corresponding object.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;run&nbsp;the&nbsp;loop&nbsp;through&nbsp;all&nbsp;internal&nbsp;layers</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevL</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Further functionality is not covered by the methods of internal objects. Therefore, as in previous </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> implementations, we will implement it within the body of the method. Here, it is important to remember that in all implementations of our library, we provided the user with the option to choose the device and the technology for performing mathematical operations. In this class, we will not deviate from our principles and will also implement algorithm separation based on the chosen computational device. But first, let's perform some preparatory work and extract pointers to the objects of the analyzed internal layer from the collections. Do not forget to validate the obtained pointers.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevL</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;initializing&nbsp;Scores</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Scores</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cScores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Scores</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;initializing&nbsp;AttentionOut</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, we split the algorithm based on the chosen computational device. In this chapter, we will discuss the organization of the process using standard MQL5 tools, and we will revisit the implementation of multi-threaded computations using the OpenCL technology in the following sections.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;branching&nbsp;of&nbsp;the&nbsp;algorithm&nbsp;by&nbsp;the&nbsp;computing&nbsp;device</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">array</span><span class="f_CodeExample">[];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Vsplit</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">3</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">array</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">array</span><span class="f_CodeExample">[</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iCurrentPosition</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">array</span><span class="f_CodeExample">[</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">2</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iCurrentPosition</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">As you may recall, during the execution of the feed-forward pass of the specified object, we simultaneously construct all the vectors for the </span><span class="f_Text" style="font-style: italic;">Query, Key, </span><span class="f_Text">and </span><span class="f_Text" style="font-style: italic;">Value</span><span class="f_Text"> tensors for all attention heads. In the next step, we move the vectors of the last two tensors to the corresponding stacks. For this purpose, we will divide the result buffer of the </span><span class="f_Text" style="font-style: italic;">Querys</span><span class="f_Text"> layer into 3 equal parts: query, key, and value. First, we copy the data into the appropriate data buffers. When copying data, we will use the </span><span class="f_Text" style="font-style: italic;">m_iCurrentPosition</span><span class="f_Text"> variable to determine the offset in the buffers.</span></p>
<p class="p_Text"><span class="f_Text">Then we will do a bit of preparatory work. To facilitate access to the elements of the objects, we will create local pointers to the result buffers of the internal neural layers for </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">Key.</span><span class="f_Text"> We will also prepare dynamic arrays to perform the computational part.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">out</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">out</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Init</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">array_keys</span><span class="f_CodeExample">[],&nbsp;</span><span class="f_CodeExample" style="color: #333333;">array_values</span><span class="f_CodeExample">[];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">array_querys</span><span class="f_CodeExample">[];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">values</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Similarly to the construction of the feed-forward algorithm in the previously discussed implementation of multi-head attention, we will split the data matrices according to the attention heads.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">array</span><span class="f_CodeExample">[</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">Vsplit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">array_querys</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Vsplit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">array_keys</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">values</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">values</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Vsplit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">array_values</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After that, we create a nested loop for computations. In it, we iterate through the attention heads used. Right here in the body, we extract the </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text"> vector and the </span><span class="f_Text" style="font-style: italic;">Keys </span><span class="f_Text">matrix of the analyzed attention head. We multiply them and divide the resulting vector by the square root of the dimension of the description vector for one element in the </span><span class="f_Text" style="font-style: italic;">Keys</span><span class="f_Text"> matrix. We normalize it using the </span><span class="f_Text" style="font-style: italic;">Softmax</span><span class="f_Text"> function.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;define&nbsp;Scores</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">=</span><span class="f_CodeExample" style="color: #333333;">array_querys</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">array_keys</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">())/</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Functions">sqrt</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;normalize&nbsp;Scores</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Activation</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">,</span><span class="f_Definition">AF_SOFTMAX</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Scores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Thus, after normalizing the data, the sum of all dependency coefficients will be equal to one. This gives us reason to expect a vector with appropriate characteristics at the output of the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> block. We save the normalized data in a buffer for later use during the backpropagation pass.</span></p>
<p class="p_Text"><span class="f_Text">After calculating and normalizing the dependency coefficient vector, we have all the necessary data to calculate the output values of the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> block. We multiply the normalized </span><span class="f_Text" style="font-style: italic;">Score</span><span class="f_Text"> vector by the </span><span class="f_Text" style="font-style: italic;">Value</span><span class="f_Text"> tensor. Then we copy the resulting vector into the local result matrix. </span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;attention&nbsp;block&nbsp;output</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">o</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">array_values</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">]);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">out</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">o</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">As a result of performing all iterations of the loop system in our </span><span class="f_Text" style="font-style: italic;">out</span><span class="f_Text"> matrix, the concatenated output of the </span><span class="f_Text" style="font-style: italic;">Multi-Heads Self-Attention</span><span class="f_Text"> block will be collected. We transfer them to the result buffer of the </span><span class="f_Text" style="font-style: italic;">AttentionOut</span><span class="f_Text"> neural layer to use in our algorithm later.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">out</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">out</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//&nbsp;OpenCL&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">This completes the operation separation block depending on the computing device. Next, we will use the methods of our internal objects.</span></p>
<p class="p_Text"><span class="f_Text">According to the </span><span class="f_Text" style="font-style: italic;">Multi-Heads Self-Attention</span><span class="f_Text"> algorithm, the next step is to create a single ordered weighted vector of results for the entire multi-head attention block from the concatenated output of all attention heads. For this purpose, the matrix </span><span class="f_Text" style="font-style: italic;">W</span><span class="f_Text" style="font-size: 7pt; font-style: italic; vertical-align: sub;">0</span><span class="f_Text"> is provided in the method algorithm. In contrast, we have assigned this functionality to the internal fully connected neural layer </span><span class="f_Text" style="font-style: italic;">W0</span><span class="f_Text">. We extract the pointer to the object of the corresponding neural layer and call its feed-forward method. To prevent critical errors, we must validate the pointer to the object before calling its method.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;weighted&nbsp;output&nbsp;of&nbsp;all&nbsp;heads&nbsp;of&nbsp;attention</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We are nearing the completion of the implementation of the </span><span class="f_Text" style="font-style: italic;">Multi-Heads Self-Attention</span><span class="f_Text"> block algorithm. According to the </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> model algorithm, we need to add the obtained result to the original data and normalize the result using the formulas.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:91px;height:47px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 364 188"><path d="M31 86 L35 83 L37 84 L32 108 C31 110,31 112,31 113 C31 114,31 115,31 115 C32 116,32 116,33 116 C34 116,35 116,35 115 C36 115,37 114,39 112 L41 114 C39 116,37 118,35 119 C34 120,32 120,30 120 C29 120,28 120,27 119 C26 118,25 116,25 115 C25 114,26 112,26 111 L26 110 C23 114,21 116,19 118 C17 119,15 120,12 120 C10 120,8 119,6 117 C5 115,4 112,4 108 C4 104,5 100,6 96 C8 92,10 89,13 87 C16 84,19 83,23 83 C24 83,26 83,27 84 C29 84,30 85,31 86 L31 86 Z M28 97 C28 96,29 95,29 94 C29 93,29 93,29 92 C29 90,28 88,28 87 C27 87,25 86,23 86 C21 86,19 87,17 89 C15 91,13 94,12 98 C11 101,10 105,10 108 C10 111,11 113,11 114 C12 115,13 116,15 116 C17 116,18 115,20 114 C21 113,23 110,24 108 C26 105,27 102,28 98 L28 97 Z" fill="#212121"/><path d="M35 75 L16 75 L16 71 L35 71 L35 75 Z" fill="#212121"/><path d="M70 93 L70 88 L116 88 L116 93 L70 93 Z M70 108 L70 103 L116 103 L116 108 L70 108 Z" fill="#212121"/><path d="M168 52 C168 54,168 55,168 55 C168 56,169 57,169 57 C169 58,170 58,171 58 C172 59,172 59,174 59 C175 59,176 59,178 59 L178 62 L151 62 L151 59 C154 59,156 59,157 59 C158 58,159 58,159 58 C160 57,160 57,161 56 C161 55,161 54,161 52 L161 22 C161 21,161 21,161 20 C160 20,160 20,159 20 C158 20,157 20,156 21 C154 22,153 23,151 24 L149 21 L166 11 L168 11 C168 13,168 17,168 21 L168 52 Z" fill="#212121"/><path d="M155 127 C155 125,156 123,156 122 C156 121,155 120,155 120 C154 119,153 119,151 119 L152 117 L161 117 L163 117 L157 141 L157 142 C160 139,162 136,164 135 C166 134,168 133,170 133 C172 133,174 134,175 135 C177 136,177 138,177 141 C177 142,177 145,176 147 L174 157 C173 160,173 162,173 163 C173 164,173 165,173 165 C174 166,174 166,175 166 C176 166,176 166,177 165 C178 165,179 164,181 162 L183 164 C180 166,178 168,177 169 C175 170,174 170,172 170 C170 170,169 169,168 168 C167 167,167 166,167 164 C167 162,167 160,168 157 L169 150 C170 148,170 146,171 145 C171 144,171 143,171 142 C171 140,171 139,170 138 C170 137,169 137,167 137 C166 137,165 138,164 138 C163 139,162 141,160 142 C159 144,158 146,157 147 C156 149,156 151,155 153 L152 169 L145 169 L155 127 Z" fill="#212121"/><rect x="142" y="95" width="42" height="5" fill="#212121"/><path d="M230 59 L254 94 L254 97 L227 133 L260 133 C263 133,265 132,266 130 C268 129,269 126,269 122 L273 122 L272 140 L217 140 L217 138 L247 98 L219 57 L219 55 L273 55 L273 72 L269 72 C268 68,267 65,265 63 C263 60,260 59,256 59 L230 59 Z" fill="#212121"/><path d="M252 27 C252 28,252 28,251 29 C251 29,251 30,251 30 C251 30,251 30,251 31 C251 31,251 31,251 31 C251 32,251 32,251 33 C252 33,252 33,253 33 C253 33,253 33,254 33 C254 33,254 33,255 32 C255 32,255 32,256 31 C256 31,257 30,257 30 C258 30,258 30,258 31 C259 31,259 31,259 31 C258 33,257 33,257 34 C256 35,255 35,255 36 C254 36,253 37,253 37 C252 37,251 37,250 37 C250 37,249 37,249 37 C248 37,248 36,247 36 C247 35,247 35,246 34 C246 34,246 33,246 32 C246 32,246 31,246 30 C246 29,247 28,247 27 C247 26,247 25,248 24 C248 23,248 22,248 22 C249 21,249 20,249 19 C249 19,249 18,249 17 C249 17,249 17,249 16 C249 16,249 15,249 15 C248 15,248 15,248 14 C247 14,247 14,246 14 C246 14,245 14,244 15 C243 16,242 17,241 18 C240 19,239 20,238 21 C238 22,237 24,237 25 C236 27,236 29,236 31 C235 33,235 35,234 37 L229 37 L235 8 C236 7,236 6,236 6 C236 5,236 5,236 4 C236 4,236 3,236 3 C236 3,236 3,235 3 C235 2,235 2,234 2 C234 2,233 2,233 2 L233 0 L240 0 L242 0 L239 16 L239 16 C240 15,241 14,242 13 C242 13,243 12,244 12 C245 11,246 11,246 10 C247 10,248 10,249 10 C251 10,252 11,253 12 C254 13,254 14,254 16 C254 16,254 17,254 18 C254 18,254 19,254 20 L252 27 Z" fill="#212121"/><path d="M214 147 L212 153 L207 153 L208 147 L214 147 Z M207 175 C207 176,207 177,207 178 C207 178,207 179,207 179 C207 180,207 180,207 181 C207 181,208 181,208 181 C209 181,209 181,209 181 C210 181,210 181,210 180 C211 180,211 180,211 179 C212 179,212 178,213 178 L215 180 C214 181,213 182,212 182 C211 183,211 183,210 184 C209 184,209 185,208 185 C207 185,207 185,206 185 C205 185,205 185,204 185 C204 185,203 184,203 184 C202 183,202 183,202 182 C202 182,201 181,201 180 C201 180,202 179,202 178 C202 177,202 176,202 175 C203 174,203 173,203 172 C203 171,204 170,204 169 C204 168,204 167,204 166 C205 165,205 164,205 163 C205 163,205 163,205 162 C205 162,205 162,205 162 C205 161,205 161,204 161 C204 160,203 160,202 160 L202 158 L210 158 L211 158 L207 175 Z" fill="#212121"/><path d="M223 165 L223 161 L253 161 L253 165 L223 165 Z M223 177 L223 173 L253 173 L253 177 L223 177 Z" fill="#212121"/><path d="M278 178 C278 179,278 179,278 180 C278 180,278 181,278 181 C278 181,278 181,278 182 C278 182,279 182,279 182 C279 182,279 182,280 183 C280 183,280 183,281 183 C281 183,282 183,283 183 C284 183,285 183,286 183 L286 185 L264 185 L264 183 C265 183,266 183,267 183 C268 183,268 183,269 183 C269 183,270 183,270 183 C271 182,271 182,271 182 C271 182,272 182,272 182 C272 182,272 181,272 181 C272 181,272 180,272 180 C272 180,272 179,272 178 L272 159 C272 159,272 158,272 158 C272 158,271 157,271 157 C270 157,270 158,268 158 C267 159,266 160,264 161 C264 160,264 160,264 160 C263 159,263 159,263 158 C265 157,267 156,270 155 C272 154,274 152,276 151 L278 151 C278 152,278 153,278 154 C278 155,278 155,278 156 C278 157,278 157,278 158 L278 178 Z" fill="#212121"/><path d="M334 86 L338 83 L340 84 L335 108 C334 110,334 112,334 113 C334 114,334 115,334 115 C335 116,335 116,336 116 C337 116,338 116,338 115 C339 115,340 114,342 112 L344 114 C342 116,340 118,338 119 C337 120,335 120,333 120 C332 120,331 120,330 119 C329 118,328 116,328 115 C328 114,329 112,329 111 L329 110 C326 114,324 116,322 118 C320 119,318 120,315 120 C313 120,311 119,309 117 C308 115,307 112,307 108 C307 104,308 100,309 96 C311 92,313 89,316 87 C319 84,322 83,326 83 C327 83,329 83,330 84 C332 84,333 85,334 86 L334 86 Z M331 97 C331 96,332 95,332 94 C332 93,332 93,332 92 C332 90,331 88,331 87 C330 87,328 86,326 86 C324 86,322 87,320 89 C318 91,316 94,315 98 C314 101,313 105,313 108 C313 111,314 113,314 114 C315 115,316 116,318 116 C320 116,321 115,323 114 C324 113,326 110,327 108 C329 105,330 102,331 98 L331 97 Z" fill="#212121"/><path d="M357 107 L355 113 L350 113 L351 107 L357 107 Z M350 135 C350 136,350 137,350 138 C350 138,350 139,350 139 C350 140,350 140,350 141 C350 141,351 141,351 141 C352 141,352 141,352 141 C353 141,353 141,353 140 C354 140,354 140,354 139 C355 139,355 138,356 138 L358 140 C357 141,356 142,355 142 C354 143,354 143,353 144 C352 144,352 145,351 145 C350 145,350 145,349 145 C348 145,348 145,347 145 C347 145,346 144,346 144 C345 143,345 143,345 142 C345 142,344 141,344 140 C344 140,345 139,345 138 C345 137,345 136,345 135 C346 134,346 133,346 132 C346 131,347 130,347 129 C347 128,347 127,347 126 C348 125,348 124,348 123 C348 123,348 123,348 122 C348 122,348 122,348 122 C348 121,348 121,347 121 C347 120,346 120,345 120 L345 118 L353 118 L354 118 L350 135 Z" fill="#212121"/></svg></span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:152px;height:87px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 608 348"><path d="M31 72 L35 69 L37 70 L32 94 C31 96,31 98,31 99 C31 100,31 101,31 101 C32 102,32 102,33 102 C34 102,35 102,35 101 C36 101,37 100,39 98 L41 100 C39 102,37 104,35 105 C34 106,32 106,30 106 C29 106,28 106,27 105 C26 104,25 102,25 101 C25 100,26 98,26 97 L26 96 C23 100,21 102,19 104 C17 105,15 106,12 106 C10 106,8 105,6 103 C5 101,4 98,4 94 C4 90,5 86,6 82 C8 78,10 75,13 73 C16 70,19 69,23 69 C24 69,26 69,27 70 C29 70,30 71,31 72 L31 72 Z M28 83 C28 82,29 81,29 80 C29 79,29 79,29 78 C29 76,28 74,28 73 C27 73,25 72,23 72 C21 72,19 73,17 75 C15 77,13 80,12 84 C11 87,10 91,10 94 C10 97,11 99,11 100 C12 101,13 102,15 102 C17 102,18 101,20 100 C21 99,23 96,24 94 C26 91,27 88,28 84 L28 83 Z" fill="#212121"/><path d="M28 52 L36 64 L33 66 L26 57 L26 57 L17 66 L15 64 L23 52 L28 52 Z" fill="#212121"/><path d="M70 79 L70 74 L116 74 L116 79 L70 79 Z M70 94 L70 89 L116 89 L116 94 L70 94 Z" fill="#212121"/><path d="M317 15 L321 12 L323 13 L318 37 C317 39,317 41,317 42 C317 43,317 44,317 44 C318 45,318 45,319 45 C320 45,321 45,321 44 C322 44,323 43,325 41 L327 43 C325 45,323 47,321 48 C320 49,318 49,316 49 C315 49,314 49,313 48 C312 47,311 45,311 44 C311 43,312 41,312 40 L312 39 C309 43,307 45,305 47 C303 48,301 49,298 49 C296 49,294 48,292 46 C291 44,290 41,290 37 C290 33,291 29,292 25 C294 21,296 18,299 16 C302 13,305 12,309 12 C310 12,312 12,313 13 C315 13,316 14,317 15 L317 15 Z M314 26 C314 25,315 24,315 23 C315 22,315 22,315 21 C315 19,314 17,314 16 C313 16,311 15,309 15 C307 15,305 16,303 18 C301 20,299 23,298 27 C297 30,296 34,296 37 C296 40,297 42,297 43 C298 44,299 45,301 45 C303 45,304 44,306 43 C307 42,309 39,310 37 C312 34,313 31,314 27 L314 26 Z" fill="#212121"/><path d="M352 29 L352 24 L398 24 L398 29 L352 29 Z" fill="#212121"/><path d="M451 15 L455 12 L457 13 L452 37 C451 39,451 41,451 42 C451 43,451 44,451 44 C452 45,452 45,453 45 C454 45,455 45,455 44 C456 44,457 43,459 41 L461 43 C459 45,457 47,455 48 C454 49,452 49,450 49 C449 49,448 49,447 48 C446 47,445 45,445 44 C445 43,446 41,446 40 L446 39 C443 43,441 45,439 47 C437 48,435 49,432 49 C430 49,428 48,426 46 C425 44,424 41,424 37 C424 33,425 29,426 25 C428 21,430 18,433 16 C436 13,439 12,443 12 C444 12,446 12,447 13 C449 13,450 14,451 15 L451 15 Z M448 26 C448 25,449 24,449 23 C449 22,449 22,449 21 C449 19,448 17,448 16 C447 16,445 15,443 15 C441 15,439 16,437 18 C435 20,433 23,432 27 C431 30,430 34,430 37 C430 40,431 42,431 43 C432 44,433 45,435 45 C437 45,438 44,440 43 C441 42,443 39,444 37 C446 34,447 31,448 27 L448 26 Z" fill="#212121"/><path d="M455 4 L436 4 L436 0 L455 0 L455 4 Z" fill="#212121"/><path d="M156 302 L175 337 L191 100 L202 100 L202 105 L195 105 L178 351 L175 351 L152 308 L145 312 L143 309 L156 302 Z" fill="#212121"/><path d="M220 204 C220 205,220 205,220 206 C220 206,220 207,220 207 C220 207,220 207,220 208 C220 208,221 208,221 208 C221 208,221 208,222 209 C222 209,222 209,223 209 C223 209,224 209,225 209 C226 209,227 209,228 209 L228 211 L206 211 L206 209 C207 209,208 209,209 209 C210 209,210 209,211 209 C211 209,212 209,212 209 C213 208,213 208,213 208 C213 208,214 208,214 208 C214 208,214 207,214 207 C214 207,214 206,214 206 C214 206,214 205,214 204 L214 185 C214 185,214 184,214 184 C214 184,213 183,213 183 C212 183,212 184,210 184 C209 185,208 186,206 187 C206 186,206 186,206 186 C205 185,205 185,205 184 C207 183,209 182,212 181 C214 180,216 178,218 177 L220 177 C220 178,220 179,220 180 C220 181,220 181,220 182 C220 183,220 183,220 184 L220 204 Z" fill="#212121"/><path d="M225 283 C225 284,225 284,224 285 C224 285,224 286,224 286 C224 286,224 286,224 287 C224 287,224 287,224 287 C224 288,224 288,224 289 C225 289,225 289,226 289 C226 289,226 289,227 289 C227 289,227 289,228 288 C228 288,228 288,229 287 C229 287,230 286,230 286 C231 286,231 286,231 287 C232 287,232 287,232 287 C231 289,230 289,230 290 C229 291,228 291,228 292 C227 292,226 293,226 293 C225 293,224 293,223 293 C223 293,222 293,222 293 C221 293,221 292,220 292 C220 291,220 291,219 290 C219 290,219 289,219 288 C219 288,219 287,219 286 C219 285,220 284,220 283 C220 282,220 281,221 280 C221 279,221 278,221 278 C222 277,222 276,222 275 C222 275,222 274,222 273 C222 273,222 273,222 272 C222 272,222 271,222 271 C221 271,221 271,221 270 C220 270,220 270,219 270 C219 270,218 270,217 271 C216 272,215 273,214 274 C213 275,212 276,211 277 C211 278,210 280,210 281 C209 283,209 285,209 287 C208 289,208 291,207 293 L202 293 L208 264 C209 263,209 262,209 262 C209 261,209 261,209 260 C209 260,209 259,209 259 C209 259,209 259,208 259 C208 258,208 258,207 258 C207 258,206 258,206 258 L206 256 L213 256 L215 256 L212 272 L212 272 C213 271,214 270,215 269 C215 269,216 268,217 268 C218 267,219 267,219 266 C220 266,221 266,222 266 C224 266,225 267,226 268 C227 269,227 270,227 272 C227 272,227 273,227 274 C227 274,227 275,227 276 L225 283 Z" fill="#212121"/><rect x="199" y="231" width="34" height="5" fill="#212121"/><path d="M279 207 L296 234 L296 237 L277 264 L301 264 C302 264,303 264,304 264 C305 264,306 263,306 263 C307 262,307 262,308 261 C308 260,308 259,309 258 C309 257,310 255,310 253 L314 253 L313 271 L267 271 L267 269 L289 238 L268 205 L268 203 L313 203 L313 219 L309 219 C308 217,308 215,307 214 C307 212,306 211,306 210 C305 210,305 209,304 208 C303 208,303 208,302 207 C301 207,300 207,299 207 L279 207 Z" fill="#212121"/><path d="M301 175 C301 176,301 176,300 177 C300 177,300 178,300 178 C300 178,300 178,300 179 C300 179,300 179,300 179 C300 180,300 180,300 181 C301 181,301 181,302 181 C302 181,302 181,303 181 C303 181,303 181,304 180 C304 180,304 180,305 179 C305 179,306 178,306 178 C307 178,307 178,307 179 C308 179,308 179,308 179 C307 181,306 181,306 182 C305 183,304 183,304 184 C303 184,302 185,302 185 C301 185,300 185,299 185 C299 185,298 185,298 185 C297 185,297 184,296 184 C296 183,296 183,295 182 C295 182,295 181,295 180 C295 180,295 179,295 178 C295 177,296 176,296 175 C296 174,296 173,297 172 C297 171,297 170,297 170 C298 169,298 168,298 167 C298 167,298 166,298 165 C298 165,298 165,298 164 C298 164,298 163,298 163 C297 163,297 163,297 162 C296 162,296 162,295 162 C295 162,294 162,293 163 C292 164,291 165,290 166 C289 167,288 168,287 169 C287 170,286 172,286 173 C285 175,285 177,285 179 C284 181,284 183,283 185 L278 185 L284 156 C285 155,285 154,285 154 C285 153,285 153,285 152 C285 152,285 151,285 151 C285 151,285 151,284 151 C284 150,284 150,283 150 C283 150,282 150,282 150 L282 148 L289 148 L291 148 L288 164 L288 164 C289 163,290 162,291 161 C291 161,292 160,293 160 C294 159,295 159,295 158 C296 158,297 158,298 158 C300 158,301 159,302 160 C303 161,303 162,303 164 C303 164,303 165,303 166 C303 166,303 167,303 168 L301 175 Z" fill="#212121"/><path d="M263 278 L261 284 L256 284 L257 278 L263 278 Z M256 306 C256 307,256 308,256 309 C256 309,256 310,256 310 C256 311,256 311,256 312 C256 312,257 312,257 312 C258 312,258 312,258 312 C259 312,259 312,259 311 C260 311,260 311,260 310 C261 310,261 309,262 309 L264 311 C263 312,262 313,261 313 C260 314,260 314,259 315 C258 315,258 316,257 316 C256 316,256 316,255 316 C254 316,254 316,253 316 C253 316,252 315,252 315 C251 314,251 314,251 313 C251 313,250 312,250 311 C250 311,251 310,251 309 C251 308,251 307,251 306 C252 305,252 304,252 303 C252 302,253 301,253 300 C253 299,253 298,253 297 C254 296,254 295,254 294 C254 294,254 294,254 293 C254 293,254 293,254 293 C254 292,254 292,253 292 C253 291,252 291,251 291 L251 289 L259 289 L260 289 L256 306 Z" fill="#212121"/><path d="M272 296 L272 292 L302 292 L302 296 L272 296 Z M272 308 L272 304 L302 304 L302 308 L272 308 Z" fill="#212121"/><path d="M327 309 C327 310,327 310,327 311 C327 311,327 312,327 312 C327 312,327 312,327 313 C327 313,328 313,328 313 C328 313,328 313,329 314 C329 314,329 314,330 314 C330 314,331 314,332 314 C333 314,334 314,335 314 L335 316 L313 316 L313 314 C314 314,315 314,316 314 C317 314,317 314,318 314 C318 314,319 314,319 314 C320 313,320 313,320 313 C320 313,321 313,321 313 C321 313,321 312,321 312 C321 312,321 311,321 311 C321 311,321 310,321 309 L321 290 C321 290,321 289,321 289 C321 289,320 288,320 288 C319 288,319 289,317 289 C316 290,315 291,313 292 C313 291,313 291,313 291 C312 290,312 290,312 289 C314 288,316 287,319 286 C321 285,323 283,325 282 L327 282 C327 283,327 284,327 285 C327 286,327 286,327 287 C327 288,327 288,327 289 L327 309 Z" fill="#212121"/><path d="M352 237 C352 245,353 253,356 258 C359 264,363 267,369 269 L368 272 C361 270,355 266,352 260 C348 254,346 246,346 237 C346 228,348 221,352 214 C355 208,361 204,368 202 L369 205 C363 207,359 211,356 216 C353 222,352 228,352 237 L352 237 Z" fill="#212121"/><path d="M401 222 L405 219 L407 220 L402 244 C401 246,401 248,401 249 C401 250,401 251,401 251 C402 252,402 252,403 252 C404 252,405 252,405 251 C406 251,407 250,409 248 L411 250 C409 252,407 254,405 255 C404 256,402 256,400 256 C399 256,398 256,397 255 C396 254,395 252,395 251 C395 250,396 248,396 247 L396 246 C393 250,391 252,389 254 C387 255,385 256,382 256 C380 256,378 255,376 253 C375 251,374 248,374 244 C374 240,375 236,376 232 C378 228,380 225,383 223 C386 220,389 219,393 219 C394 219,396 219,397 220 C399 220,400 221,401 222 L401 222 Z M398 233 C398 232,399 231,399 230 C399 229,399 229,399 228 C399 226,398 224,398 223 C397 223,395 222,393 222 C391 222,389 223,387 225 C385 227,383 230,382 234 C381 237,380 241,380 244 C380 247,381 249,381 250 C382 251,383 252,385 252 C387 252,388 251,390 250 C391 249,393 246,394 244 C396 241,397 238,398 234 L398 233 Z" fill="#212121"/><path d="M436 236 L436 231 L482 231 L482 236 L436 236 Z" fill="#212121"/><path d="M535 222 L539 219 L541 220 L536 244 C535 246,535 248,535 249 C535 250,535 251,535 251 C536 252,536 252,537 252 C538 252,539 252,539 251 C540 251,541 250,543 248 L545 250 C543 252,541 254,539 255 C538 256,536 256,534 256 C533 256,532 256,531 255 C530 254,529 252,529 251 C529 250,530 248,530 247 L530 246 C527 250,525 252,523 254 C521 255,519 256,516 256 C514 256,512 255,510 253 C509 251,508 248,508 244 C508 240,509 236,510 232 C512 228,514 225,517 223 C520 220,523 219,527 219 C528 219,530 219,531 220 C533 220,534 221,535 222 L535 222 Z M532 233 C532 232,533 231,533 230 C533 229,533 229,533 228 C533 226,532 224,532 223 C531 223,529 222,527 222 C525 222,523 223,521 225 C519 227,517 230,516 234 C515 237,514 241,514 244 C514 247,515 249,515 250 C516 251,517 252,519 252 C521 252,522 251,524 250 C525 249,527 246,528 244 C530 241,531 238,532 234 L532 233 Z" fill="#212121"/><path d="M539 211 L520 211 L520 207 L539 207 L539 211 Z" fill="#212121"/><path d="M566 237 C566 228,565 222,562 216 C559 211,555 207,549 205 L550 202 C557 204,563 208,566 214 C570 221,572 228,572 237 C572 246,570 254,566 260 C563 266,557 270,550 272 L549 269 C555 267,559 264,562 258 C565 253,566 245,566 237 L566 237 Z" fill="#212121"/><path d="M597 215 C597 215,598 215,599 215 C599 215,599 215,600 215 C600 214,600 214,601 214 C601 213,601 213,601 212 L604 212 C604 213,604 214,604 216 C604 217,604 218,603 219 L581 219 L581 218 C581 217,582 216,582 215 C583 214,583 213,584 212 C585 211,586 210,587 209 C587 208,588 207,590 206 C591 204,593 203,594 202 C595 200,596 199,596 198 C597 197,597 197,597 196 C598 195,598 194,598 193 C598 192,598 192,597 191 C597 190,597 189,596 189 C596 188,595 188,594 188 C594 187,593 187,592 187 C590 187,589 188,588 188 C587 189,586 190,585 192 L582 192 L582 187 C584 186,586 185,588 185 C590 184,592 184,593 184 C595 184,597 184,598 185 C599 185,600 186,601 186 C602 187,602 188,603 189 C603 190,603 191,603 192 C603 193,603 194,603 194 C603 195,603 196,603 196 C602 197,602 197,602 198 C601 199,601 199,600 200 C600 201,599 201,599 202 C598 203,597 204,596 204 C595 205,594 206,594 207 C593 208,592 209,591 210 C590 211,589 212,588 213 C588 214,587 214,587 215 L597 215 Z" fill="#212121"/><rect x="199" y="100" width="408" height="5" fill="#212121"/><rect x="142" y="81" width="465" height="5" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">First, we call the </span><span class="f_Text" style="font-style: italic;">CBufferType::SumArray</span><span class="f_Text"> method of summarizing two buffers. Then we normalize the data using the </span><span class="f_Text" style="font-style: italic;">CNeuronGPT::NormlizeBuffer</span><span class="f_Text"> method. Its algorithm completely repeats the relevant method of the </span><span class="f_Text" style="font-style: italic;">CNeuronAttention</span><span class="f_Text"> class.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;add&nbsp;to&nbsp;the&nbsp;input&nbsp;data&nbsp;and&nbsp;normalize</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">SumArray</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">NormlizeBuffer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">(),&nbsp;</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_dStd</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">]),&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After successfully normalizing all the data, we will pass the signal through two internal neural layers of the </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> block. This operation is straightforward: we sequentially extract pointers to the respective neural layer objects, validate the pointers, and call the feed-forward method for each internal layer.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;forward&nbsp;pass&nbsp;of&nbsp;Feed&nbsp;Forward&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">FF1</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">FF1</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">FF1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">FF1</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Finally, we add the result of the </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> block to the result of the </span><span class="f_Text" style="font-style: italic;">Multi-Heads Self-Attention</span><span class="f_Text"> block. Then we normalize the obtained values</span><span class="f_Text" style="font-style: italic;">.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;perform&nbsp;summation&nbsp;with&nbsp;the&nbsp;attention&nbsp;output&nbsp;and&nbsp;normalizing</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prev</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">prev</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SumArray</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">NormlizeBuffer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prev</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_dStd</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">]),&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">This completes the feed-forward pass for one internal layer. We can proceed to the next iteration of the loop and the next internal neural layer. But first, we need to change the pointer to the neural layer of the initial data, as we discussed at the beginning of the method. The results of the forward pass are contained in the buffer of the internal neural layer </span><span class="f_Text" style="font-style: italic;">FF2</span><span class="f_Text">. We write the pointer to it into the local variable </span><span class="f_Text" style="font-style: italic;">prevL</span><span class="f_Text">, with which we work at the next iteration of the loop.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">prevL</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">So, upon completing all iterations of the nested neural layer enumeration loop, we obtain a complete recalculation of the feed-forward pass for our block. To change the number of such neural layers, we only need to modify one parameter when calling the initialization method of the </span><span class="f_Text" style="font-style: italic;">CNeuronGPT</span><span class="f_Text"> class in the </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> model.</span></p>
<p class="p_Text"><span class="f_Text">With this, we conclude the work on the feed-forward pass method and move on to organizing the backpropagation process.</span></p>

</div>

</body>
</html>
