<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>1.3 Weight initialization methods in neural networks</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="1_about_ai.htm"> 1. Basic principles of artificial intelligence construction </a>/ 1.3 Weight initialization methods in neural networks
          </td>
          <td width="70" align="right">
          <a href="1_2_activation.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="1_4_study.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H2"><span class="f_H2">1.3 Weight initialization methods in neural networks</span></p>
<p class="p_Text"><span class="f_Text">When creating a neural network, before its first training run, we need to somehow set the initial weight. This seemingly simple task is of great importance for the subsequent training of the neural network and, in general, has a significant impact on the result of the entire work.</span></p>
<p class="p_Text"><span class="f_Text">The fact is that the gradient descent method, which is most often used for training neural networks, cannot distinguish the local minima of a function from its global minimum. In practice, various solutions are applied to minimize this problem, and we will talk about them a bit later. However, the question remains open.</span></p>
<p class="p_Text"><span class="f_Text">The second point is that the gradient descent method is an iterative process. Therefore, the total training time for a neural network directly depends on how far from the endpoint we are at the beginning.</span></p>
<p class="p_Text"><span class="f_Text">Moreover, let's not forget about the laws of mathematics and the peculiarities of the activation functions that we discussed in the previous section of this book.</span></p>
<p class="p_H3"><span class="f_H3">Initializing weights with a single value</span></p>
<p class="p_Text"><span class="f_Text">Probably the first thing that comes to mind is to take a certain constant (0 or 1) and initialize all weights with a single value. Unfortunately, this is far from the best option, which is related to the laws of mathematics. </span></p>
<p class="p_Text"><span class="f_Text">Using zero as a synaptic coefficient is often fatal to neural networks. In this case, the weighted sum of the input data would be zero. As we know from the previous section, many versions of the activation function in such a case return 0, and the neuron remains deactivated. Consequently, no signal goes further down the neural network.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:146px;height:61px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 584 244"><path d="M6 127 C6 130,7 132,8 134 C10 135,12 136,15 136 C18 136,21 135,23 133 C24 131,25 129,25 125 C25 124,25 122,25 121 C24 120,23 119,22 118 C21 117,20 116,18 114 C16 113,14 112,13 110 C12 109,11 108,10 107 C10 105,9 103,9 102 C9 99,10 96,11 94 C13 92,15 90,17 89 C20 88,23 87,26 87 C28 87,30 87,32 88 C34 88,36 88,39 89 L37 98 L33 98 C33 96,33 95,32 93 C32 92,31 91,30 91 C29 90,27 90,26 90 C24 90,22 91,20 91 C19 92,18 93,17 95 C16 96,16 98,16 100 C16 101,16 103,17 104 C18 106,20 108,23 109 C25 111,27 113,28 114 C29 115,30 117,31 118 C32 120,32 122,32 123 C32 127,31 129,30 132 C28 134,26 136,24 137 C21 138,18 139,15 139 C13 139,10 139,8 139 C5 138,3 138,1 137 L3 127 L6 127 Z" fill="#212121"/><path d="M51 162 C47 162,44 161,42 158 C40 155,39 150,39 144 C39 141,39 139,40 136 C40 134,41 132,42 131 C43 129,45 128,46 127 C48 126,49 126,51 126 C53 126,55 127,57 127 C58 128,59 129,60 131 C61 132,62 134,62 136 C63 139,63 141,63 144 C63 150,62 154,60 157 C58 161,55 162,51 162 L51 162 Z M44 144 C44 146,45 149,45 151 C45 153,46 154,46 155 C47 157,47 158,48 158 C49 159,50 159,51 159 C52 159,53 159,54 158 C55 158,55 157,56 156 C56 155,57 153,57 151 C57 149,57 147,57 145 C57 142,57 140,57 138 C57 136,56 134,56 133 C55 132,55 131,54 130 C53 129,52 129,51 129 C50 129,49 129,48 130 C47 131,46 132,46 133 C45 134,45 136,45 138 C45 139,44 141,44 144 L44 144 Z" fill="#212121"/><path d="M91 112 L91 107 L137 107 L137 112 L91 112 Z M91 127 L91 122 L137 122 L137 127 L91 127 Z" fill="#212121"/><path d="M180 50 L224 109 L224 114 L177 177 L240 177 C243 177,245 176,247 174 C248 172,249 168,250 161 L256 161 L254 188 L164 188 L164 184 L215 115 L165 49 L165 45 L254 45 L254 67 L249 67 C249 62,247 58,246 55 C244 52,241 50,238 50 L180 50 Z" fill="#212121"/><path d="M208 7 C210 5,211 3,213 2 C215 1,217 0,218 0 C220 0,222 1,223 2 C224 3,224 4,224 6 C224 6,224 7,224 8 C224 9,223 10,223 11 C223 12,223 13,222 14 C222 15,222 17,221 17 C221 18,221 19,221 20 C221 21,221 21,221 21 C221 22,221 22,221 23 C221 23,222 23,222 23 C223 23,223 23,223 23 C224 23,224 23,224 22 C225 22,225 22,225 21 C226 21,226 20,227 20 L229 21 C228 23,227 23,226 24 C226 25,225 25,224 26 C224 26,223 27,222 27 C221 27,221 27,220 27 C219 27,219 27,218 27 C218 27,217 26,217 26 C216 25,216 25,216 24 C216 24,216 23,216 22 C216 22,216 21,216 20 C216 19,216 18,216 17 C217 16,217 15,217 14 C218 13,218 12,218 11 C218 10,218 10,219 9 C219 8,219 8,219 7 C219 6,219 5,218 5 C218 4,217 4,216 4 C216 4,215 4,214 4 C214 5,213 5,212 6 C212 6,211 7,211 7 C210 8,209 9,209 9 C208 10,208 11,208 12 C207 13,207 13,207 14 L204 27 L199 27 L203 10 C203 9,203 9,203 8 C203 7,203 6,203 6 C203 5,203 4,202 4 C201 4,201 4,201 4 C200 4,200 5,200 5 C199 5,199 6,198 6 C198 6,198 7,197 7 L195 6 C196 5,197 4,198 3 C198 2,199 2,200 1 C200 1,201 1,202 0 C202 0,203 0,204 0 C205 0,206 1,207 1 C208 2,208 3,208 5 C208 5,208 6,208 6 C208 6,208 7,208 7 L208 7 Z" fill="#212121"/><path d="M183 195 L181 201 L176 201 L177 195 L183 195 Z M176 223 C176 224,176 225,176 226 C176 226,176 227,176 227 C176 228,176 228,176 229 C176 229,177 229,177 229 C178 229,178 229,178 229 C179 229,179 229,179 228 C180 228,180 228,180 227 C181 227,181 226,182 226 L184 228 C183 229,182 230,181 230 C180 231,180 231,179 232 C178 232,178 233,177 233 C176 233,176 233,175 233 C174 233,174 233,173 233 C173 233,172 232,172 232 C171 231,171 231,171 230 C171 230,170 229,170 228 C170 228,171 227,171 226 C171 225,171 224,171 223 C172 222,172 221,172 220 C172 219,173 218,173 217 C173 216,173 215,173 214 C174 213,174 212,174 211 C174 211,174 211,174 210 C174 210,174 210,174 210 C174 209,174 209,173 209 C173 208,172 208,171 208 L171 206 L179 206 L180 206 L176 223 Z" fill="#212121"/><path d="M192 213 L192 209 L222 209 L222 213 L192 213 Z M192 225 L192 221 L222 221 L222 225 L192 225 Z" fill="#212121"/><path d="M247 226 C247 227,247 227,247 228 C247 228,247 229,247 229 C247 229,247 229,247 230 C247 230,248 230,248 230 C248 230,248 230,249 231 C249 231,249 231,250 231 C250 231,251 231,252 231 C253 231,254 231,255 231 L255 233 L233 233 L233 231 C234 231,235 231,236 231 C237 231,237 231,238 231 C238 231,239 231,239 231 C240 230,240 230,240 230 C240 230,241 230,241 230 C241 230,241 229,241 229 C241 229,241 228,241 228 C241 228,241 227,241 226 L241 207 C241 207,241 206,241 206 C241 206,240 205,240 205 C239 205,239 206,237 206 C236 207,235 208,233 209 C233 208,233 208,233 208 C232 207,232 207,232 206 C234 205,236 204,239 203 C241 202,243 200,245 199 L247 199 C247 200,247 201,247 202 C247 203,247 203,247 204 C247 205,247 205,247 206 L247 226 Z" fill="#212121"/><path d="M296 139 C290 139,286 137,284 133 C281 129,280 122,280 114 C280 109,280 105,281 102 C282 98,283 96,285 93 C286 91,288 90,290 89 C292 88,294 87,296 87 C302 87,306 89,308 93 C311 98,312 104,312 112 C312 121,311 128,308 132 C305 137,301 139,296 139 L296 139 Z M287 112 C287 121,288 127,289 130 C291 134,293 136,296 136 C299 136,301 134,303 131 C304 127,305 122,305 114 C305 108,305 104,304 100 C303 97,302 94,301 92 C300 91,298 90,296 90 C294 90,293 91,292 92 C291 93,290 94,289 96 C288 98,288 100,287 103 C287 106,287 109,287 112 L287 112 Z" fill="#212121"/><path d="M365 122 L363 126 L353 119 L355 131 L350 131 L351 119 L341 126 L339 122 L349 117 L339 112 L341 108 L351 115 L350 103 L355 103 L353 115 L363 108 L365 112 L355 117 L365 122 Z" fill="#212121"/><path d="M404 121 C404 119,404 118,403 116 C403 114,403 112,402 111 C402 109,401 108,401 107 C401 107,401 107,400 106 C400 106,400 106,399 106 C399 106,398 106,398 106 C397 107,397 107,396 108 C396 108,395 109,394 110 L392 108 C394 106,395 105,396 104 C398 103,400 102,401 102 C402 102,403 102,403 102 C404 103,405 103,405 103 C405 104,406 104,406 105 C407 105,407 106,407 107 C408 108,408 109,408 111 C408 112,409 114,409 115 L409 115 C412 112,413 109,414 108 C416 106,417 105,418 104 C418 103,419 103,420 103 C421 102,422 102,423 102 C424 102,425 102,426 103 L424 110 L422 110 C422 109,421 108,421 108 C420 108,420 108,420 108 C420 108,419 108,419 109 C419 109,418 109,417 110 C417 111,416 112,415 113 C414 114,413 115,412 116 L410 119 C410 122,411 124,411 125 C412 127,412 129,412 130 C413 131,413 132,413 133 C413 133,414 134,414 134 C414 135,414 135,415 135 C415 135,415 135,416 135 C416 135,417 135,417 134 C418 134,419 133,420 131 L423 133 C421 135,419 137,418 138 C417 139,415 139,413 139 C412 139,411 139,410 138 C410 138,409 137,408 137 C408 136,407 134,407 133 C406 129,406 127,406 125 L405 125 C403 129,401 132,399 133 C398 135,397 136,396 137 C396 138,395 138,394 139 C393 139,392 139,391 139 C390 139,389 139,388 139 L390 131 L392 131 C392 132,393 133,393 133 C394 133,394 133,395 133 C395 133,395 132,396 131 C397 131,398 130,399 128 C400 127,402 124,404 121 L404 121 Z" fill="#212121"/><path d="M438 126 L436 132 L431 132 L432 126 L438 126 Z M431 154 C431 155,431 156,431 157 C431 157,431 158,431 158 C431 159,431 159,431 160 C431 160,432 160,432 160 C433 160,433 160,433 160 C434 160,434 160,434 159 C435 159,435 159,435 158 C436 158,436 157,437 157 L439 159 C438 160,437 161,436 161 C435 162,435 162,434 163 C433 163,433 164,432 164 C431 164,431 164,430 164 C429 164,429 164,428 164 C428 164,427 163,427 163 C426 162,426 162,426 161 C426 161,425 160,425 159 C425 159,426 158,426 157 C426 156,426 155,426 154 C427 153,427 152,427 151 C427 150,428 149,428 148 C428 147,428 146,428 145 C429 144,429 143,429 142 C429 142,429 142,429 141 C429 141,429 141,429 141 C429 140,429 140,428 140 C428 139,427 139,426 139 L426 137 L434 137 L435 137 L431 154 Z" fill="#212121"/><path d="M469 112 L469 107 L515 107 L515 112 L469 112 Z M469 127 L469 122 L515 122 L515 127 L469 127 Z" fill="#212121"/><path d="M562 139 C556 139,552 137,550 133 C547 129,546 122,546 114 C546 109,546 105,547 102 C548 98,549 96,551 93 C552 91,554 90,556 89 C558 88,560 87,562 87 C568 87,572 89,574 93 C577 98,578 104,578 112 C578 121,577 128,574 132 C571 137,567 139,562 139 L562 139 Z M553 112 C553 121,554 127,555 130 C557 134,559 136,562 136 C565 136,567 134,569 131 C570 127,571 122,571 114 C571 108,571 104,570 100 C569 97,568 94,567 92 C566 91,564 90,562 90 C560 90,559 91,558 92 C557 93,556 94,555 96 C554 98,554 100,553 103 C553 106,553 109,553 112 L553 112 Z" fill="#212121"/></svg></span><span class="f_Text"> </span></p>
<p class="p_Text"><span class="f_Text">The derivative of such a function with </span><span class="f_Text" style="font-style: italic;">respect to x</span><span class="f_Text"> </span><span class="f_Text" style="font-size: 7pt; font-style: italic; vertical-align: sub;">i</span><span class="f_Text"> will be zero. Consequently, during the training of the neural network, the error gradient through such a neuron will also not be passed to the preceding layers, paralyzing the training process.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:62px;height:46px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 248 184"><path d="M26 43 C23 47,21 49,19 51 C17 52,15 53,12 53 C10 53,8 52,6 50 C5 48,4 45,4 41 C4 37,5 33,6 29 C8 25,10 22,13 20 C16 17,19 16,23 16 C24 16,26 16,27 17 C28 17,29 17,31 18 L33 10 C33 9,33 8,33 7 C33 6,33 6,33 5 C33 4,33 4,33 3 C33 3,32 3,32 2 C31 2,30 2,29 2 L30 0 L39 0 L41 0 L32 41 C31 43,31 45,31 46 C31 47,31 48,31 48 C32 49,32 49,33 49 C34 49,35 49,35 48 C36 48,37 47,39 45 L41 47 C39 49,37 51,35 52 C34 53,32 53,30 53 C29 53,28 53,27 52 C26 51,25 49,25 48 C25 47,26 45,26 44 L26 43 Z M28 30 C28 28,29 26,29 25 C29 23,28 21,27 20 C27 20,25 19,23 19 C21 19,19 20,17 22 C15 24,13 27,12 31 C11 34,10 38,10 41 C10 44,11 46,11 47 C12 48,13 49,15 49 C16 49,16 49,17 49 C18 48,19 48,20 47 C21 46,21 45,22 44 C23 43,24 42,24 41 C25 40,26 39,26 37 C27 35,27 34,28 31 L28 30 Z" fill="#212121"/><path d="M49 41 C49 44,50 46,51 48 C53 49,55 50,58 50 C61 50,64 49,66 47 C67 45,68 43,68 39 C68 38,68 36,68 35 C67 34,66 33,65 32 C64 31,63 30,61 28 C59 27,57 26,56 24 C55 23,54 22,53 21 C53 19,52 17,52 16 C52 13,53 10,54 8 C56 6,58 4,60 3 C63 2,66 1,69 1 C71 1,73 1,75 2 C77 2,79 2,82 3 L80 12 L76 12 C76 10,76 9,75 7 C75 6,74 5,73 5 C72 4,70 4,69 4 C67 4,65 5,63 5 C62 6,61 7,60 9 C59 10,59 12,59 14 C59 15,59 17,60 18 C61 20,63 22,66 23 C68 25,70 27,71 28 C72 29,73 31,74 32 C75 34,75 36,75 37 C75 41,74 43,73 46 C71 48,69 50,67 51 C64 52,61 53,58 53 C56 53,53 53,51 53 C48 52,46 52,44 51 L46 41 L49 41 Z" fill="#212121"/><path d="M94 76 C90 76,87 75,85 72 C83 69,82 64,82 58 C82 55,82 53,83 50 C83 48,84 46,85 45 C86 43,88 42,89 41 C91 40,92 40,94 40 C96 40,98 41,100 41 C101 42,102 43,103 45 C104 46,105 48,105 50 C106 53,106 55,106 58 C106 64,105 68,103 71 C101 75,98 76,94 76 L94 76 Z M87 58 C87 60,88 63,88 65 C88 67,89 68,89 69 C90 71,90 72,91 72 C92 73,93 73,94 73 C95 73,96 73,97 72 C98 72,98 71,99 70 C99 69,100 67,100 65 C100 63,100 61,100 59 C100 56,100 54,100 52 C100 50,99 48,99 47 C98 46,98 45,97 44 C96 43,95 43,94 43 C93 43,92 43,91 44 C90 45,89 46,89 47 C88 48,88 50,88 52 C88 53,87 55,87 58 L87 58 Z" fill="#212121"/><path d="M30 151 C27 155,25 157,23 159 C21 160,19 161,16 161 C14 161,12 160,10 158 C9 156,8 153,8 149 C8 145,9 141,10 137 C12 133,14 130,17 128 C20 125,23 124,27 124 C28 124,30 124,31 125 C32 125,33 125,35 126 L37 118 C37 117,37 116,37 115 C37 114,37 114,37 113 C37 112,37 112,37 111 C37 111,36 111,36 110 C35 110,34 110,33 110 L34 108 L43 108 L45 108 L36 149 C35 151,35 153,35 154 C35 155,35 156,35 156 C36 157,36 157,37 157 C38 157,39 157,39 156 C40 156,41 155,43 153 L45 155 C43 157,41 159,39 160 C38 161,36 161,34 161 C33 161,32 161,31 160 C30 159,29 157,29 156 C29 155,30 153,30 152 L30 151 Z M32 138 C32 136,33 134,33 133 C33 131,32 129,31 128 C31 128,29 127,27 127 C25 127,23 128,21 130 C19 132,17 135,16 139 C15 142,14 146,14 149 C14 152,15 154,15 155 C16 156,17 157,19 157 C20 157,20 157,21 157 C22 156,23 156,24 155 C25 154,25 153,26 152 C27 151,28 150,28 149 C29 148,30 147,30 145 C31 143,31 142,32 139 L32 138 Z" fill="#212121"/><path d="M64 143 C64 141,64 140,63 138 C63 136,63 134,62 133 C62 131,61 130,61 129 C61 129,61 129,60 128 C60 128,60 128,59 128 C59 128,58 128,58 128 C57 129,57 129,56 130 C56 130,55 131,54 132 L52 130 C54 128,55 127,56 126 C58 125,60 124,61 124 C62 124,63 124,63 124 C64 125,65 125,65 125 C65 126,66 126,66 127 C67 127,67 128,67 129 C68 130,68 131,68 133 C68 134,69 136,69 137 L69 137 C72 134,73 131,74 130 C76 128,77 127,78 126 C78 125,79 125,80 125 C81 124,82 124,83 124 C84 124,85 124,86 125 L84 132 L82 132 C82 131,81 130,81 130 C80 130,80 130,80 130 C80 130,79 130,79 131 C79 131,78 131,77 132 C77 133,76 134,75 135 C74 136,73 137,72 138 L70 141 C70 144,71 146,71 147 C72 149,72 151,72 152 C73 153,73 154,73 155 C73 155,74 156,74 156 C74 157,74 157,75 157 C75 157,75 157,76 157 C76 157,77 157,77 156 C78 156,79 155,80 153 L83 155 C81 157,79 159,78 160 C77 161,75 161,73 161 C72 161,71 161,70 160 C70 160,69 159,68 159 C68 158,67 156,67 155 C66 151,66 149,66 147 L65 147 C63 151,61 154,59 155 C58 157,57 158,56 159 C56 160,55 160,54 161 C53 161,52 161,51 161 C50 161,49 161,48 161 L50 153 L52 153 C52 154,53 155,53 155 C54 155,54 155,55 155 C55 155,55 154,56 153 C57 153,58 152,59 150 C60 149,62 146,64 143 L64 143 Z" fill="#212121"/><path d="M98 148 L96 154 L91 154 L92 148 L98 148 Z M91 176 C91 177,91 178,91 179 C91 179,91 180,91 180 C91 181,91 181,91 182 C91 182,92 182,92 182 C93 182,93 182,93 182 C94 182,94 182,94 181 C95 181,95 181,95 180 C96 180,96 179,97 179 L99 181 C98 182,97 183,96 183 C95 184,95 184,94 185 C93 185,93 186,92 186 C91 186,91 186,90 186 C89 186,89 186,88 186 C88 186,87 185,87 185 C86 184,86 184,86 183 C86 183,85 182,85 181 C85 181,86 180,86 179 C86 178,86 177,86 176 C87 175,87 174,87 173 C87 172,88 171,88 170 C88 169,88 168,88 167 C89 166,89 165,89 164 C89 164,89 164,89 163 C89 163,89 163,89 163 C89 162,89 162,88 162 C88 161,87 161,86 161 L86 159 L94 159 L95 159 L91 176 Z" fill="#212121"/><rect x="0" y="86" width="108" height="5" fill="#212121"/><path d="M134 84 L134 79 L180 79 L180 84 L134 84 Z M134 99 L134 94 L180 94 L180 99 L134 99 Z" fill="#212121"/><path d="M227 111 C221 111,217 109,215 105 C212 101,211 94,211 86 C211 81,211 77,212 74 C213 70,214 68,216 65 C217 63,219 62,221 61 C223 60,225 59,227 59 C233 59,237 61,239 65 C242 70,243 76,243 84 C243 93,242 100,239 104 C236 109,232 111,227 111 L227 111 Z M218 84 C218 93,219 99,220 102 C222 106,224 108,227 108 C230 108,232 106,234 103 C235 99,236 94,236 86 C236 80,236 76,235 72 C234 69,233 66,232 64 C231 63,229 62,227 62 C225 62,224 63,223 64 C222 65,221 66,220 68 C219 70,219 72,218 75 C218 78,218 81,218 84 L218 84 Z" fill="#212121"/></svg></span></p>
<p class="p_Quote"><span class="f_Quote">Using 0 for the initialization of synaptic (weight) coefficients results in an untrainable neural network, which in most cases will generate 0 (depending on the activation function) regardless of the input data received. </span></p>
<p class="p_Text"><span class="f_Text">Using a constant other than zero as a weighting factor also has disadvantages. The input layer of the neural network is supplied with a set of initial data. All neurons of the subsequent layer work with this dataset in the same way. Within the framework of a single neuron, according to the laws of mathematics, the constant can be factored out in the formula for calculating the weighted sum. As a result, in the first stage, we get a scaling of the sum of the initial values. Changes in weights are possible during training. However, this only applies to the first layer of neurons receiving the initial data.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:292px;height:61px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 1168 244"><path d="M6 127 C6 130,7 132,8 134 C10 135,12 136,15 136 C18 136,21 135,23 133 C24 131,25 129,25 125 C25 124,25 122,25 121 C24 120,23 119,22 118 C21 117,20 116,18 114 C16 113,14 112,13 110 C12 109,11 108,10 107 C10 105,9 103,9 102 C9 99,10 96,11 94 C13 92,15 90,17 89 C20 88,23 87,26 87 C28 87,30 87,32 88 C34 88,36 88,39 89 L37 98 L33 98 C33 96,33 95,32 93 C32 92,31 91,30 91 C29 90,27 90,26 90 C24 90,22 91,20 91 C19 92,18 93,17 95 C16 96,16 98,16 100 C16 101,16 103,17 104 C18 106,20 108,23 109 C25 111,27 113,28 114 C29 115,30 117,31 118 C32 120,32 122,32 123 C32 127,31 129,30 132 C28 134,26 136,24 137 C21 138,18 139,15 139 C13 139,10 139,8 139 C5 138,3 138,1 137 L3 127 L6 127 Z" fill="#212121"/><path d="M57 141 C57 140,57 139,57 139 C57 138,57 138,57 137 C56 137,56 137,56 136 C55 136,55 136,54 136 C53 136,52 136,51 137 C50 137,50 138,49 138 C48 139,47 140,47 141 C46 142,46 143,45 144 C45 145,44 147,44 148 C44 149,44 150,44 151 C44 153,44 155,45 156 C46 157,47 157,48 157 C49 157,50 157,51 157 C51 157,52 156,53 156 C53 156,54 155,55 155 C55 154,56 153,57 152 C57 153,58 153,58 154 C58 154,59 154,59 155 C57 156,55 158,53 159 C51 160,49 160,47 160 C44 160,42 159,41 158 C39 156,38 154,38 151 C38 149,39 148,39 146 C39 145,40 143,41 142 C41 141,42 140,43 138 C44 137,45 136,46 136 C48 135,49 134,50 134 C52 133,53 133,55 133 C56 133,57 133,59 133 C60 133,61 134,62 134 L60 141 L57 141 Z" fill="#212121"/><path d="M76 160 C75 160,73 160,72 159 C71 159,70 158,69 158 C68 157,68 156,67 155 C67 154,66 152,66 151 C66 148,67 146,68 144 C69 142,70 140,71 138 C72 137,74 135,76 134 C78 134,80 133,82 133 C84 133,85 133,86 134 C88 134,89 135,90 136 C90 137,91 138,91 139 C92 140,92 141,92 143 C92 145,92 148,91 150 C90 152,89 154,87 155 C86 157,84 158,82 159 C80 160,78 160,76 160 L76 160 Z M72 151 C72 153,72 155,73 156 C74 157,75 157,77 157 C78 157,79 157,80 156 C81 156,81 155,82 155 C83 154,83 153,84 152 C85 151,85 150,85 149 C86 148,86 147,86 145 C87 144,87 143,87 142 C87 140,86 139,85 138 C85 137,83 136,82 136 C81 136,80 136,79 137 C78 137,77 138,77 138 C76 139,75 140,75 141 C74 142,73 143,73 144 C73 145,72 147,72 148 C72 149,72 150,72 151 L72 151 Z" fill="#212121"/><path d="M109 140 C111 138,112 136,114 135 C116 134,118 133,119 133 C121 133,123 134,124 135 C125 136,125 137,125 139 C125 139,125 140,125 141 C125 142,124 143,124 144 C124 145,124 146,123 147 C123 148,123 150,122 150 C122 151,122 152,122 153 C122 154,122 154,122 154 C122 155,122 155,122 156 C122 156,123 156,123 156 C124 156,124 156,124 156 C125 156,125 156,125 155 C126 155,126 155,126 154 C127 154,127 153,128 153 L130 154 C129 156,128 156,127 157 C127 158,126 158,125 159 C125 159,124 160,123 160 C122 160,122 160,121 160 C120 160,120 160,119 160 C119 160,118 159,118 159 C117 158,117 158,117 157 C117 157,117 156,117 155 C117 155,117 154,117 153 C117 152,117 151,117 150 C118 149,118 148,118 147 C119 146,119 145,119 144 C119 143,119 143,120 142 C120 141,120 141,120 140 C120 139,120 138,119 138 C119 137,118 137,117 137 C117 137,116 137,115 137 C115 138,114 138,113 139 C113 139,112 140,112 140 C111 141,110 142,110 142 C109 143,109 144,109 145 C108 146,108 146,108 147 L105 160 L100 160 L104 143 C104 142,104 142,104 141 C104 140,104 139,104 139 C104 138,104 137,103 137 C102 137,102 137,102 137 C101 137,101 138,101 138 C100 138,100 139,99 139 C99 139,99 140,98 140 L96 139 C97 138,98 137,99 136 C99 135,100 135,101 134 C101 134,102 134,103 133 C103 133,104 133,105 133 C106 133,107 134,108 134 C109 135,109 136,109 138 C109 138,109 139,109 139 C109 139,109 140,109 140 L109 140 Z" fill="#212121"/><path d="M152 141 C152 139,152 138,151 137 C150 137,149 136,147 136 C146 136,145 136,145 136 C144 137,144 137,143 137 C143 137,142 138,142 138 C142 139,142 139,142 140 C142 140,142 141,142 141 C142 141,142 142,143 142 C143 143,143 143,144 143 C145 144,146 144,147 145 C148 145,149 146,150 147 C150 147,151 148,151 148 C152 149,152 149,152 150 C153 151,153 151,153 152 C153 153,152 155,152 156 C151 157,151 157,150 158 C149 159,148 159,146 160 C145 160,144 160,142 160 C141 160,139 160,137 160 C136 160,134 159,132 159 L134 152 L136 152 C136 154,137 155,138 156 C139 157,140 157,142 157 C144 157,145 157,146 156 C147 155,148 154,148 153 C148 153,148 152,148 152 C148 151,147 151,147 151 C147 150,146 150,145 149 C145 149,144 149,143 148 C142 147,141 147,140 146 C139 146,139 145,138 145 C138 144,138 143,137 143 C137 142,137 141,137 141 C137 140,137 139,138 138 C138 137,139 136,140 135 C141 135,142 134,143 134 C144 133,145 133,147 133 C149 133,150 133,152 133 C153 134,155 134,156 134 L155 141 L152 141 Z" fill="#212121"/><path d="M179 154 C178 155,177 156,176 157 C175 158,175 158,174 159 C173 159,172 160,171 160 C171 160,170 160,169 160 C167 160,165 160,164 159 C163 158,163 156,163 154 C163 154,163 153,163 152 C163 151,163 151,163 150 L166 136 L162 136 L162 134 C163 134,164 134,165 134 C166 134,166 133,166 133 C167 133,167 132,167 132 C167 132,168 131,168 131 C168 130,168 129,169 129 C169 128,169 127,170 126 L174 126 L172 133 L181 133 L181 136 L172 136 L169 147 C169 148,169 148,168 149 C168 150,168 150,168 150 C168 151,168 151,168 152 C168 152,168 152,168 153 C168 154,168 155,169 155 C169 156,170 156,171 156 C172 156,173 156,174 155 C175 155,176 154,177 152 L179 154 Z" fill="#212121"/><path d="M211 112 L211 107 L257 107 L257 112 L211 112 Z M211 127 L211 122 L257 122 L257 127 L211 127 Z" fill="#212121"/><path d="M300 50 L344 109 L344 114 L297 177 L360 177 C363 177,365 176,367 174 C368 172,369 168,370 161 L376 161 L374 188 L284 188 L284 184 L335 115 L285 49 L285 45 L374 45 L374 67 L369 67 C369 62,367 58,366 55 C364 52,361 50,358 50 L300 50 Z" fill="#212121"/><path d="M328 7 C330 5,331 3,333 2 C335 1,337 0,338 0 C340 0,342 1,343 2 C344 3,344 4,344 6 C344 6,344 7,344 8 C344 9,343 10,343 11 C343 12,343 13,342 14 C342 15,342 17,341 17 C341 18,341 19,341 20 C341 21,341 21,341 21 C341 22,341 22,341 23 C341 23,342 23,342 23 C343 23,343 23,343 23 C344 23,344 23,344 22 C345 22,345 22,345 21 C346 21,346 20,347 20 L349 21 C348 23,347 23,346 24 C346 25,345 25,344 26 C344 26,343 27,342 27 C341 27,341 27,340 27 C339 27,339 27,338 27 C338 27,337 26,337 26 C336 25,336 25,336 24 C336 24,336 23,336 22 C336 22,336 21,336 20 C336 19,336 18,336 17 C337 16,337 15,337 14 C338 13,338 12,338 11 C338 10,338 10,339 9 C339 8,339 8,339 7 C339 6,339 5,338 5 C338 4,337 4,336 4 C336 4,335 4,334 4 C334 5,333 5,332 6 C332 6,331 7,331 7 C330 8,329 9,329 9 C328 10,328 11,328 12 C327 13,327 13,327 14 L324 27 L319 27 L323 10 C323 9,323 9,323 8 C323 7,323 6,323 6 C323 5,323 4,322 4 C321 4,321 4,321 4 C320 4,320 5,320 5 C319 5,319 6,318 6 C318 6,318 7,317 7 L315 6 C316 5,317 4,318 3 C318 2,319 2,320 1 C320 1,321 1,322 0 C322 0,323 0,324 0 C325 0,326 1,327 1 C328 2,328 3,328 5 C328 5,328 6,328 6 C328 6,328 7,328 7 L328 7 Z" fill="#212121"/><path d="M303 195 L301 201 L296 201 L297 195 L303 195 Z M296 223 C296 224,296 225,296 226 C296 226,296 227,296 227 C296 228,296 228,296 229 C296 229,297 229,297 229 C298 229,298 229,298 229 C299 229,299 229,299 228 C300 228,300 228,300 227 C301 227,301 226,302 226 L304 228 C303 229,302 230,301 230 C300 231,300 231,299 232 C298 232,298 233,297 233 C296 233,296 233,295 233 C294 233,294 233,293 233 C293 233,292 232,292 232 C291 231,291 231,291 230 C291 230,290 229,290 228 C290 228,291 227,291 226 C291 225,291 224,291 223 C292 222,292 221,292 220 C292 219,293 218,293 217 C293 216,293 215,293 214 C294 213,294 212,294 211 C294 211,294 211,294 210 C294 210,294 210,294 210 C294 209,294 209,293 209 C293 208,292 208,291 208 L291 206 L299 206 L300 206 L296 223 Z" fill="#212121"/><path d="M312 213 L312 209 L342 209 L342 213 L312 213 Z M312 225 L312 221 L342 221 L342 225 L312 225 Z" fill="#212121"/><path d="M367 226 C367 227,367 227,367 228 C367 228,367 229,367 229 C367 229,367 229,367 230 C367 230,368 230,368 230 C368 230,368 230,369 231 C369 231,369 231,370 231 C370 231,371 231,372 231 C373 231,374 231,375 231 L375 233 L353 233 L353 231 C354 231,355 231,356 231 C357 231,357 231,358 231 C358 231,359 231,359 231 C360 230,360 230,360 230 C360 230,361 230,361 230 C361 230,361 229,361 229 C361 229,361 228,361 228 C361 228,361 227,361 226 L361 207 C361 207,361 206,361 206 C361 206,360 205,360 205 C359 205,359 206,357 206 C356 207,355 208,353 209 C353 208,353 208,353 208 C352 207,352 207,352 206 C354 205,356 204,359 203 C361 202,363 200,365 199 L367 199 C367 200,367 201,367 202 C367 203,367 203,367 204 C367 205,367 205,367 206 L367 226 Z" fill="#212121"/><path d="M423 112 C423 110,422 109,422 108 C422 107,421 106,421 106 C420 105,420 105,418 105 C416 105,414 106,412 108 C410 110,408 113,407 117 C406 121,405 124,405 127 C405 130,406 132,407 133 C408 134,409 135,411 135 C413 135,415 135,417 134 C418 133,420 132,422 130 L424 132 C422 135,420 136,417 137 C415 139,412 139,410 139 C406 139,403 138,402 136 C400 134,399 131,399 126 C399 124,399 121,400 118 C401 115,403 112,405 109 C406 107,409 105,411 104 C414 103,416 102,419 102 C423 102,425 102,428 103 L426 112 L423 112 Z" fill="#212121"/><path d="M446 139 C442 139,439 138,437 136 C435 134,434 131,434 126 C434 125,434 122,435 120 C436 116,437 113,439 110 C441 108,443 106,445 104 C448 103,451 102,454 102 C458 102,461 103,463 105 C465 108,466 111,466 115 C466 118,465 121,464 124 C463 127,462 130,460 132 C459 135,457 136,454 137 C452 139,449 139,446 139 L446 139 Z M440 128 C440 131,441 133,442 134 C443 135,445 136,447 136 C450 136,452 135,454 133 C455 131,457 128,458 124 C459 120,460 116,460 113 C460 111,459 109,458 107 C457 106,455 105,453 105 C451 105,448 106,447 108 C445 111,443 114,442 118 C441 122,440 125,440 128 L440 128 Z" fill="#212121"/><path d="M480 114 C481 112,481 110,481 109 C481 108,481 107,480 107 C480 106,480 106,479 106 C478 106,477 106,476 107 C476 108,474 109,473 110 L471 108 C473 106,475 104,477 103 C478 103,480 102,482 102 C483 102,484 103,485 104 C486 105,487 106,487 107 C487 109,486 110,486 112 L486 112 C488 108,491 106,493 104 C495 103,497 102,500 102 C502 102,504 103,505 104 C506 105,507 107,507 110 C507 111,506 114,506 116 L503 126 C503 129,502 131,502 132 C502 133,502 134,503 134 C503 135,504 135,504 135 C505 135,506 135,507 134 C507 134,509 133,510 131 L512 133 C510 135,508 137,507 138 C505 139,503 139,501 139 C500 139,499 138,498 137 C497 136,496 135,496 133 C496 131,497 129,497 126 L499 119 C500 117,500 115,500 114 C500 113,500 112,500 111 C500 109,500 108,500 107 C499 106,498 106,497 106 C496 106,495 107,493 107 C492 108,491 109,490 111 C488 113,487 114,486 116 C486 118,485 120,484 122 L481 138 L475 138 L480 114 Z" fill="#212121"/><path d="M541 112 C540 110,540 108,538 107 C537 106,536 105,533 105 C531 105,530 106,528 107 C527 108,527 109,527 111 C527 112,527 112,527 113 C527 114,528 115,529 115 C530 116,531 117,533 118 C535 119,536 120,537 121 C538 122,539 123,539 123 C540 124,540 125,540 126 C540 127,541 128,541 129 C541 131,540 133,539 134 C538 136,536 137,534 138 C532 139,530 139,527 139 C525 139,523 139,521 139 C519 138,517 138,515 137 L517 129 L519 129 C520 131,520 133,521 134 C523 136,525 136,527 136 C530 136,531 136,533 135 C534 134,535 132,535 130 C535 129,534 128,534 127 C534 126,533 126,532 125 C531 124,530 123,528 122 C526 121,525 120,524 119 C523 118,522 117,522 116 C521 115,521 113,521 112 C521 110,521 108,522 107 C523 105,525 104,527 103 C529 103,531 102,533 102 C536 102,538 102,540 103 C542 103,543 103,545 104 L544 112 L541 112 Z" fill="#212121"/><path d="M573 131 C571 134,569 136,567 137 C565 138,563 139,561 139 C556 139,554 137,554 131 C554 130,554 128,554 126 L558 107 L552 107 L553 105 C554 105,555 105,556 105 C557 105,557 104,558 104 C558 104,559 103,559 102 C560 102,560 101,561 100 C561 99,562 97,562 94 L567 94 L565 103 L576 103 L576 107 L565 107 L561 121 C561 124,560 126,560 128 C560 129,560 130,560 130 C560 134,561 135,564 135 C565 135,566 135,567 134 C568 133,570 131,571 129 L573 131 Z" fill="#212121"/><path d="M626 122 L624 126 L614 119 L616 131 L611 131 L612 119 L602 126 L600 122 L610 117 L600 112 L602 108 L612 115 L611 103 L616 103 L614 115 L624 108 L626 112 L616 117 L626 122 Z" fill="#212121"/><path d="M665 121 C665 119,665 118,664 116 C664 114,664 112,663 111 C663 109,662 108,662 107 C662 107,662 107,661 106 C661 106,661 106,660 106 C660 106,659 106,659 106 C658 107,658 107,657 108 C657 108,656 109,655 110 L653 108 C655 106,656 105,657 104 C659 103,661 102,662 102 C663 102,664 102,664 102 C665 103,666 103,666 103 C666 104,667 104,667 105 C668 105,668 106,668 107 C669 108,669 109,669 111 C669 112,670 114,670 115 L670 115 C673 112,674 109,675 108 C677 106,678 105,679 104 C679 103,680 103,681 103 C682 102,683 102,684 102 C685 102,686 102,687 103 L685 110 L683 110 C683 109,682 108,682 108 C681 108,681 108,681 108 C681 108,680 108,680 109 C680 109,679 109,678 110 C678 111,677 112,676 113 C675 114,674 115,673 116 L671 119 C671 122,672 124,672 125 C673 127,673 129,673 130 C674 131,674 132,674 133 C674 133,675 134,675 134 C675 135,675 135,676 135 C676 135,676 135,677 135 C677 135,678 135,678 134 C679 134,680 133,681 131 L684 133 C682 135,680 137,679 138 C678 139,676 139,674 139 C673 139,672 139,671 138 C671 138,670 137,669 137 C669 136,668 134,668 133 C667 129,667 127,667 125 L666 125 C664 129,662 132,660 133 C659 135,658 136,657 137 C657 138,656 138,655 139 C654 139,653 139,652 139 C651 139,650 139,649 139 L651 131 L653 131 C653 132,654 133,654 133 C655 133,655 133,656 133 C656 133,656 132,657 131 C658 131,659 130,660 128 C661 127,663 124,665 121 L665 121 Z" fill="#212121"/><path d="M699 126 L697 132 L692 132 L693 126 L699 126 Z M692 154 C692 155,692 156,692 157 C692 157,692 158,692 158 C692 159,692 159,692 160 C692 160,693 160,693 160 C694 160,694 160,694 160 C695 160,695 160,695 159 C696 159,696 159,696 158 C697 158,697 157,698 157 L700 159 C699 160,698 161,697 161 C696 162,696 162,695 163 C694 163,694 164,693 164 C692 164,692 164,691 164 C690 164,690 164,689 164 C689 164,688 163,688 163 C687 162,687 162,687 161 C687 161,686 160,686 159 C686 159,687 158,687 157 C687 156,687 155,687 154 C688 153,688 152,688 151 C688 150,689 149,689 148 C689 147,689 146,689 145 C690 144,690 143,690 142 C690 142,690 142,690 141 C690 141,690 141,690 141 C690 140,690 140,689 140 C689 139,688 139,687 139 L687 137 L695 137 L696 137 L692 154 Z" fill="#212121"/><path d="M730 112 L730 107 L776 107 L776 112 L730 112 Z M730 127 L730 122 L776 122 L776 127 L730 127 Z" fill="#212121"/><path d="M830 112 C830 110,829 109,829 108 C829 107,828 106,828 106 C827 105,827 105,825 105 C823 105,821 106,819 108 C817 110,815 113,814 117 C813 121,812 124,812 127 C812 130,813 132,814 133 C815 134,816 135,818 135 C820 135,822 135,824 134 C825 133,827 132,829 130 L831 132 C829 135,827 136,824 137 C822 139,819 139,817 139 C813 139,810 138,809 136 C807 134,806 131,806 126 C806 124,806 121,807 118 C808 115,810 112,812 109 C813 107,816 105,818 104 C821 103,823 102,826 102 C830 102,832 102,835 103 L833 112 L830 112 Z" fill="#212121"/><path d="M853 139 C849 139,846 138,844 136 C842 134,841 131,841 126 C841 125,841 122,842 120 C843 116,844 113,846 110 C848 108,850 106,852 104 C855 103,858 102,861 102 C865 102,868 103,870 105 C872 108,873 111,873 115 C873 118,872 121,871 124 C870 127,869 130,867 132 C866 135,864 136,861 137 C859 139,856 139,853 139 L853 139 Z M847 128 C847 131,848 133,849 134 C850 135,852 136,854 136 C857 136,859 135,861 133 C862 131,864 128,865 124 C866 120,867 116,867 113 C867 111,866 109,865 107 C864 106,862 105,860 105 C858 105,855 106,854 108 C852 111,850 114,849 118 C848 122,847 125,847 128 L847 128 Z" fill="#212121"/><path d="M887 114 C888 112,888 110,888 109 C888 108,888 107,887 107 C887 106,887 106,886 106 C885 106,884 106,883 107 C883 108,881 109,880 110 L878 108 C880 106,882 104,884 103 C885 103,887 102,889 102 C890 102,891 103,892 104 C893 105,894 106,894 107 C894 109,893 110,893 112 L893 112 C895 108,898 106,900 104 C902 103,904 102,907 102 C909 102,911 103,912 104 C913 105,914 107,914 110 C914 111,913 114,913 116 L910 126 C910 129,909 131,909 132 C909 133,909 134,910 134 C910 135,911 135,911 135 C912 135,913 135,914 134 C914 134,916 133,917 131 L919 133 C917 135,915 137,914 138 C912 139,910 139,908 139 C907 139,906 138,905 137 C904 136,903 135,903 133 C903 131,904 129,904 126 L906 119 C907 117,907 115,907 114 C907 113,907 112,907 111 C907 109,907 108,907 107 C906 106,905 106,904 106 C903 106,902 107,900 107 C899 108,898 109,897 111 C895 113,894 114,893 116 C893 118,892 120,891 122 L888 138 L882 138 L887 114 Z" fill="#212121"/><path d="M948 112 C947 110,947 108,945 107 C944 106,943 105,940 105 C938 105,937 106,935 107 C934 108,934 109,934 111 C934 112,934 112,934 113 C934 114,935 115,936 115 C937 116,938 117,940 118 C942 119,943 120,944 121 C945 122,946 123,946 123 C947 124,947 125,947 126 C947 127,948 128,948 129 C948 131,947 133,946 134 C945 136,943 137,941 138 C939 139,937 139,934 139 C932 139,930 139,928 139 C926 138,924 138,922 137 L924 129 L926 129 C927 131,927 133,928 134 C930 136,932 136,934 136 C937 136,938 136,940 135 C941 134,942 132,942 130 C942 129,941 128,941 127 C941 126,940 126,939 125 C938 124,937 123,935 122 C933 121,932 120,931 119 C930 118,929 117,929 116 C928 115,928 113,928 112 C928 110,928 108,929 107 C930 105,932 104,934 103 C936 103,938 102,940 102 C943 102,945 102,947 103 C949 103,950 103,952 104 L951 112 L948 112 Z" fill="#212121"/><path d="M980 131 C978 134,976 136,974 137 C972 138,970 139,968 139 C963 139,961 137,961 131 C961 130,961 128,961 126 L965 107 L959 107 L960 105 C961 105,962 105,963 105 C964 105,964 104,965 104 C965 104,966 103,966 102 C967 102,967 101,968 100 C968 99,969 97,969 94 L974 94 L972 103 L983 103 L983 107 L972 107 L968 121 C968 124,967 126,967 128 C967 129,967 130,967 130 C967 134,968 135,971 135 C972 135,973 135,974 134 C975 133,977 131,978 129 L980 131 Z" fill="#212121"/><path d="M1031 78 L1055 113 L1055 116 L1028 152 L1061 152 C1064 152,1066 151,1067 149 C1069 148,1070 145,1070 141 L1074 141 L1073 159 L1018 159 L1018 157 L1048 117 L1020 76 L1020 74 L1074 74 L1074 91 L1070 91 C1069 87,1068 84,1066 82 C1064 79,1061 78,1057 78 L1031 78 Z" fill="#212121"/><path d="M1040 36 C1042 34,1043 32,1045 31 C1047 30,1049 29,1050 29 C1052 29,1054 30,1055 31 C1056 32,1056 33,1056 35 C1056 35,1056 36,1056 37 C1056 38,1055 39,1055 40 C1055 41,1055 42,1054 43 C1054 44,1054 46,1053 46 C1053 47,1053 48,1053 49 C1053 50,1053 50,1053 50 C1053 51,1053 51,1053 52 C1053 52,1054 52,1054 52 C1055 52,1055 52,1055 52 C1056 52,1056 52,1056 51 C1057 51,1057 51,1057 50 C1058 50,1058 49,1059 49 L1061 50 C1060 52,1059 52,1058 53 C1058 54,1057 54,1056 55 C1056 55,1055 56,1054 56 C1053 56,1053 56,1052 56 C1051 56,1051 56,1050 56 C1050 56,1049 55,1049 55 C1048 54,1048 54,1048 53 C1048 53,1048 52,1048 51 C1048 51,1048 50,1048 49 C1048 48,1048 47,1048 46 C1049 45,1049 44,1049 43 C1050 42,1050 41,1050 40 C1050 39,1050 39,1051 38 C1051 37,1051 37,1051 36 C1051 35,1051 34,1050 34 C1050 33,1049 33,1048 33 C1048 33,1047 33,1046 33 C1046 34,1045 34,1044 35 C1044 35,1043 36,1043 36 C1042 37,1041 38,1041 38 C1040 39,1040 40,1040 41 C1039 42,1039 42,1039 43 L1036 56 L1031 56 L1035 39 C1035 38,1035 38,1035 37 C1035 36,1035 35,1035 35 C1035 34,1035 33,1034 33 C1033 33,1033 33,1033 33 C1032 33,1032 34,1032 34 C1031 34,1031 35,1030 35 C1030 35,1030 36,1029 36 L1027 35 C1028 34,1029 33,1030 32 C1030 31,1031 31,1032 30 C1032 30,1033 30,1034 29 C1034 29,1035 29,1036 29 C1037 29,1038 30,1039 30 C1040 31,1040 32,1040 34 C1040 34,1040 35,1040 35 C1040 35,1040 36,1040 36 L1040 36 Z" fill="#212121"/><path d="M1015 166 L1013 172 L1008 172 L1009 166 L1015 166 Z M1008 194 C1008 195,1008 196,1008 197 C1008 197,1008 198,1008 198 C1008 199,1008 199,1008 200 C1008 200,1009 200,1009 200 C1010 200,1010 200,1010 200 C1011 200,1011 200,1011 199 C1012 199,1012 199,1012 198 C1013 198,1013 197,1014 197 L1016 199 C1015 200,1014 201,1013 201 C1012 202,1012 202,1011 203 C1010 203,1010 204,1009 204 C1008 204,1008 204,1007 204 C1006 204,1006 204,1005 204 C1005 204,1004 203,1004 203 C1003 202,1003 202,1003 201 C1003 201,1002 200,1002 199 C1002 199,1003 198,1003 197 C1003 196,1003 195,1003 194 C1004 193,1004 192,1004 191 C1004 190,1005 189,1005 188 C1005 187,1005 186,1005 185 C1006 184,1006 183,1006 182 C1006 182,1006 182,1006 181 C1006 181,1006 181,1006 181 C1006 180,1006 180,1005 180 C1005 179,1004 179,1003 179 L1003 177 L1011 177 L1012 177 L1008 194 Z" fill="#212121"/><path d="M1024 184 L1024 180 L1054 180 L1054 184 L1024 184 Z M1024 196 L1024 192 L1054 192 L1054 196 L1024 196 Z" fill="#212121"/><path d="M1079 197 C1079 198,1079 198,1079 199 C1079 199,1079 200,1079 200 C1079 200,1079 200,1079 201 C1079 201,1080 201,1080 201 C1080 201,1080 201,1081 202 C1081 202,1081 202,1082 202 C1082 202,1083 202,1084 202 C1085 202,1086 202,1087 202 L1087 204 L1065 204 L1065 202 C1066 202,1067 202,1068 202 C1069 202,1069 202,1070 202 C1070 202,1071 202,1071 202 C1072 201,1072 201,1072 201 C1072 201,1073 201,1073 201 C1073 201,1073 200,1073 200 C1073 200,1073 199,1073 199 C1073 199,1073 198,1073 197 L1073 178 C1073 178,1073 177,1073 177 C1073 177,1072 176,1072 176 C1071 176,1071 177,1069 177 C1068 178,1067 179,1065 180 C1065 179,1065 179,1065 179 C1064 178,1064 178,1064 177 C1066 176,1068 175,1071 174 C1073 173,1075 171,1077 170 L1079 170 C1079 171,1079 172,1079 173 C1079 174,1079 174,1079 175 C1079 176,1079 176,1079 177 L1079 197 Z" fill="#212121"/><path d="M1121 121 C1121 119,1121 118,1120 116 C1120 114,1120 112,1119 111 C1119 109,1118 108,1118 107 C1118 107,1118 107,1117 106 C1117 106,1117 106,1116 106 C1116 106,1115 106,1115 106 C1114 107,1114 107,1113 108 C1113 108,1112 109,1111 110 L1109 108 C1111 106,1112 105,1113 104 C1115 103,1117 102,1118 102 C1119 102,1120 102,1120 102 C1121 103,1122 103,1122 103 C1122 104,1123 104,1123 105 C1124 105,1124 106,1124 107 C1125 108,1125 109,1125 111 C1125 112,1126 114,1126 115 L1126 115 C1129 112,1130 109,1131 108 C1133 106,1134 105,1135 104 C1135 103,1136 103,1137 103 C1138 102,1139 102,1140 102 C1141 102,1142 102,1143 103 L1141 110 L1139 110 C1139 109,1138 108,1138 108 C1137 108,1137 108,1137 108 C1137 108,1136 108,1136 109 C1136 109,1135 109,1134 110 C1134 111,1133 112,1132 113 C1131 114,1130 115,1129 116 L1127 119 C1127 122,1128 124,1128 125 C1129 127,1129 129,1129 130 C1130 131,1130 132,1130 133 C1130 133,1131 134,1131 134 C1131 135,1131 135,1132 135 C1132 135,1132 135,1133 135 C1133 135,1134 135,1134 134 C1135 134,1136 133,1137 131 L1140 133 C1138 135,1136 137,1135 138 C1134 139,1132 139,1130 139 C1129 139,1128 139,1127 138 C1127 138,1126 137,1125 137 C1125 136,1124 134,1124 133 C1123 129,1123 127,1123 125 L1122 125 C1120 129,1118 132,1116 133 C1115 135,1114 136,1113 137 C1113 138,1112 138,1111 139 C1110 139,1109 139,1108 139 C1107 139,1106 139,1105 139 L1107 131 L1109 131 C1109 132,1110 133,1110 133 C1111 133,1111 133,1112 133 C1112 133,1112 132,1113 131 C1114 131,1115 130,1116 128 C1117 127,1119 124,1121 121 L1121 121 Z" fill="#212121"/><path d="M1155 126 L1153 132 L1148 132 L1149 126 L1155 126 Z M1148 154 C1148 155,1148 156,1148 157 C1148 157,1148 158,1148 158 C1148 159,1148 159,1148 160 C1148 160,1149 160,1149 160 C1150 160,1150 160,1150 160 C1151 160,1151 160,1151 159 C1152 159,1152 159,1152 158 C1153 158,1153 157,1154 157 L1156 159 C1155 160,1154 161,1153 161 C1152 162,1152 162,1151 163 C1150 163,1150 164,1149 164 C1148 164,1148 164,1147 164 C1146 164,1146 164,1145 164 C1145 164,1144 163,1144 163 C1143 162,1143 162,1143 161 C1143 161,1142 160,1142 159 C1142 159,1143 158,1143 157 C1143 156,1143 155,1143 154 C1144 153,1144 152,1144 151 C1144 150,1145 149,1145 148 C1145 147,1145 146,1145 145 C1146 144,1146 143,1146 142 C1146 142,1146 142,1146 141 C1146 141,1146 141,1146 141 C1146 140,1146 140,1145 140 C1145 139,1144 139,1143 139 L1143 137 L1151 137 L1152 137 L1148 154 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">If you look at the neural layer as a whole. Then all neurons in the same layer receive the same dataset. By using the same coefficient, all neurons generate the same signal. As a consequence, all neurons of one layer work synchronously as one neuron. This, in turn, leads to the same value being present at all inputs of all neurons of the subsequent layer. This happens from layer to layer throughout the neural network.</span></p>
<p class="p_Text"><span class="f_Text">The applied learning algorithms do not allow the isolation of an individual neuron among a large number of identical values. Therefore, all weights will be changed synchronously during the training process. Each layer, except for the first one after the input, will receive its weights, uniform for the entire layer. This results in the linear scaling of the results obtained on the same neuron.</span></p>
<p class="p_Quote"><span class="f_Quote">Initializing the synaptic coefficients with a single number other than zero causes the neural network to degenerate down to one neuron.</span></p>
<p class="p_H3"><a name="random" class="hmanchor"></a><span class="f_H3">Initializing weights with random values</span></p>
<p class="p_Text"><span class="f_Text">Since we cannot initialize a neural network with a single number, let’s try initializing with random values. For maximum efficiency, let's not forget about what was mentioned above. We need to make sure that no two synaptic coefficients are the same. This will be facilitated by a continuous uniform distribution.</span></p>
<p class="p_Text"><span class="f_Text">As practice has shown, such an approach yields results. Unfortunately, this is not always the case. Due to the random selection of weights, it is sometimes necessary to initialize the neural network several times before the desired result is achieved. The range of variation in the weights has a significant impact. If the gap between the minimum and maximum is large enough, some neurons will be isolated and others completely ignored.</span></p>
<p class="p_Text"><span class="f_Text">Moreover, in deep neural networks, there is a risk of the so-called &quot;gradient explosion&quot; and &quot;gradient vanishing&quot;.</span></p>
<p class="p_Text"><span class="f_Text">The gradient explosion manifests itself when using weights greater than one. In this case, when the initial data is multiplied by factors greater than one, the weighted sum increases continuously and exponentially with each layer. At the same time, generating a large number at the output often leads to a large error.</span></p>
<p class="p_Text"><span class="f_Text">During the training process, we will use an error gradient to adjust the weights. In order to pass the error gradient from the output layer to each neuron of our network, we need to multiply the obtained error by the weights. As a result, the error gradient, just like the weighted sum, will grow exponentially as it progresses through the layers of the neural network.</span></p>
<p class="p_Text"><span class="f_Text">As a consequence, at some point, we will get a number that exceeds our technical capabilities for recording values, and we won't be able to further train and use the network.</span></p>
<p class="p_Text"><span class="f_Text">The opposite situation occurs if we choose weight values close to zero. Constantly multiplying the initial data by weights less than one reduces the weighted sum of weight values. This process progresses exponentially with the increase in the number of layers of the neural network.</span></p>
<p class="p_Text"><span class="f_Text">As a consequence, during the training process, we may encounter a situation where the gradient of a small error, when passing through layers, becomes smaller than the technically feasible precision. For our neurons, the error gradient will become zero, and they will not learn.</span></p>
<p class="p_Text"><span class="f_Text">At the time of writing the book, the common practice is to initialize neurons using the Xavier method, proposed in 2010. Xavier Glorot and Yoshua Bengio proposed initializing the neural network with random numbers from a continuous normal distribution centered at point 0 and with a variance (&#948;</span><span class="f_Text" style="font-size: 7pt; vertical-align: super;">2)</span><span class="f_Text"> equal to 1/n</span><span class="f_Parameters">.</span></p>
<p class="p_Text"><span class="f_Parameters">This approach enables the generating of synaptic coefficients such that the average of the neuron activations will be zero, and their variance will be the same for all layers of the neural network. Xavier initialization is most relevant when using hyperbolic tangent (</span><span class="f_Parameters" style="font-style: italic;">tanh</span><span class="f_Parameters">) as an activation function.</span></p>
<p class="p_Text"><span class="f_Text">The theoretical justification for this approach was given in the article &quot;<a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi" target="_blank" rel="external" class="weblink">Understanding the difficulty of training deep feedforward neural networks</a>&quot;.</span></p>
<p class="p_Text"><span class="f_Text">Xavier initialization gives good results when using sigmoid activation functions. But when ReLU is used as an activation function, it is not as efficient. This is due to the characteristics of the ReLU itself.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:183px;height:19px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 732 76"><path d="M30 30 C31 31,32 32,33 33 C34 34,34 36,35 38 L36 43 C36 44,36 45,37 46 C37 47,37 48,38 49 C38 50,39 50,39 51 C40 51,41 51,42 51 L42 53 L31 53 C31 51,30 48,29 45 L28 37 C28 36,27 35,27 34 C26 33,26 32,25 31 C25 31,24 31,24 30 C23 30,22 30,21 30 L17 30 L14 42 C14 43,14 44,14 44 C14 45,14 46,14 46 C14 47,14 47,14 48 C14 49,14 49,14 50 C14 50,15 50,15 51 C16 51,16 51,18 51 L17 53 L2 53 L2 51 C3 51,4 51,5 50 C5 50,5 50,6 49 C6 48,6 48,7 47 C7 46,7 44,8 42 L14 14 C15 12,15 10,15 8 C15 7,15 6,14 6 C14 5,13 5,11 5 L12 3 L28 3 C31 3,33 3,34 4 C36 4,37 4,39 5 C40 6,41 6,41 7 C42 8,43 9,43 10 C44 12,44 13,44 14 C44 18,43 21,40 23 C38 26,35 28,30 29 L30 30 Z M22 27 C25 27,28 27,30 26 C32 25,34 23,35 21 C36 19,37 17,37 14 C37 13,37 12,37 11 C36 11,36 10,36 9 C36 9,35 8,35 8 C34 8,34 7,33 7 C32 7,31 6,30 6 C29 6,28 6,26 6 C25 6,24 6,22 6 L18 27 L22 27 Z" fill="#212121"/><path d="M79 46 C76 49,74 51,71 52 C69 53,66 54,63 54 C59 54,57 53,55 51 C53 49,52 46,52 42 C52 39,52 36,53 33 C54 30,56 27,58 25 C59 22,62 20,64 19 C67 18,70 17,73 17 C76 17,78 18,80 19 C81 20,82 22,82 25 C82 29,80 32,76 34 C72 36,66 37,59 37 C58 39,58 40,58 42 C58 45,59 47,60 48 C61 49,62 50,65 50 C67 50,69 50,71 49 C72 48,74 46,76 44 L79 46 Z M59 34 C63 34,66 34,68 33 C71 33,72 32,74 30 C75 29,76 27,76 25 C76 23,75 22,75 21 C74 21,73 20,72 20 C69 20,67 21,64 24 C62 26,60 30,59 34 L59 34 Z" fill="#212121"/><path d="M87 51 C88 51,89 51,90 51 C90 50,90 50,91 49 C91 49,91 48,92 47 C92 46,92 45,93 42 L99 14 C100 12,100 10,100 8 C100 7,100 6,99 6 C98 5,98 5,96 5 L97 3 L111 3 L111 5 C110 5,109 6,109 6 C109 6,108 6,108 7 C108 7,107 8,107 9 C107 10,106 12,106 14 L98 50 L107 50 C108 50,110 50,111 50 C112 49,112 49,113 48 C114 47,115 46,116 44 C117 43,117 41,118 39 L122 39 L118 53 L87 53 L87 51 Z" fill="#212121"/><path d="M134 5 L134 3 L149 3 L148 5 C148 5,147 6,146 6 C146 6,146 6,145 7 C145 7,145 8,144 9 C144 10,144 12,143 14 L139 35 C138 36,138 38,138 39 C138 40,137 42,137 43 C137 46,138 48,139 49 C141 50,143 51,145 51 C148 51,150 51,152 50 C153 49,155 47,156 45 C157 43,157 41,158 37 L163 14 C164 12,164 10,164 8 C164 7,164 6,163 6 C163 5,162 5,160 5 L161 3 L175 3 L174 5 C173 5,173 6,172 6 C172 6,172 6,171 7 C171 7,171 8,170 9 C170 10,170 12,169 14 L164 36 C163 41,162 44,160 47 C158 49,156 51,154 52 C151 53,148 54,144 54 C140 54,137 53,134 51 C132 49,131 46,131 42 C131 41,131 39,132 37 C132 36,132 34,133 31 L137 14 C137 12,137 10,137 8 C137 7,137 6,137 6 C136 5,135 5,134 5 L134 5 Z" fill="#212121"/><path d="M191 35 C191 43,192 51,195 56 C198 62,202 65,208 67 L207 70 C200 68,194 64,191 58 C187 52,185 44,185 35 C185 26,187 19,191 12 C194 6,200 2,207 0 L208 3 C202 5,198 9,195 14 C192 20,191 26,191 35 L191 35 Z" fill="#212121"/><path d="M226 36 C226 34,226 33,225 31 C225 29,225 27,224 26 C224 24,223 23,223 22 C223 22,223 22,222 21 C222 21,222 21,221 21 C221 21,220 21,220 21 C219 22,219 22,218 23 C218 23,217 24,216 25 L214 23 C216 21,217 20,218 19 C220 18,222 17,223 17 C224 17,225 17,225 17 C226 18,227 18,227 18 C227 19,228 19,228 20 C229 20,229 21,229 22 C230 23,230 24,230 26 C230 27,231 29,231 30 L231 30 C234 27,235 24,236 23 C238 21,239 20,240 19 C240 18,241 18,242 18 C243 17,244 17,245 17 C246 17,247 17,248 18 L246 25 L244 25 C244 24,243 23,243 23 C242 23,242 23,242 23 C242 23,241 23,241 24 C241 24,240 24,239 25 C239 26,238 27,237 28 C236 29,235 30,234 31 L232 34 C232 37,233 39,233 40 C234 42,234 44,234 45 C235 46,235 47,235 48 C235 48,236 49,236 49 C236 50,236 50,237 50 C237 50,237 50,238 50 C238 50,239 50,239 49 C240 49,241 48,242 46 L245 48 C243 50,241 52,240 53 C239 54,237 54,235 54 C234 54,233 54,232 53 C232 53,231 52,230 52 C230 51,229 49,229 48 C228 44,228 42,228 40 L227 40 C225 44,223 47,221 48 C220 50,219 51,218 52 C218 53,217 53,216 54 C215 54,214 54,213 54 C212 54,211 54,210 54 L212 46 L214 46 C214 47,215 48,215 48 C216 48,216 48,217 48 C217 48,217 47,218 46 C219 46,220 45,221 43 C222 42,224 39,226 36 L226 36 Z" fill="#212121"/><path d="M260 41 L258 47 L253 47 L254 41 L260 41 Z M253 69 C253 70,253 71,253 72 C253 72,253 73,253 73 C253 74,253 74,253 75 C253 75,254 75,254 75 C255 75,255 75,255 75 C256 75,256 75,256 74 C257 74,257 74,257 73 C258 73,258 72,259 72 L261 74 C260 75,259 76,258 76 C257 77,257 77,256 78 C255 78,255 79,254 79 C253 79,253 79,252 79 C251 79,251 79,250 79 C250 79,249 78,249 78 C248 77,248 77,248 76 C248 76,247 75,247 74 C247 74,248 73,248 72 C248 71,248 70,248 69 C249 68,249 67,249 66 C249 65,250 64,250 63 C250 62,250 61,250 60 C251 59,251 58,251 57 C251 57,251 57,251 56 C251 56,251 56,251 56 C251 55,251 55,250 55 C250 54,249 54,248 54 L248 52 L256 52 L257 52 L253 69 Z" fill="#212121"/><path d="M283 35 C283 26,282 20,279 14 C276 9,272 5,266 3 L267 0 C274 2,280 6,283 12 C287 19,289 26,289 35 C289 44,287 52,283 58 C280 64,274 68,267 70 L266 67 C272 65,276 62,279 56 C282 51,283 43,283 35 L283 35 Z" fill="#212121"/><path d="M322 27 L322 22 L368 22 L368 27 L322 27 Z M322 42 L322 37 L368 37 L368 42 L322 42 Z" fill="#212121"/><path d="M410 27 C412 23,415 21,417 19 C419 18,421 17,424 17 C426 17,428 18,429 19 C430 20,431 22,431 25 L431 25 C431 25,431 25,431 26 C433 23,435 21,437 19 C439 18,441 17,443 17 C446 17,447 18,449 19 C450 20,451 22,451 25 C451 26,450 29,449 31 L447 41 C446 44,446 46,446 47 C446 48,446 49,447 49 C447 50,447 50,448 50 C449 50,450 50,450 49 C451 49,452 48,454 46 L456 48 C454 50,452 52,451 53 C449 54,447 54,445 54 C444 54,442 53,441 52 C440 51,440 50,440 48 C440 46,440 44,441 41 L443 34 C443 32,444 30,444 29 C444 28,444 27,444 26 C444 24,444 23,443 22 C443 21,442 21,441 21 C440 21,438 22,437 22 C436 23,435 24,434 26 C432 28,431 29,430 31 C430 32,429 34,428 37 L425 53 L419 53 L423 34 C424 32,424 30,424 29 C424 28,424 27,424 26 C424 24,424 23,423 22 C423 21,422 21,420 21 C420 21,419 22,417 22 C416 23,415 24,414 26 C412 28,411 29,410 31 C410 33,409 35,408 37 L405 53 L399 53 L404 29 C405 27,405 25,405 24 C405 23,405 22,404 22 C404 21,404 21,403 21 C402 21,401 21,400 22 C400 23,398 24,397 25 L395 23 C397 21,399 20,400 19 C402 18,404 17,406 17 C407 17,408 18,409 19 C410 20,411 21,411 22 C411 24,410 25,410 27 L410 27 Z" fill="#212121"/><path d="M488 20 L492 17 L494 18 L489 42 C488 44,488 46,488 47 C488 48,488 49,488 49 C489 50,489 50,490 50 C491 50,492 50,492 49 C493 49,494 48,496 46 L498 48 C496 50,494 52,492 53 C491 54,489 54,487 54 C486 54,485 54,484 53 C483 52,482 50,482 49 C482 48,483 46,483 45 L483 44 C480 48,478 50,476 52 C474 53,472 54,469 54 C467 54,465 53,463 51 C462 49,461 46,461 42 C461 38,462 34,463 30 C465 26,467 23,470 21 C473 18,476 17,480 17 C481 17,483 17,484 18 C486 18,487 19,488 20 L488 20 Z M485 31 C485 30,486 29,486 28 C486 27,486 27,486 26 C486 24,485 22,485 21 C484 21,482 20,480 20 C478 20,476 21,474 23 C472 25,470 28,469 32 C468 35,467 39,467 42 C467 45,468 47,468 48 C469 49,470 50,472 50 C474 50,475 49,477 48 C478 47,480 44,481 42 C483 39,484 36,485 32 L485 31 Z" fill="#212121"/><path d="M516 36 C516 34,516 33,515 31 C515 29,515 27,514 26 C514 24,513 23,513 22 C513 22,513 22,512 21 C512 21,512 21,511 21 C511 21,510 21,510 21 C509 22,509 22,508 23 C508 23,507 24,506 25 L504 23 C506 21,507 20,508 19 C510 18,512 17,513 17 C514 17,515 17,515 17 C516 18,517 18,517 18 C517 19,518 19,518 20 C519 20,519 21,519 22 C520 23,520 24,520 26 C520 27,521 29,521 30 L521 30 C524 27,525 24,526 23 C528 21,529 20,530 19 C530 18,531 18,532 18 C533 17,534 17,535 17 C536 17,537 17,538 18 L536 25 L534 25 C534 24,533 23,533 23 C532 23,532 23,532 23 C532 23,531 23,531 24 C531 24,530 24,529 25 C529 26,528 27,527 28 C526 29,525 30,524 31 L522 34 C522 37,523 39,523 40 C524 42,524 44,524 45 C525 46,525 47,525 48 C525 48,526 49,526 49 C526 50,526 50,527 50 C527 50,527 50,528 50 C528 50,529 50,529 49 C530 49,531 48,532 46 L535 48 C533 50,531 52,530 53 C529 54,527 54,525 54 C524 54,523 54,522 53 C522 53,521 52,520 52 C520 51,519 49,519 48 C518 44,518 42,518 40 L517 40 C515 44,513 47,511 48 C510 50,509 51,508 52 C508 53,507 53,506 54 C505 54,504 54,503 54 C502 54,501 54,500 54 L502 46 L504 46 C504 47,505 48,505 48 C506 48,506 48,507 48 C507 48,507 47,508 46 C509 46,510 45,511 43 C512 42,514 39,516 36 L516 36 Z" fill="#212121"/><path d="M554 35 C554 43,555 51,558 56 C561 62,565 65,571 67 L570 70 C563 68,557 64,554 58 C550 52,548 44,548 35 C548 26,550 19,554 12 C557 6,563 2,570 0 L571 3 C565 5,561 9,558 14 C555 20,554 26,554 35 L554 35 Z" fill="#212121"/><path d="M593 54 C587 54,583 52,581 48 C578 44,577 37,577 29 C577 24,577 20,578 17 C579 13,580 11,582 8 C583 6,585 5,587 4 C589 3,591 2,593 2 C599 2,603 4,605 8 C608 13,609 19,609 27 C609 36,608 43,605 47 C602 52,598 54,593 54 L593 54 Z M584 27 C584 36,585 42,586 45 C588 49,590 51,593 51 C596 51,598 49,600 46 C601 42,602 37,602 29 C602 23,602 19,601 15 C600 12,599 9,598 7 C597 6,595 5,593 5 C591 5,590 6,589 7 C588 8,587 9,586 11 C585 13,585 15,584 18 C584 21,584 24,584 27 L584 27 Z M625 44 C626 46,626 48,626 49 C626 51,626 53,625 54 C625 55,624 57,622 58 C621 60,619 61,617 63 L615 61 C616 60,617 59,618 58 C618 57,618 56,619 54 C619 53,619 52,619 50 C619 48,619 46,619 44 L625 44 Z" fill="#212121"/><path d="M646 36 C646 34,646 33,645 31 C645 29,645 27,644 26 C644 24,643 23,643 22 C643 22,643 22,642 21 C642 21,642 21,641 21 C641 21,640 21,640 21 C639 22,639 22,638 23 C638 23,637 24,636 25 L634 23 C636 21,637 20,638 19 C640 18,642 17,643 17 C644 17,645 17,645 17 C646 18,647 18,647 18 C647 19,648 19,648 20 C649 20,649 21,649 22 C650 23,650 24,650 26 C650 27,651 29,651 30 L651 30 C654 27,655 24,656 23 C658 21,659 20,660 19 C660 18,661 18,662 18 C663 17,664 17,665 17 C666 17,667 17,668 18 L666 25 L664 25 C664 24,663 23,663 23 C662 23,662 23,662 23 C662 23,661 23,661 24 C661 24,660 24,659 25 C659 26,658 27,657 28 C656 29,655 30,654 31 L652 34 C652 37,653 39,653 40 C654 42,654 44,654 45 C655 46,655 47,655 48 C655 48,656 49,656 49 C656 50,656 50,657 50 C657 50,657 50,658 50 C658 50,659 50,659 49 C660 49,661 48,662 46 L665 48 C663 50,661 52,660 53 C659 54,657 54,655 54 C654 54,653 54,652 53 C652 53,651 52,650 52 C650 51,649 49,649 48 C648 44,648 42,648 40 L647 40 C645 44,643 47,641 48 C640 50,639 51,638 52 C638 53,637 53,636 54 C635 54,634 54,633 54 C632 54,631 54,630 54 L632 46 L634 46 C634 47,635 48,635 48 C636 48,636 48,637 48 C637 48,637 47,638 46 C639 46,640 45,641 43 C642 42,644 39,646 36 L646 36 Z" fill="#212121"/><path d="M680 41 L678 47 L673 47 L674 41 L680 41 Z M673 69 C673 70,673 71,673 72 C673 72,673 73,673 73 C673 74,673 74,673 75 C673 75,674 75,674 75 C675 75,675 75,675 75 C676 75,676 75,676 74 C677 74,677 74,677 73 C678 73,678 72,679 72 L681 74 C680 75,679 76,678 76 C677 77,677 77,676 78 C675 78,675 79,674 79 C673 79,673 79,672 79 C671 79,671 79,670 79 C670 79,669 78,669 78 C668 77,668 77,668 76 C668 76,667 75,667 74 C667 74,668 73,668 72 C668 71,668 70,668 69 C669 68,669 67,669 66 C669 65,670 64,670 63 C670 62,670 61,670 60 C671 59,671 58,671 57 C671 57,671 57,671 56 C671 56,671 56,671 56 C671 55,671 55,670 55 C670 54,669 54,668 54 L668 52 L676 52 L677 52 L673 69 Z" fill="#212121"/><path d="M703 35 C703 26,702 20,699 14 C696 9,692 5,686 3 L687 0 C694 2,700 6,703 12 C707 19,709 26,709 35 C709 44,707 52,703 58 C700 64,694 68,687 70 L686 67 C692 65,696 62,699 56 C702 51,703 43,703 35 L703 35 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">Since ReLU only misses positive weighted sum values, and negative ones are zeroed, the probability theory states that half the neurons will be deactivated most of the time. Consequently, the neurons of the subsequent layer will receive only half of the information, and the weighted sum of their inputs will be less. As the number of layers in the neural network increases, this effect will intensify: fewer and fewer neurons will reach the threshold value, and more and more information will be lost as it passes through the neural network.</span></p>
<p class="p_Text"><span class="f_Text">A solution was proposed by Kaiming He in February 2015 in the article &quot;<a href="https://arxiv.org/pdf/1502.01852.pdf%20Delving%20Deep%20into%20Rectifiers:" target="_blank" rel="external" class="weblink">Surpassing Human-Level Performance on ImageNet Classification</a>&quot;. In the article, it's suggested to initialize the weights for neurons with ReLU activation from a continuous normal distribution with a variance (&#948;</span><span class="f_Text" style="font-size: 7pt; vertical-align: super;">2</span><span class="f_Text">) equal to 2/n. And when using PReLU as activation, the distribution variance should be 2/((1+a</span><span class="f_Text" style="font-size: 7pt; vertical-align: super;">2</span><span class="f_Text">) *n). This method of initializing synaptic scales is called “He-initialization”.</span></p>
<p class="p_H3"><span class="f_H3">Initializing with a random orthogonal matrix</span></p>
<p class="p_Text"><span class="f_Text">In December 2013, Andrew M. Saxe presented a three-layer neural network in the form of matrix multiplication in the article &quot;<a href="https://arxiv.org/pdf/1312.6120v3.pdf" target="_blank" rel="external" class="weblink">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</a>&quot;, thereby showing the correspondence between the neural network and singular decomposition. The synaptic weight matrix of the first layer is represented by an orthogonal matrix, the vectors of which are the coordinates of the initial data in some </span><span class="f_Text" style="font-style: italic;">n</span><span class="f_Text">-dimensional space.</span></p>
<p class="p_Text"><span class="f_Text">Since the vectors of an orthogonal matrix are orthonormalized, the initial data projections they generate are completely independent. This approach allows for the neural network to be pre-prepared in such a way that each neuron will learn to recognize its feature in the input data independently of the training of other neurons located in the same layer.</span></p>
<p class="p_Text"><span class="f_Text">However, the method is not used widely, primarily due to the complexity of generating orthogonal matrices. The advantages of the method are demonstrated with the growth of the number of layers of the neural network. Therefore, in practice, initialization with orthogonal matrices can be found in deep neural networks when initialization with random values does not yield results.</span></p>
<p class="p_H3"><span class="f_H3">Using pre-trained neural networks</span></p>
<p class="p_Text"><span class="f_Text">This method can hardly be referred to as initialization, but its practical application is becoming increasingly popular. The essence of the method is as follows: to solve the problem, use a neural network that was trained on the same or similar data but solves different tasks. A series of lower layers are taken from a pre-trained neural network. These layers have already been trained to extract features from the initial data. Then, a few new layers of neurons are added, which will solve the given task based on the already extracted features.</span></p>
<p class="p_Text"><span class="f_Text">In the first step, pre-trained layers are blocked and new layers are trained. If the training fails to produce the desired result, the learning block is removed from the borrowed neural layers and the neural network is retrained.</span></p>
<p class="p_Text"><span class="f_Text">A variation of this method is the approach of first creating a multilayer neural network and training it to extract different features from the initial data. These can be unsupervised learning algorithms for dividing data into classes or autoencoder algorithms. In the latter, the neural network first extracts features from the initial data and then tries to return the original data based on the selected features.</span></p>
<p class="p_Text"><span class="f_Text">After pre-training, the layers of neurons responsible for feature extraction are taken, and additional layers of neurons for solving the given task are added to them.</span></p>
<p class="p_Text"><span class="f_Text">When constructing deep networks, this approach can help train the neural network faster compared to training a large neural network directly. This is because, during one training pass, a smaller neural network requires fewer operations to be performed compared to training a deep neural network. In addition, smaller neural networks are less prone to the risk of gradient explosion or vanishing.</span></p>
<p class="p_Text"><span class="f_Text">In the practical part of the book, we will return to the process of initializing neural networks and in practice evaluate the advantages and disadvantages of each method. </span></p>

</div>

</body>
</html>
