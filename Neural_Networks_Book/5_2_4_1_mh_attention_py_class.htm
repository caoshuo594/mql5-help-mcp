<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>5.2.4.1 Creating a new neural layer class</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="5_transformer.htm"> 5. Attention mechanisms </a> / <a class="h_m" href="5_2_mh_attention.htm"> 5.2 Multi-Head attention </a> / <a class="h_m" href="5_2_4_mh_attention_python.htm"> 5.2.4 Building Multi-Head Self-Attention in Python </a>/ 5.2.4.1 Creating a new neural layer class
          </td>
          <td width="70" align="right">
          <a href="5_2_4_mh_attention_python.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="5_2_4_2_mh_attention_py_script.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H3"><span class="f_H3">5.2.4.1 Creating a new neural layer class</span></p>
<p class="p_Text"><span class="f_Text">Let's get to the practical part and look at the implementation of our multi-head attention neural layer. To implement it, we create a new </span><span class="f_Text" style="font-style: italic;">MHAttention</span><span class="f_Text"> class that inherits from the base class of all neural layers </span><span class="f_Text" style="font-style: italic;">tf.keras.layers.Layer</span><span class="f_Text">.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">#&nbsp;Multi-Head&nbsp;Self-Attention&nbsp;Model</span><br>
<span class="f_CodeExample" style="color: #0000ff;">class</span><span class="f_CodeExample">&nbsp;MHAttention(tf.keras.layers.Layer):</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">First, we'll override the layer initialization method </span><span class="f_Text" style="font-style: italic;">__init__</span><span class="f_Text">. In the parameters of the initialization method, we will specify two constants:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li" style="font-style: italic;">key_size</span><span class="f_li"> – size of the vector describing one element of the sequence in the tensor of Keys</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">heads</span><span class="f_li"> – number of attention heads</span></li>
</ul>
<p class="p_Text"><span class="f_Text">In the body of the method, we will save the parameters in local variables for future use and immediately calculate the size of the concatenated output of attention heads into the variable </span><span class="f_Text" style="font-style: italic;">m_iDimension</span><span class="f_Text">.</span></p>
<p class="p_Text"><span class="f_Text">For your convenience, I made an effort to repeat the names of variables from the MQL5 implementation as much as possible.</span></p>
<p class="p_Text"><span class="f_Text">Next, we declare the internal objects of our neural layer. However, note that in this case, we do not specify the vector size of one element of the source data sequence. This is made possible by the use of multidimensional tensors.</span></p>
<p class="p_Text"><span class="f_Text">The </span><span class="f_Text" style="font-style: italic;">TensorFlow</span><span class="f_Text"> library works with multidimensional arrays or tensors represented as objects. This approach makes understanding the model more convenient and visual. To be able to implement the task in OpenCL, we were forced to use one-dimensional data buffers. To gain access to the required element, we calculated the offset in the one-dimensional buffer. Now, when using multidimensional arrays, to access the matrix element, we just need to specify the row and column of the element. It is convenient and clear.</span></p>
<p class="p_Text"><span class="f_Text">Another advantage of this approach is that we do not need to specify the dimension of the source data. We can get it from the tensor itself. We will take advantage of this. We won't ask the user for the size of the description vector for one element of the input data sequence. Instead, we will receive the input data tensor as a matrix. Each line of such a matrix is a vector description of one element of the sequence. We can operate with the size of this vector. That is, the first dimension indicates the number of elements of the sequence, and the second means the length of the description vector of one element of the sequence.</span></p>
<p class="p_Text"><span class="f_Text">However, there is also the other side of the coin. At the time of class initialization, we have not yet received the initial data. So, we do not know its size, as the user did not specify them in the parameters. Therefore, we cannot create all objects in the initialization method. But it doesn't matter. We will do what we can.</span></p>
<p class="p_Text"><span class="f_Text">In the initialization method, we will declare objects that can be created without understanding the dimension of the source data:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li" style="font-style: italic;">m_cQuerys</span><span class="f_li"> – neural layer for the formation of the concatenated tensor of queries </span><span class="f_li" style="font-style: italic;">Query</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">m_cKeys</span><span class="f_li"> – neural layer for the formation of the concatenated tensor of keys </span><span class="f_li" style="font-style: italic;">Key</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">m_cValues</span><span class="f_li"> – neural layer for the formation of the concatenated tensor of values </span><span class="f_li" style="font-style: italic;">Values</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">m_cNormAttention</span><span class="f_li"> – data normalization layer for the </span><span class="f_li" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_li"> block</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">m_cNormOutput</span><span class="f_li"> – normalization layer for the results of the neural layer</span></li>
</ul>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">def</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="font-weight: bold;">__init__</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">,key_size,&nbsp;heads,&nbsp;**kwargs):</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;super(MHAttention,&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">).</span><span class="f_CodeExample" style="font-weight: bold;">__init__</span><span class="f_CodeExample">(**kwargs)</span>
<br><span class="f_CodeExample">&nbsp;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iHeads&nbsp;=&nbsp;heads</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iKeysSize&nbsp;=&nbsp;key_size</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iDimension=</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iHeads*</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iKeysSize;</span>
<br><span class="f_CodeExample">&nbsp;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cQuerys&nbsp;=&nbsp;tf.keras.layers.Dense(</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iDimension)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cKeys&nbsp;=&nbsp;tf.keras.layers.Dense(</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iDimension)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cValues&nbsp;=&nbsp;tf.keras.layers.Dense(</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iDimension)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cNormAttention=tf.keras.layers.LayerNormalization(epsilon=</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1e-6</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cNormOutput=tf.keras.layers.LayerNormalization(epsilon=</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1e-6</span><span class="f_CodeExample">)</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After creating the initialization method, we proceed to the </span><span class="f_Text" style="font-style: italic;">build</span><span class="f_Text"> method. This method will allow us to initialize the missing objects. This method is run only once before the first call of the </span><span class="f_Text" style="font-style: italic;">call </span><span class="f_Text">method. It receives the source data size in the parameters. Knowing this size, we can initialize objects, structures, and/or parameters that depend on the size of the source data.</span></p>
<p class="p_Text"><span class="f_Text">In the method body, we save the last dimension of the source data tensor as the size of the description vector of one element of the source data sequence to the </span><span class="f_Text" style="font-style: italic;">m_iWindow</span><span class="f_Text"> local variable. After that, we will create three more internal neural layers:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li" style="font-style: italic;">m_cW0</span><span class="f_li"> – fully connected layer of the reduction matrix </span><span class="f_li" style="font-style: italic;">W</span><span class="f_li" style="font-size: 7pt; font-style: italic; vertical-align: sub;">0</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">m_cFF1</span><span class="f_li"> – the first fully connected layer of the </span><span class="f_li" style="font-style: italic;">Feed Forward</span><span class="f_li"> block</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">m_cFF2</span><span class="f_li"> – the second fully connected layer of the </span><span class="f_li" style="font-style: italic;">Feed Forward </span><span class="f_li">block</span></li>
</ul>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">def</span><span class="f_CodeExample">&nbsp;build(</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">,&nbsp;input_shape):</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iWindow=input_shape[-</span><span class="f_CodeExample" style="color: #008100;">1</span><span class="f_CodeExample">]</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cW0&nbsp;=&nbsp;tf.keras.layers.Dense(</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iWindow)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cFF1=tf.keras.layers.Dense(</span><span class="f_CodeExample" style="color: #008100;">4</span><span class="f_CodeExample">*</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iWindow,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;activation=tf.nn.swish)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cFF2=tf.keras.layers.Dense(</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iWindow)</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">So, we have defined all the internal objects necessary to implement the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> algorithm inside our new layer. Before proceeding with the implementation, let's once again look at how we can write the algorithm of multi-head attention using matrix mathematics since when working with multidimensional tensors, we must operate with matrix operations.</span></p>
<p class="p_Text"><span class="f_Text">The first step is to define the </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text">, </span><span class="f_Text" style="font-style: italic;">Key</span><span class="f_Text">, and</span><span class="f_Text" style="font-style: italic;"> Value</span><span class="f_Text"> tensors. To obtain query data, we need to multiply the tensor of the source data by the corresponding matrix of weights. This operation is performed in three internal neural layers.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">def</span><span class="f_CodeExample">&nbsp;call(</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">,&nbsp;data):</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;batch_size&nbsp;=&nbsp;tf.shape(data)[</span><span class="f_CodeExample" style="color: #008100;">0</span><span class="f_CodeExample">]</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;query&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cQuerys(data)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;key&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cKeys(data)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cValues(data)</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">The second step is to determine the matrix of dependency coefficients. According to the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm, we first need to multiply the query tensor by the transposed key tensor.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:183px;height:22px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 732 88"><path d="M6 57 C6 60,7 62,8 64 C10 65,12 66,15 66 C18 66,21 65,23 63 C24 61,25 59,25 55 C25 54,25 52,25 51 C24 50,23 49,22 48 C21 47,20 46,18 44 C16 43,14 42,13 40 C12 39,11 38,10 37 C10 35,9 33,9 32 C9 29,10 26,11 24 C13 22,15 20,17 19 C20 18,23 17,26 17 C28 17,30 17,32 18 C34 18,36 18,39 19 L37 28 L33 28 C33 26,33 25,32 23 C32 22,31 21,30 21 C29 20,27 20,26 20 C24 20,22 21,20 21 C19 22,18 23,17 25 C16 26,16 28,16 30 C16 31,16 33,17 34 C18 36,20 38,23 39 C25 41,27 43,28 44 C29 45,30 47,31 48 C32 50,32 52,32 53 C32 57,31 59,30 62 C28 64,26 66,24 67 C21 68,18 69,15 69 C13 69,10 69,8 69 C5 68,3 68,1 67 L3 57 L6 57 Z" fill="#212121"/><path d="M68 42 C68 40,67 39,67 38 C67 37,66 36,66 36 C65 35,65 35,63 35 C61 35,59 36,57 38 C55 40,53 43,52 47 C51 51,50 54,50 57 C50 60,51 62,52 63 C53 64,54 65,56 65 C58 65,60 65,62 64 C63 63,65 62,67 60 L69 62 C67 65,65 66,62 67 C60 69,57 69,55 69 C51 69,48 68,47 66 C45 64,44 61,44 56 C44 54,44 51,45 48 C46 45,48 42,50 39 C51 37,54 35,56 34 C59 33,61 32,64 32 C68 32,70 32,73 33 L71 42 L68 42 Z" fill="#212121"/><path d="M91 69 C87 69,84 68,82 66 C80 64,79 61,79 56 C79 55,79 52,80 50 C81 46,82 43,84 40 C86 38,88 36,90 34 C93 33,96 32,99 32 C103 32,106 33,108 35 C110 38,111 41,111 45 C111 48,110 51,109 54 C108 57,107 60,105 62 C104 65,102 66,99 67 C97 69,94 69,91 69 L91 69 Z M85 58 C85 61,86 63,87 64 C88 65,90 66,92 66 C95 66,97 65,99 63 C100 61,102 58,103 54 C104 50,105 46,105 43 C105 41,104 39,103 37 C102 36,100 35,98 35 C96 35,93 36,92 38 C90 41,88 44,87 48 C86 52,85 55,85 58 L85 58 Z" fill="#212121"/><path d="M131 42 C134 38,136 36,138 34 C140 33,143 32,145 32 C147 32,148 32,149 32 L148 41 L144 41 C144 40,143 39,143 39 C143 38,143 38,142 37 C142 37,141 37,141 37 C140 37,139 37,138 38 C137 39,136 40,134 42 C133 43,132 45,131 46 C131 48,130 50,129 52 L126 68 L120 68 L125 44 C125 43,126 42,126 41 C126 40,126 40,126 39 C126 38,126 37,125 37 C125 36,125 36,124 36 C123 36,122 36,121 37 C121 38,119 39,118 40 L116 38 C118 36,120 34,122 33 C123 33,125 32,127 32 C128 32,129 33,130 34 C131 35,131 36,131 37 C131 39,131 40,131 41 L131 42 Z" fill="#212121"/><path d="M182 61 C179 64,177 66,174 67 C172 68,169 69,166 69 C162 69,160 68,158 66 C156 64,155 61,155 57 C155 54,155 51,156 48 C157 45,159 42,161 40 C162 37,165 35,167 34 C170 33,173 32,176 32 C179 32,181 33,183 34 C184 35,185 37,185 40 C185 44,183 47,179 49 C175 51,169 52,162 52 C161 54,161 55,161 57 C161 60,162 62,163 63 C164 64,165 65,168 65 C170 65,172 65,174 64 C175 63,177 61,179 59 L182 61 Z M162 49 C166 49,169 49,171 48 C174 48,175 47,177 45 C178 44,179 42,179 40 C179 38,178 37,178 36 C177 36,176 35,175 35 C172 35,170 36,167 39 C165 41,163 45,162 49 L162 49 Z" fill="#212121"/><path d="M216 42 L216 37 L262 37 L262 42 L216 42 Z M216 57 L216 52 L262 52 L262 57 L216 57 Z" fill="#212121"/><path d="M313 68 C314 70,314 72,315 73 C315 74,316 75,316 76 C317 76,317 77,317 77 C318 78,318 78,319 78 C319 78,320 78,320 78 C321 78,322 78,323 77 C324 77,325 76,326 75 L328 77 C326 79,324 80,323 81 C321 82,319 82,318 82 C316 82,314 82,313 81 C311 80,310 78,309 77 C308 75,307 72,307 69 C302 69,299 67,296 65 C294 62,293 58,293 53 C293 49,294 45,295 40 C296 36,298 32,300 28 C303 24,305 22,309 20 C312 18,315 17,319 17 C324 17,327 19,330 21 C333 24,334 28,334 33 C334 37,333 41,332 45 C331 49,330 53,328 56 C326 60,324 62,321 64 C319 66,316 68,313 68 L313 68 Z M327 32 C327 28,326 25,325 23 C324 21,321 20,319 20 C316 20,314 21,311 23 C309 25,307 28,305 31 C304 35,302 39,301 43 C300 47,300 51,300 54 C300 62,303 66,308 66 C311 66,315 64,317 61 C320 58,323 53,324 48 C326 42,327 37,327 32 L327 32 Z" fill="#212121"/><path d="M372 57 C372 59,371 61,371 62 C371 63,371 64,372 64 C372 65,373 65,373 65 C374 65,375 65,376 64 C376 64,378 63,379 61 L381 63 C379 65,377 67,376 68 C374 69,372 69,371 69 C369 69,368 69,367 68 C366 67,366 65,366 64 C366 63,366 61,366 60 L366 59 C364 63,361 65,359 67 C357 68,355 69,352 69 C350 69,348 68,347 67 C346 66,345 64,345 61 C345 60,346 58,346 55 L349 45 C349 42,350 40,350 39 C350 38,350 37,349 37 C349 36,349 36,348 36 C347 36,346 36,345 37 C345 38,343 39,342 40 L340 38 C342 36,344 35,345 34 C346 33,347 33,348 33 C349 32,350 32,351 32 C352 32,354 33,355 34 C355 35,356 36,356 38 C356 40,356 43,355 46 L353 52 C353 54,352 55,352 56 C352 57,352 58,352 58 C352 59,352 60,352 60 C352 62,352 63,353 64 C353 65,354 65,355 65 C356 65,357 65,359 64 C360 63,361 62,362 60 C364 58,365 57,366 55 C366 54,367 52,368 49 L371 33 L377 33 L372 57 Z" fill="#212121"/><path d="M413 61 C410 64,408 66,405 67 C403 68,400 69,397 69 C393 69,391 68,389 66 C387 64,386 61,386 57 C386 54,386 51,387 48 C388 45,390 42,392 40 C393 37,396 35,398 34 C401 33,404 32,407 32 C410 32,412 33,414 34 C415 35,416 37,416 40 C416 44,414 47,410 49 C406 51,400 52,393 52 C392 54,392 55,392 57 C392 60,393 62,394 63 C395 64,396 65,399 65 C401 65,403 65,405 64 C406 63,408 61,410 59 L413 61 Z M393 49 C397 49,400 49,402 48 C405 48,406 47,408 45 C409 44,410 42,410 40 C410 38,409 37,409 36 C408 36,407 35,406 35 C403 35,401 36,398 39 C396 41,394 45,393 49 L393 49 Z" fill="#212121"/><path d="M435 42 C438 38,440 36,442 34 C444 33,447 32,449 32 C451 32,452 32,453 32 L452 41 L448 41 C448 40,447 39,447 39 C447 38,447 38,446 37 C446 37,445 37,445 37 C444 37,443 37,442 38 C441 39,440 40,438 42 C437 43,436 45,435 46 C435 48,434 50,433 52 L430 68 L424 68 L429 44 C429 43,430 42,430 41 C430 40,430 40,430 39 C430 38,430 37,429 37 C429 36,429 36,428 36 C427 36,426 36,425 37 C425 38,423 39,422 40 L420 38 C422 36,424 34,426 33 C427 33,429 32,431 32 C432 32,433 33,434 34 C435 35,435 36,435 37 C435 39,435 40,435 41 L435 42 Z" fill="#212121"/><path d="M473 68 C472 64,471 59,470 53 C469 46,468 42,467 40 C467 38,466 37,466 37 C466 36,465 36,465 36 C464 36,463 36,462 37 C462 38,461 39,460 40 L457 38 C459 36,461 35,462 34 C464 33,465 32,467 32 C468 32,469 32,470 32 C470 33,471 33,471 34 C472 34,472 35,472 35 C473 36,473 37,473 38 C474 39,474 41,474 43 C475 46,476 49,476 53 C477 56,477 60,477 63 C480 59,482 56,484 53 C485 49,486 47,487 44 C488 42,488 40,488 38 C488 37,488 36,487 36 C487 35,486 35,485 35 L485 33 L495 33 L496 35 C494 40,492 45,488 51 C485 57,482 63,478 68 C474 73,471 77,468 80 C466 81,464 83,463 83 C462 84,460 84,459 84 C458 84,458 84,457 84 C456 84,456 84,455 84 L457 77 L460 77 C460 78,460 79,461 79 C463 79,464 78,466 76 C468 75,470 72,473 68 L473 68 Z" fill="#212121"/><path d="M546 52 L544 56 L534 49 L536 61 L531 61 L532 49 L522 56 L520 52 L530 47 L520 42 L522 38 L532 45 L531 33 L536 33 L534 45 L544 38 L546 42 L536 47 L546 52 Z" fill="#212121"/><path d="M586 40 L587 40 C588 40,589 40,590 40 C591 39,592 39,593 38 C594 37,596 35,598 33 C601 31,603 29,604 28 C605 27,605 25,606 25 C606 24,607 23,607 22 C607 22,606 21,606 21 C605 20,605 20,604 20 L604 18 L620 18 L619 20 C618 20,617 21,616 22 C615 22,613 24,610 26 L596 40 L604 59 C605 61,606 62,606 63 C607 64,607 65,608 65 C609 66,610 66,611 66 L610 68 L596 68 L596 66 C598 66,598 65,598 64 C598 64,598 63,598 62 C598 61,597 60,597 59 L592 48 C592 46,591 45,591 45 C591 44,590 44,590 43 C589 43,588 43,587 43 L585 43 L582 57 C582 59,582 60,582 61 C582 61,582 62,582 63 C582 64,582 65,582 65 C583 66,584 66,585 66 L585 68 L570 68 L570 66 C571 66,572 66,573 65 C573 65,573 65,574 64 C574 63,574 63,575 62 C575 61,575 59,576 57 L582 29 C583 27,583 25,583 23 C583 22,583 21,582 21 C581 20,581 20,579 20 L580 18 L594 18 L594 20 C593 20,592 21,592 21 C592 21,591 21,591 22 C591 22,590 23,590 24 C590 25,589 27,589 29 L586 40 Z" fill="#212121"/><path d="M651 61 C648 64,646 66,643 67 C641 68,638 69,635 69 C631 69,629 68,627 66 C625 64,624 61,624 57 C624 54,624 51,625 48 C626 45,628 42,630 40 C631 37,634 35,636 34 C639 33,642 32,645 32 C648 32,650 33,652 34 C653 35,654 37,654 40 C654 44,652 47,648 49 C644 51,638 52,631 52 C630 54,630 55,630 57 C630 60,631 62,632 63 C633 64,634 65,637 65 C639 65,641 65,643 64 C644 63,646 61,648 59 L651 61 Z M631 49 C635 49,638 49,640 48 C643 48,644 47,646 45 C647 44,648 42,648 40 C648 38,647 37,647 36 C646 36,645 35,644 35 C641 35,639 36,636 39 C634 41,632 45,631 49 L631 49 Z" fill="#212121"/><path d="M675 68 C674 64,673 59,672 53 C671 46,670 42,669 40 C669 38,668 37,668 37 C668 36,667 36,667 36 C666 36,665 36,664 37 C664 38,663 39,662 40 L659 38 C661 36,663 35,664 34 C666 33,667 32,669 32 C670 32,671 32,672 32 C672 33,673 33,673 34 C674 34,674 35,674 35 C675 36,675 37,675 38 C676 39,676 41,676 43 C677 46,678 49,678 53 C679 56,679 60,679 63 C682 59,684 56,686 53 C687 49,688 47,689 44 C690 42,690 40,690 38 C690 37,690 36,689 36 C689 35,688 35,687 35 L687 33 L697 33 L698 35 C696 40,694 45,690 51 C687 57,684 63,680 68 C676 73,673 77,670 80 C668 81,666 83,665 83 C664 84,662 84,661 84 C660 84,660 84,659 84 C658 84,658 84,657 84 L659 77 L662 77 C662 78,662 79,663 79 C665 79,666 78,668 76 C670 75,672 72,675 68 L675 68 Z" fill="#212121"/><path d="M705 35 L706 33 C706 33,706 33,707 33 C707 33,707 33,708 33 C708 32,708 32,708 32 C708 32,709 31,709 31 C709 31,709 30,709 29 C709 29,710 28,710 27 L715 3 L711 3 C711 3,710 3,710 3 C710 3,709 3,709 3 C708 4,708 4,708 4 C708 4,707 5,707 5 C707 6,706 6,706 7 C706 8,705 8,705 9 L702 9 L704 0 L733 0 L731 10 L728 10 L728 8 C728 8,728 8,728 7 C728 7,728 7,728 6 C728 6,728 5,728 5 C728 5,728 5,728 4 C727 4,727 4,727 4 C727 4,727 3,727 3 C726 3,726 3,726 3 C726 3,725 3,725 3 L720 3 L715 27 C715 28,715 29,715 29 C714 30,714 31,714 31 C714 31,714 32,715 32 C715 32,715 32,715 33 C715 33,716 33,716 33 C716 33,717 33,717 33 L717 35 L705 35 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">Everything is simple for just one attention head. But we have concatenated tensors, which in the last dimension contain the data of all attention heads. Multiplying them in this form will give us a result comparable to one-headed attention. As an option, we can transform the two-dimensional tensor into a three-dimensional one, separating the attention head into a distinct dimension.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:400px;height:17px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 1600 68"><path d="M24 70 L8 70 L8 0 L24 0 L24 3 L14 3 L14 67 L24 67 L24 70 Z" fill="#212121"/><path d="M35 5 L35 3 L50 3 L49 5 C49 5,48 6,47 6 C47 6,47 6,46 7 C46 7,46 8,45 9 C45 10,45 12,44 14 L40 35 C39 36,39 38,39 39 C39 40,38 42,38 43 C38 46,39 48,40 49 C42 50,44 51,46 51 C49 51,51 51,53 50 C54 49,56 47,57 45 C58 43,58 41,59 37 L64 14 C65 12,65 10,65 8 C65 7,65 6,64 6 C64 5,63 5,61 5 L62 3 L76 3 L75 5 C74 5,74 6,73 6 C73 6,73 6,72 7 C72 7,72 8,71 9 C71 10,71 12,70 14 L65 36 C64 41,63 44,61 47 C59 49,57 51,55 52 C52 53,49 54,45 54 C41 54,38 53,35 51 C33 49,32 46,32 42 C32 41,32 39,33 37 C33 36,33 34,34 31 L38 14 C38 12,38 10,38 8 C38 7,38 6,38 6 C37 5,36 5,35 5 L35 5 Z" fill="#212121"/><path d="M88 29 C89 27,89 25,89 24 C89 23,89 22,88 22 C88 21,88 21,87 21 C86 21,85 21,84 22 C84 23,82 24,81 25 L79 23 C81 21,83 19,85 18 C86 18,88 17,90 17 C91 17,92 18,93 19 C94 20,95 21,95 22 C95 24,94 25,94 27 L94 27 C96 23,99 21,101 19 C103 18,105 17,108 17 C110 17,112 18,113 19 C114 20,115 22,115 25 C115 26,114 29,114 31 L111 41 C111 44,110 46,110 47 C110 48,110 49,111 49 C111 50,112 50,112 50 C113 50,114 50,115 49 C115 49,117 48,118 46 L120 48 C118 50,116 52,115 53 C113 54,111 54,109 54 C108 54,107 53,106 52 C105 51,104 50,104 48 C104 46,105 44,105 41 L107 34 C108 32,108 30,108 29 C108 28,108 27,108 26 C108 24,108 23,108 22 C107 21,106 21,105 21 C104 21,103 22,101 22 C100 23,99 24,98 26 C96 28,95 29,94 31 C94 33,93 35,92 37 L89 53 L83 53 L88 29 Z" fill="#212121"/><path d="M142 3 L140 10 L134 10 L135 3 L142 3 Z M130 29 C130 26,131 24,131 23 C131 22,130 21,130 21 C129 20,128 20,127 20 L127 18 L136 17 L138 17 L133 42 C133 44,132 46,132 47 C132 48,132 49,133 49 C133 50,134 50,134 50 C135 50,136 50,137 49 C137 49,139 48,140 46 L142 48 C140 50,138 52,136 53 C135 54,133 54,131 54 C130 54,129 53,128 52 C127 51,126 50,126 48 C126 46,127 44,127 41 L130 29 Z" fill="#212121"/><path d="M170 46 C168 49,166 51,164 52 C162 53,160 54,158 54 C153 54,151 52,151 46 C151 45,151 43,151 41 L155 22 L149 22 L150 20 C151 20,152 20,153 20 C154 20,154 19,155 19 C155 19,156 18,156 17 C157 17,157 16,158 15 C158 14,159 12,159 9 L164 9 L162 18 L173 18 L173 22 L162 22 L158 36 C158 39,157 41,157 43 C157 44,157 45,157 45 C157 49,158 50,161 50 C162 50,163 50,164 49 C165 48,167 46,168 44 L170 46 Z" fill="#212121"/><path d="M203 27 C202 25,202 23,200 22 C199 21,198 20,195 20 C193 20,192 21,190 22 C189 23,189 24,189 26 C189 27,189 27,189 28 C189 29,190 30,191 30 C192 31,193 32,195 33 C197 34,198 35,199 36 C200 37,201 38,201 38 C202 39,202 40,202 41 C202 42,203 43,203 44 C203 46,202 48,201 49 C200 51,198 52,196 53 C194 54,192 54,189 54 C187 54,185 54,183 54 C181 53,179 53,177 52 L179 44 L181 44 C182 46,182 48,183 49 C185 51,187 51,189 51 C192 51,193 51,195 50 C196 49,197 47,197 45 C197 44,196 43,196 42 C196 41,195 41,194 40 C193 39,192 38,190 37 C188 36,187 35,186 34 C185 33,184 32,184 31 C183 30,183 28,183 27 C183 25,183 23,184 22 C185 20,187 19,189 18 C191 18,193 17,195 17 C198 17,200 17,202 18 C204 18,205 18,207 19 L206 27 L203 27 Z" fill="#212121"/><path d="M223 44 C224 46,224 48,224 49 C224 51,224 53,223 54 C223 55,222 57,220 58 C219 60,217 61,215 63 L213 61 C214 60,215 59,216 58 C216 57,216 56,217 54 C217 53,217 52,217 50 C217 48,217 46,217 44 L223 44 Z" fill="#212121"/><path d="M284 4 L281 13 L278 13 C278 10,277 8,276 7 C275 6,273 5,271 5 C267 5,264 7,261 10 C258 14,256 18,254 24 C253 30,252 35,252 40 C252 44,253 47,254 48 C255 50,257 51,260 51 C262 51,263 51,265 50 C266 50,267 49,269 47 C270 46,271 44,272 41 L276 41 L273 52 C269 53,264 54,259 54 C254 54,251 53,249 50 C246 47,245 44,245 38 C245 35,246 30,247 26 C248 21,250 17,252 13 C255 10,257 7,261 5 C264 3,267 2,271 2 C273 2,275 2,277 2 C279 3,281 3,284 4 L284 4 Z" fill="#212121"/><path d="M301 54 C297 54,294 53,292 51 C290 49,289 46,289 41 C289 40,289 37,290 35 C291 31,292 28,294 25 C296 23,298 21,300 19 C303 18,306 17,309 17 C313 17,316 18,318 20 C320 23,321 26,321 30 C321 33,320 36,319 39 C318 42,317 45,315 47 C314 50,312 51,309 52 C307 54,304 54,301 54 L301 54 Z M295 43 C295 46,296 48,297 49 C298 50,300 51,302 51 C305 51,307 50,309 48 C310 46,312 43,313 39 C314 35,315 31,315 28 C315 26,314 24,313 22 C312 21,310 20,308 20 C306 20,303 21,302 23 C300 26,298 29,297 33 C296 37,295 40,295 43 L295 43 Z" fill="#212121"/><path d="M335 29 C336 27,336 25,336 24 C336 23,336 22,335 22 C335 21,335 21,334 21 C333 21,332 21,331 22 C331 23,329 24,328 25 L326 23 C328 21,330 19,332 18 C333 18,335 17,337 17 C338 17,339 18,340 19 C341 20,342 21,342 22 C342 24,341 25,341 27 L341 27 C343 23,346 21,348 19 C350 18,352 17,355 17 C357 17,359 18,360 19 C361 20,362 22,362 25 C362 26,361 29,361 31 L358 41 C358 44,357 46,357 47 C357 48,357 49,358 49 C358 50,359 50,359 50 C360 50,361 50,362 49 C362 49,364 48,365 46 L367 48 C365 50,363 52,362 53 C360 54,358 54,356 54 C355 54,354 53,353 52 C352 51,351 50,351 48 C351 46,352 44,352 41 L354 34 C355 32,355 30,355 29 C355 28,355 27,355 26 C355 24,355 23,355 22 C354 21,353 21,352 21 C351 21,350 22,348 22 C347 23,346 24,345 26 C343 28,342 29,341 31 C341 33,340 35,339 37 L336 53 L330 53 L335 29 Z" fill="#212121"/><path d="M396 27 C396 25,395 24,395 23 C395 22,394 21,394 21 C393 20,393 20,391 20 C389 20,387 21,385 23 C383 25,381 28,380 32 C379 36,378 39,378 42 C378 45,379 47,380 48 C381 49,382 50,384 50 C386 50,388 50,390 49 C391 48,393 47,395 45 L397 47 C395 50,393 51,390 52 C388 54,385 54,383 54 C379 54,376 53,375 51 C373 49,372 46,372 41 C372 39,372 36,373 33 C374 30,376 27,378 24 C379 22,382 20,384 19 C387 18,389 17,392 17 C396 17,398 17,401 18 L399 27 L396 27 Z" fill="#212121"/><path d="M434 20 L438 17 L440 18 L435 42 C434 44,434 46,434 47 C434 48,434 49,434 49 C435 50,435 50,436 50 C437 50,438 50,438 49 C439 49,440 48,442 46 L444 48 C442 50,440 52,438 53 C437 54,435 54,433 54 C432 54,431 54,430 53 C429 52,428 50,428 49 C428 48,429 46,429 45 L429 44 C426 48,424 50,422 52 C420 53,418 54,415 54 C413 54,411 53,409 51 C408 49,407 46,407 42 C407 38,408 34,409 30 C411 26,413 23,416 21 C419 18,422 17,426 17 C427 17,429 17,430 18 C432 18,433 19,434 20 L434 20 Z M431 31 C431 30,432 29,432 28 C432 27,432 27,432 26 C432 24,431 22,431 21 C430 21,428 20,426 20 C424 20,422 21,420 23 C418 25,416 28,415 32 C414 35,413 39,413 42 C413 45,414 47,414 48 C415 49,416 50,418 50 C420 50,421 49,423 48 C424 47,426 44,427 42 C429 39,430 36,431 32 L431 31 Z" fill="#212121"/><path d="M470 46 C468 49,466 51,464 52 C462 53,460 54,458 54 C453 54,451 52,451 46 C451 45,451 43,451 41 L455 22 L449 22 L450 20 C451 20,452 20,453 20 C454 20,454 19,455 19 C455 19,456 18,456 17 C457 17,457 16,458 15 C458 14,459 12,459 9 L464 9 L462 18 L473 18 L473 22 L462 22 L458 36 C458 39,457 41,457 43 C457 44,457 45,457 45 C457 49,458 50,461 50 C462 50,463 50,464 49 C465 48,467 46,468 44 L470 46 Z" fill="#212121"/><path d="M506 46 C503 49,501 51,498 52 C496 53,493 54,490 54 C486 54,484 53,482 51 C480 49,479 46,479 42 C479 39,479 36,480 33 C481 30,483 27,485 25 C486 22,489 20,491 19 C494 18,497 17,500 17 C503 17,505 18,507 19 C508 20,509 22,509 25 C509 29,507 32,503 34 C499 36,493 37,486 37 C485 39,485 40,485 42 C485 45,486 47,487 48 C488 49,489 50,492 50 C494 50,496 50,498 49 C499 48,501 46,503 44 L506 46 Z M486 34 C490 34,493 34,495 33 C498 33,499 32,501 30 C502 29,503 27,503 25 C503 23,502 22,502 21 C501 21,500 20,499 20 C496 20,494 21,491 24 C489 26,487 30,486 34 L486 34 Z" fill="#212121"/><path d="M522 29 C523 27,523 25,523 24 C523 23,523 22,522 22 C522 21,522 21,521 21 C520 21,519 21,518 22 C518 23,516 24,515 25 L513 23 C515 21,517 19,519 18 C520 18,522 17,524 17 C525 17,526 18,527 19 C528 20,529 21,529 22 C529 24,528 25,528 27 L528 27 C530 23,533 21,535 19 C537 18,539 17,542 17 C544 17,546 18,547 19 C548 20,549 22,549 25 C549 26,548 29,548 31 L545 41 C545 44,544 46,544 47 C544 48,544 49,545 49 C545 50,546 50,546 50 C547 50,548 50,549 49 C549 49,551 48,552 46 L554 48 C552 50,550 52,549 53 C547 54,545 54,543 54 C542 54,541 53,540 52 C539 51,538 50,538 48 C538 46,539 44,539 41 L541 34 C542 32,542 30,542 29 C542 28,542 27,542 26 C542 24,542 23,542 22 C541 21,540 21,539 21 C538 21,537 22,535 22 C534 23,533 24,532 26 C530 28,529 29,528 31 C528 33,527 35,526 37 L523 53 L517 53 L522 29 Z" fill="#212121"/><path d="M586 20 L590 17 L592 18 L587 42 C586 44,586 46,586 47 C586 48,586 49,586 49 C587 50,587 50,588 50 C589 50,590 50,590 49 C591 49,592 48,594 46 L596 48 C594 50,592 52,590 53 C589 54,587 54,585 54 C584 54,583 54,582 53 C581 52,580 50,580 49 C580 48,581 46,581 45 L581 44 C578 48,576 50,574 52 C572 53,570 54,567 54 C565 54,563 53,561 51 C560 49,559 46,559 42 C559 38,560 34,561 30 C563 26,565 23,568 21 C571 18,574 17,578 17 C579 17,581 17,582 18 C584 18,585 19,586 20 L586 20 Z M583 31 C583 30,584 29,584 28 C584 27,584 27,584 26 C584 24,583 22,583 21 C582 21,580 20,578 20 C576 20,574 21,572 23 C570 25,568 28,567 32 C566 35,565 39,565 42 C565 45,566 47,566 48 C567 49,568 50,570 50 C572 50,573 49,575 48 C576 47,578 44,579 42 C581 39,582 36,583 32 L583 31 Z" fill="#212121"/><path d="M622 46 C620 49,618 51,616 52 C614 53,612 54,610 54 C605 54,603 52,603 46 C603 45,603 43,603 41 L607 22 L601 22 L602 20 C603 20,604 20,605 20 C606 20,606 19,607 19 C607 19,608 18,608 17 C609 17,609 16,610 15 C610 14,611 12,611 9 L616 9 L614 18 L625 18 L625 22 L614 22 L610 36 C610 39,609 41,609 43 C609 44,609 45,609 45 C609 49,610 50,613 50 C614 50,615 50,616 49 C617 48,619 46,620 44 L622 46 Z" fill="#212121"/><path d="M658 46 C655 49,653 51,650 52 C648 53,645 54,642 54 C638 54,636 53,634 51 C632 49,631 46,631 42 C631 39,631 36,632 33 C633 30,635 27,637 25 C638 22,641 20,643 19 C646 18,649 17,652 17 C655 17,657 18,659 19 C660 20,661 22,661 25 C661 29,659 32,655 34 C651 36,645 37,638 37 C637 39,637 40,637 42 C637 45,638 47,639 48 C640 49,641 50,644 50 C646 50,648 50,650 49 C651 48,653 46,655 44 L658 46 Z M638 34 C642 34,645 34,647 33 C650 33,651 32,653 30 C654 29,655 27,655 25 C655 23,654 22,654 21 C653 21,652 20,651 20 C648 20,646 21,643 24 C641 26,639 30,638 34 L638 34 Z" fill="#212121"/><path d="M668 67 L678 67 L678 3 L668 3 L668 0 L684 0 L684 70 L668 70 L668 67 Z" fill="#212121"/><path d="M718 27 L718 22 L764 22 L764 27 L718 27 Z M718 42 L718 37 L764 37 L764 42 L718 42 Z" fill="#212121"/><path d="M774 55 L774 49 L814 32 L774 14 L774 8 L820 29 L820 34 L774 55 Z" fill="#212121"/><path d="M870 70 L854 70 L854 0 L870 0 L870 3 L860 3 L860 67 L870 67 L870 70 Z" fill="#212121"/><path d="M881 5 L881 3 L896 3 L895 5 C895 5,894 6,893 6 C893 6,893 6,892 7 C892 7,892 8,891 9 C891 10,891 12,890 14 L886 35 C885 36,885 38,885 39 C885 40,884 42,884 43 C884 46,885 48,886 49 C888 50,890 51,892 51 C895 51,897 51,899 50 C900 49,902 47,903 45 C904 43,904 41,905 37 L910 14 C911 12,911 10,911 8 C911 7,911 6,910 6 C910 5,909 5,907 5 L908 3 L922 3 L921 5 C920 5,920 6,919 6 C919 6,919 6,918 7 C918 7,918 8,917 9 C917 10,917 12,916 14 L911 36 C910 41,909 44,907 47 C905 49,903 51,901 52 C898 53,895 54,891 54 C887 54,884 53,881 51 C879 49,878 46,878 42 C878 41,878 39,879 37 C879 36,879 34,880 31 L884 14 C884 12,884 10,884 8 C884 7,884 6,884 6 C883 5,882 5,881 5 L881 5 Z" fill="#212121"/><path d="M934 29 C935 27,935 25,935 24 C935 23,935 22,934 22 C934 21,934 21,933 21 C932 21,931 21,930 22 C930 23,928 24,927 25 L925 23 C927 21,929 19,931 18 C932 18,934 17,936 17 C937 17,938 18,939 19 C940 20,941 21,941 22 C941 24,940 25,940 27 L940 27 C942 23,945 21,947 19 C949 18,951 17,954 17 C956 17,958 18,959 19 C960 20,961 22,961 25 C961 26,960 29,960 31 L957 41 C957 44,956 46,956 47 C956 48,956 49,957 49 C957 50,958 50,958 50 C959 50,960 50,961 49 C961 49,963 48,964 46 L966 48 C964 50,962 52,961 53 C959 54,957 54,955 54 C954 54,953 53,952 52 C951 51,950 50,950 48 C950 46,951 44,951 41 L953 34 C954 32,954 30,954 29 C954 28,954 27,954 26 C954 24,954 23,954 22 C953 21,952 21,951 21 C950 21,949 22,947 22 C946 23,945 24,944 26 C942 28,941 29,940 31 C940 33,939 35,938 37 L935 53 L929 53 L934 29 Z" fill="#212121"/><path d="M988 3 L986 10 L980 10 L981 3 L988 3 Z M976 29 C976 26,977 24,977 23 C977 22,976 21,976 21 C975 20,974 20,973 20 L973 18 L982 17 L984 17 L979 42 C979 44,978 46,978 47 C978 48,978 49,979 49 C979 50,980 50,980 50 C981 50,982 50,983 49 C983 49,985 48,986 46 L988 48 C986 50,984 52,982 53 C981 54,979 54,977 54 C976 54,975 53,974 52 C973 51,972 50,972 48 C972 46,973 44,973 41 L976 29 Z" fill="#212121"/><path d="M1016 46 C1014 49,1012 51,1010 52 C1008 53,1006 54,1004 54 C999 54,997 52,997 46 C997 45,997 43,997 41 L1001 22 L995 22 L996 20 C997 20,998 20,999 20 C1000 20,1000 19,1001 19 C1001 19,1002 18,1002 17 C1003 17,1003 16,1004 15 C1004 14,1005 12,1005 9 L1010 9 L1008 18 L1019 18 L1019 22 L1008 22 L1004 36 C1004 39,1003 41,1003 43 C1003 44,1003 45,1003 45 C1003 49,1004 50,1007 50 C1008 50,1009 50,1010 49 C1011 48,1013 46,1014 44 L1016 46 Z" fill="#212121"/><path d="M1049 27 C1048 25,1048 23,1046 22 C1045 21,1044 20,1041 20 C1039 20,1038 21,1036 22 C1035 23,1035 24,1035 26 C1035 27,1035 27,1035 28 C1035 29,1036 30,1037 30 C1038 31,1039 32,1041 33 C1043 34,1044 35,1045 36 C1046 37,1047 38,1047 38 C1048 39,1048 40,1048 41 C1048 42,1049 43,1049 44 C1049 46,1048 48,1047 49 C1046 51,1044 52,1042 53 C1040 54,1038 54,1035 54 C1033 54,1031 54,1029 54 C1027 53,1025 53,1023 52 L1025 44 L1027 44 C1028 46,1028 48,1029 49 C1031 51,1033 51,1035 51 C1038 51,1039 51,1041 50 C1042 49,1043 47,1043 45 C1043 44,1042 43,1042 42 C1042 41,1041 41,1040 40 C1039 39,1038 38,1036 37 C1034 36,1033 35,1032 34 C1031 33,1030 32,1030 31 C1029 30,1029 28,1029 27 C1029 25,1029 23,1030 22 C1031 20,1033 19,1035 18 C1037 18,1039 17,1041 17 C1044 17,1046 17,1048 18 C1050 18,1051 18,1053 19 L1052 27 L1049 27 Z" fill="#212121"/><path d="M1069 44 C1070 46,1070 48,1070 49 C1070 51,1070 53,1069 54 C1069 55,1068 57,1066 58 C1065 60,1063 61,1061 63 L1059 61 C1060 60,1061 59,1062 58 C1062 57,1062 56,1063 54 C1063 53,1063 52,1063 50 C1063 48,1063 46,1063 44 L1069 44 Z" fill="#212121"/><path d="M1127 42 C1126 44,1126 45,1126 46 C1126 47,1126 47,1126 48 C1126 49,1126 50,1127 50 C1127 51,1128 51,1129 51 L1129 53 L1114 53 L1115 51 C1116 51,1116 51,1117 50 C1117 50,1118 50,1118 49 C1118 48,1119 48,1119 47 C1119 46,1120 44,1120 42 L1123 28 L1103 28 L1100 42 C1100 43,1100 44,1100 45 C1100 46,1100 47,1100 48 C1100 49,1100 50,1100 50 C1101 51,1102 51,1103 51 L1103 53 L1088 53 L1088 51 C1089 51,1090 51,1091 50 C1091 50,1091 50,1092 49 C1092 48,1092 48,1093 47 C1093 46,1093 44,1094 42 L1100 14 C1100 13,1101 12,1101 11 C1101 10,1101 9,1101 8 C1101 7,1101 6,1100 6 C1099 5,1099 5,1097 5 L1098 3 L1112 3 L1112 5 C1111 5,1110 6,1110 6 C1110 6,1109 6,1109 7 C1109 7,1108 8,1108 9 C1108 10,1107 12,1107 14 L1104 25 L1124 25 L1126 14 C1127 13,1127 12,1127 11 C1127 10,1127 9,1127 8 C1127 7,1127 6,1126 6 C1126 5,1125 5,1123 5 L1124 3 L1139 3 L1138 5 C1137 5,1137 6,1136 6 C1136 6,1135 6,1135 7 C1135 7,1134 8,1134 9 C1134 10,1133 12,1133 14 L1127 42 Z" fill="#212121"/><path d="M1172 46 C1169 49,1167 51,1164 52 C1162 53,1159 54,1156 54 C1152 54,1150 53,1148 51 C1146 49,1145 46,1145 42 C1145 39,1145 36,1146 33 C1147 30,1149 27,1151 25 C1152 22,1155 20,1157 19 C1160 18,1163 17,1166 17 C1169 17,1171 18,1173 19 C1174 20,1175 22,1175 25 C1175 29,1173 32,1169 34 C1165 36,1159 37,1152 37 C1151 39,1151 40,1151 42 C1151 45,1152 47,1153 48 C1154 49,1155 50,1158 50 C1160 50,1162 50,1164 49 C1165 48,1167 46,1169 44 L1172 46 Z M1152 34 C1156 34,1159 34,1161 33 C1164 33,1165 32,1167 30 C1168 29,1169 27,1169 25 C1169 23,1168 22,1168 21 C1167 21,1166 20,1165 20 C1162 20,1160 21,1157 24 C1155 26,1153 30,1152 34 L1152 34 Z" fill="#212121"/><path d="M1209 20 L1213 17 L1215 18 L1210 42 C1209 44,1209 46,1209 47 C1209 48,1209 49,1209 49 C1210 50,1210 50,1211 50 C1212 50,1213 50,1213 49 C1214 49,1215 48,1217 46 L1219 48 C1217 50,1215 52,1213 53 C1212 54,1210 54,1208 54 C1207 54,1206 54,1205 53 C1204 52,1203 50,1203 49 C1203 48,1204 46,1204 45 L1204 44 C1201 48,1199 50,1197 52 C1195 53,1193 54,1190 54 C1188 54,1186 53,1184 51 C1183 49,1182 46,1182 42 C1182 38,1183 34,1184 30 C1186 26,1188 23,1191 21 C1194 18,1197 17,1201 17 C1202 17,1204 17,1205 18 C1207 18,1208 19,1209 20 L1209 20 Z M1206 31 C1206 30,1207 29,1207 28 C1207 27,1207 27,1207 26 C1207 24,1206 22,1206 21 C1205 21,1203 20,1201 20 C1199 20,1197 21,1195 23 C1193 25,1191 28,1190 32 C1189 35,1188 39,1188 42 C1188 45,1189 47,1189 48 C1190 49,1191 50,1193 50 C1195 50,1196 49,1198 48 C1199 47,1201 44,1202 42 C1204 39,1205 36,1206 32 L1206 31 Z" fill="#212121"/><path d="M1246 44 C1243 48,1241 50,1239 52 C1237 53,1235 54,1232 54 C1230 54,1228 53,1226 51 C1225 49,1224 46,1224 42 C1224 38,1225 34,1226 30 C1228 26,1230 23,1233 21 C1236 18,1239 17,1243 17 C1244 17,1246 17,1247 18 C1248 18,1249 18,1251 19 L1253 11 C1253 10,1253 9,1253 8 C1253 7,1253 7,1253 6 C1253 5,1253 5,1253 4 C1253 4,1252 4,1252 3 C1251 3,1250 3,1249 3 L1250 1 L1259 1 L1261 1 L1252 42 C1251 44,1251 46,1251 47 C1251 48,1251 49,1251 49 C1252 50,1252 50,1253 50 C1254 50,1255 50,1255 49 C1256 49,1257 48,1259 46 L1261 48 C1259 50,1257 52,1255 53 C1254 54,1252 54,1250 54 C1249 54,1248 54,1247 53 C1246 52,1245 50,1245 49 C1245 48,1246 46,1246 45 L1246 44 Z M1248 31 C1248 29,1249 27,1249 26 C1249 24,1248 22,1247 21 C1247 21,1245 20,1243 20 C1241 20,1239 21,1237 23 C1235 25,1233 28,1232 32 C1231 35,1230 39,1230 42 C1230 45,1231 47,1231 48 C1232 49,1233 50,1235 50 C1236 50,1236 50,1237 50 C1238 49,1239 49,1240 48 C1241 47,1241 46,1242 45 C1243 44,1244 43,1244 42 C1245 41,1246 40,1246 38 C1247 36,1247 35,1248 32 L1248 31 Z" fill="#212121"/><path d="M1291 27 C1290 25,1290 23,1288 22 C1287 21,1286 20,1283 20 C1281 20,1280 21,1278 22 C1277 23,1277 24,1277 26 C1277 27,1277 27,1277 28 C1277 29,1278 30,1279 30 C1280 31,1281 32,1283 33 C1285 34,1286 35,1287 36 C1288 37,1289 38,1289 38 C1290 39,1290 40,1290 41 C1290 42,1291 43,1291 44 C1291 46,1290 48,1289 49 C1288 51,1286 52,1284 53 C1282 54,1280 54,1277 54 C1275 54,1273 54,1271 54 C1269 53,1267 53,1265 52 L1267 44 L1269 44 C1270 46,1270 48,1271 49 C1273 51,1275 51,1277 51 C1280 51,1281 51,1283 50 C1284 49,1285 47,1285 45 C1285 44,1284 43,1284 42 C1284 41,1283 41,1282 40 C1281 39,1280 38,1278 37 C1276 36,1275 35,1274 34 C1273 33,1272 32,1272 31 C1271 30,1271 28,1271 27 C1271 25,1271 23,1272 22 C1273 20,1275 19,1277 18 C1279 18,1281 17,1283 17 C1286 17,1288 17,1290 18 C1292 18,1293 18,1295 19 L1294 27 L1291 27 Z" fill="#212121"/><path d="M1311 44 C1312 46,1312 48,1312 49 C1312 51,1312 53,1311 54 C1311 55,1310 57,1308 58 C1307 60,1305 61,1303 63 L1301 61 C1302 60,1303 59,1304 58 C1304 57,1304 56,1305 54 C1305 53,1305 52,1305 50 C1305 48,1305 46,1305 44 L1311 44 Z" fill="#212121"/><path d="M1347 3 L1347 5 C1346 5,1345 6,1344 6 C1344 7,1343 7,1343 8 C1343 9,1343 10,1343 12 C1343 12,1343 13,1343 15 L1344 42 L1344 45 L1344 45 L1360 16 C1361 15,1362 13,1362 12 C1362 11,1363 10,1363 9 C1363 7,1361 6,1359 5 L1359 3 L1374 3 L1374 5 C1373 5,1373 5,1372 6 C1371 6,1371 7,1370 8 C1369 9,1368 11,1367 13 L1343 54 L1338 54 L1336 13 C1336 12,1336 10,1335 9 C1335 8,1335 7,1334 6 C1334 6,1333 5,1331 5 L1332 3 L1347 3 Z" fill="#212121"/><path d="M1406 46 C1403 49,1401 51,1398 52 C1396 53,1393 54,1390 54 C1386 54,1384 53,1382 51 C1380 49,1379 46,1379 42 C1379 39,1379 36,1380 33 C1381 30,1383 27,1385 25 C1386 22,1389 20,1391 19 C1394 18,1397 17,1400 17 C1403 17,1405 18,1407 19 C1408 20,1409 22,1409 25 C1409 29,1407 32,1403 34 C1399 36,1393 37,1386 37 C1385 39,1385 40,1385 42 C1385 45,1386 47,1387 48 C1388 49,1389 50,1392 50 C1394 50,1396 50,1398 49 C1399 48,1401 46,1403 44 L1406 46 Z M1386 34 C1390 34,1393 34,1395 33 C1398 33,1399 32,1401 30 C1402 29,1403 27,1403 25 C1403 23,1402 22,1402 21 C1401 21,1400 20,1399 20 C1396 20,1394 21,1391 24 C1389 26,1387 30,1386 34 L1386 34 Z" fill="#212121"/><path d="M1440 27 C1440 25,1439 24,1439 23 C1439 22,1438 21,1438 21 C1437 20,1437 20,1435 20 C1433 20,1431 21,1429 23 C1427 25,1425 28,1424 32 C1423 36,1422 39,1422 42 C1422 45,1423 47,1424 48 C1425 49,1426 50,1428 50 C1430 50,1432 50,1434 49 C1435 48,1437 47,1439 45 L1441 47 C1439 50,1437 51,1434 52 C1432 54,1429 54,1427 54 C1423 54,1420 53,1419 51 C1417 49,1416 46,1416 41 C1416 39,1416 36,1417 33 C1418 30,1420 27,1422 24 C1423 22,1426 20,1428 19 C1431 18,1433 17,1436 17 C1440 17,1442 17,1445 18 L1443 27 L1440 27 Z" fill="#212121"/><path d="M1472 46 C1470 49,1468 51,1466 52 C1464 53,1462 54,1460 54 C1455 54,1453 52,1453 46 C1453 45,1453 43,1453 41 L1457 22 L1451 22 L1452 20 C1453 20,1454 20,1455 20 C1456 20,1456 19,1457 19 C1457 19,1458 18,1458 17 C1459 17,1459 16,1460 15 C1460 14,1461 12,1461 9 L1466 9 L1464 18 L1475 18 L1475 22 L1464 22 L1460 36 C1460 39,1459 41,1459 43 C1459 44,1459 45,1459 45 C1459 49,1460 50,1463 50 C1464 50,1465 50,1466 49 C1467 48,1469 46,1470 44 L1472 46 Z" fill="#212121"/><path d="M1493 54 C1489 54,1486 53,1484 51 C1482 49,1481 46,1481 41 C1481 40,1481 37,1482 35 C1483 31,1484 28,1486 25 C1488 23,1490 21,1492 19 C1495 18,1498 17,1501 17 C1505 17,1508 18,1510 20 C1512 23,1513 26,1513 30 C1513 33,1512 36,1511 39 C1510 42,1509 45,1507 47 C1506 50,1504 51,1501 52 C1499 54,1496 54,1493 54 L1493 54 Z M1487 43 C1487 46,1488 48,1489 49 C1490 50,1492 51,1494 51 C1497 51,1499 50,1501 48 C1502 46,1504 43,1505 39 C1506 35,1507 31,1507 28 C1507 26,1506 24,1505 22 C1504 21,1502 20,1500 20 C1498 20,1495 21,1494 23 C1492 26,1490 29,1489 33 C1488 37,1487 40,1487 43 L1487 43 Z" fill="#212121"/><path d="M1533 27 C1536 23,1538 21,1540 19 C1542 18,1545 17,1547 17 C1549 17,1550 17,1551 17 L1550 26 L1546 26 C1546 25,1545 24,1545 24 C1545 23,1545 23,1544 22 C1544 22,1543 22,1543 22 C1542 22,1541 22,1540 23 C1539 24,1538 25,1536 27 C1535 28,1534 30,1533 31 C1533 33,1532 35,1531 37 L1528 53 L1522 53 L1527 29 C1527 28,1528 27,1528 26 C1528 25,1528 25,1528 24 C1528 23,1528 22,1527 22 C1527 21,1527 21,1526 21 C1525 21,1524 21,1523 22 C1523 23,1521 24,1520 25 L1518 23 C1520 21,1522 19,1524 18 C1525 18,1527 17,1529 17 C1530 17,1531 18,1532 19 C1533 20,1533 21,1533 22 C1533 24,1533 25,1533 26 L1533 27 Z" fill="#212121"/><path d="M1557 67 L1567 67 L1567 3 L1557 3 L1557 0 L1573 0 L1573 70 L1557 70 L1557 67 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">Multiplying the last two dimensions in this form is also not quite what we would like to get. However, if we swap the first and second dimensions, then we can multiply the last two dimensions to get the result we are looking for.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:410px;height:17px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 1640 68"><path d="M24 70 L8 70 L8 0 L24 0 L24 3 L14 3 L14 67 L24 67 L24 70 Z" fill="#212121"/><path d="M35 5 L35 3 L50 3 L49 5 C49 5,48 6,47 6 C47 6,47 6,46 7 C46 7,46 8,45 9 C45 10,45 12,44 14 L40 35 C39 36,39 38,39 39 C39 40,38 42,38 43 C38 46,39 48,40 49 C42 50,44 51,46 51 C49 51,51 51,53 50 C54 49,56 47,57 45 C58 43,58 41,59 37 L64 14 C65 12,65 10,65 8 C65 7,65 6,64 6 C64 5,63 5,61 5 L62 3 L76 3 L75 5 C74 5,74 6,73 6 C73 6,73 6,72 7 C72 7,72 8,71 9 C71 10,71 12,70 14 L65 36 C64 41,63 44,61 47 C59 49,57 51,55 52 C52 53,49 54,45 54 C41 54,38 53,35 51 C33 49,32 46,32 42 C32 41,32 39,33 37 C33 36,33 34,34 31 L38 14 C38 12,38 10,38 8 C38 7,38 6,38 6 C37 5,36 5,35 5 L35 5 Z" fill="#212121"/><path d="M88 29 C89 27,89 25,89 24 C89 23,89 22,88 22 C88 21,88 21,87 21 C86 21,85 21,84 22 C84 23,82 24,81 25 L79 23 C81 21,83 19,85 18 C86 18,88 17,90 17 C91 17,92 18,93 19 C94 20,95 21,95 22 C95 24,94 25,94 27 L94 27 C96 23,99 21,101 19 C103 18,105 17,108 17 C110 17,112 18,113 19 C114 20,115 22,115 25 C115 26,114 29,114 31 L111 41 C111 44,110 46,110 47 C110 48,110 49,111 49 C111 50,112 50,112 50 C113 50,114 50,115 49 C115 49,117 48,118 46 L120 48 C118 50,116 52,115 53 C113 54,111 54,109 54 C108 54,107 53,106 52 C105 51,104 50,104 48 C104 46,105 44,105 41 L107 34 C108 32,108 30,108 29 C108 28,108 27,108 26 C108 24,108 23,108 22 C107 21,106 21,105 21 C104 21,103 22,101 22 C100 23,99 24,98 26 C96 28,95 29,94 31 C94 33,93 35,92 37 L89 53 L83 53 L88 29 Z" fill="#212121"/><path d="M142 3 L140 10 L134 10 L135 3 L142 3 Z M130 29 C130 26,131 24,131 23 C131 22,130 21,130 21 C129 20,128 20,127 20 L127 18 L136 17 L138 17 L133 42 C133 44,132 46,132 47 C132 48,132 49,133 49 C133 50,134 50,134 50 C135 50,136 50,137 49 C137 49,139 48,140 46 L142 48 C140 50,138 52,136 53 C135 54,133 54,131 54 C130 54,129 53,128 52 C127 51,126 50,126 48 C126 46,127 44,127 41 L130 29 Z" fill="#212121"/><path d="M170 46 C168 49,166 51,164 52 C162 53,160 54,158 54 C153 54,151 52,151 46 C151 45,151 43,151 41 L155 22 L149 22 L150 20 C151 20,152 20,153 20 C154 20,154 19,155 19 C155 19,156 18,156 17 C157 17,157 16,158 15 C158 14,159 12,159 9 L164 9 L162 18 L173 18 L173 22 L162 22 L158 36 C158 39,157 41,157 43 C157 44,157 45,157 45 C157 49,158 50,161 50 C162 50,163 50,164 49 C165 48,167 46,168 44 L170 46 Z" fill="#212121"/><path d="M203 27 C202 25,202 23,200 22 C199 21,198 20,195 20 C193 20,192 21,190 22 C189 23,189 24,189 26 C189 27,189 27,189 28 C189 29,190 30,191 30 C192 31,193 32,195 33 C197 34,198 35,199 36 C200 37,201 38,201 38 C202 39,202 40,202 41 C202 42,203 43,203 44 C203 46,202 48,201 49 C200 51,198 52,196 53 C194 54,192 54,189 54 C187 54,185 54,183 54 C181 53,179 53,177 52 L179 44 L181 44 C182 46,182 48,183 49 C185 51,187 51,189 51 C192 51,193 51,195 50 C196 49,197 47,197 45 C197 44,196 43,196 42 C196 41,195 41,194 40 C193 39,192 38,190 37 C188 36,187 35,186 34 C185 33,184 32,184 31 C183 30,183 28,183 27 C183 25,183 23,184 22 C185 20,187 19,189 18 C191 18,193 17,195 17 C198 17,200 17,202 18 C204 18,205 18,207 19 L206 27 L203 27 Z" fill="#212121"/><path d="M223 44 C224 46,224 48,224 49 C224 51,224 53,223 54 C223 55,222 57,220 58 C219 60,217 61,215 63 L213 61 C214 60,215 59,216 58 C216 57,216 56,217 54 C217 53,217 52,217 50 C217 48,217 46,217 44 L223 44 Z" fill="#212121"/><path d="M281 42 C280 44,280 45,280 46 C280 47,280 47,280 48 C280 49,280 50,281 50 C281 51,282 51,283 51 L283 53 L268 53 L269 51 C270 51,270 51,271 50 C271 50,272 50,272 49 C272 48,273 48,273 47 C273 46,274 44,274 42 L277 28 L257 28 L254 42 C254 43,254 44,254 45 C254 46,254 47,254 48 C254 49,254 50,254 50 C255 51,256 51,257 51 L257 53 L242 53 L242 51 C243 51,244 51,245 50 C245 50,245 50,246 49 C246 48,246 48,247 47 C247 46,247 44,248 42 L254 14 C254 13,255 12,255 11 C255 10,255 9,255 8 C255 7,255 6,254 6 C253 5,253 5,251 5 L252 3 L266 3 L266 5 C265 5,264 6,264 6 C264 6,263 6,263 7 C263 7,262 8,262 9 C262 10,261 12,261 14 L258 25 L278 25 L280 14 C281 13,281 12,281 11 C281 10,281 9,281 8 C281 7,281 6,280 6 C280 5,279 5,277 5 L278 3 L293 3 L292 5 C291 5,291 6,290 6 C290 6,289 6,289 7 C289 7,288 8,288 9 C288 10,287 12,287 14 L281 42 Z" fill="#212121"/><path d="M326 46 C323 49,321 51,318 52 C316 53,313 54,310 54 C306 54,304 53,302 51 C300 49,299 46,299 42 C299 39,299 36,300 33 C301 30,303 27,305 25 C306 22,309 20,311 19 C314 18,317 17,320 17 C323 17,325 18,327 19 C328 20,329 22,329 25 C329 29,327 32,323 34 C319 36,313 37,306 37 C305 39,305 40,305 42 C305 45,306 47,307 48 C308 49,309 50,312 50 C314 50,316 50,318 49 C319 48,321 46,323 44 L326 46 Z M306 34 C310 34,313 34,315 33 C318 33,319 32,321 30 C322 29,323 27,323 25 C323 23,322 22,322 21 C321 21,320 20,319 20 C316 20,314 21,311 24 C309 26,307 30,306 34 L306 34 Z" fill="#212121"/><path d="M363 20 L367 17 L369 18 L364 42 C363 44,363 46,363 47 C363 48,363 49,363 49 C364 50,364 50,365 50 C366 50,367 50,367 49 C368 49,369 48,371 46 L373 48 C371 50,369 52,367 53 C366 54,364 54,362 54 C361 54,360 54,359 53 C358 52,357 50,357 49 C357 48,358 46,358 45 L358 44 C355 48,353 50,351 52 C349 53,347 54,344 54 C342 54,340 53,338 51 C337 49,336 46,336 42 C336 38,337 34,338 30 C340 26,342 23,345 21 C348 18,351 17,355 17 C356 17,358 17,359 18 C361 18,362 19,363 20 L363 20 Z M360 31 C360 30,361 29,361 28 C361 27,361 27,361 26 C361 24,360 22,360 21 C359 21,357 20,355 20 C353 20,351 21,349 23 C347 25,345 28,344 32 C343 35,342 39,342 42 C342 45,343 47,343 48 C344 49,345 50,347 50 C349 50,350 49,352 48 C353 47,355 44,356 42 C358 39,359 36,360 32 L360 31 Z" fill="#212121"/><path d="M400 44 C397 48,395 50,393 52 C391 53,389 54,386 54 C384 54,382 53,380 51 C379 49,378 46,378 42 C378 38,379 34,380 30 C382 26,384 23,387 21 C390 18,393 17,397 17 C398 17,400 17,401 18 C402 18,403 18,405 19 L407 11 C407 10,407 9,407 8 C407 7,407 7,407 6 C407 5,407 5,407 4 C407 4,406 4,406 3 C405 3,404 3,403 3 L404 1 L413 1 L415 1 L406 42 C405 44,405 46,405 47 C405 48,405 49,405 49 C406 50,406 50,407 50 C408 50,409 50,409 49 C410 49,411 48,413 46 L415 48 C413 50,411 52,409 53 C408 54,406 54,404 54 C403 54,402 54,401 53 C400 52,399 50,399 49 C399 48,400 46,400 45 L400 44 Z M402 31 C402 29,403 27,403 26 C403 24,402 22,401 21 C401 21,399 20,397 20 C395 20,393 21,391 23 C389 25,387 28,386 32 C385 35,384 39,384 42 C384 45,385 47,385 48 C386 49,387 50,389 50 C390 50,390 50,391 50 C392 49,393 49,394 48 C395 47,395 46,396 45 C397 44,398 43,398 42 C399 41,400 40,400 38 C401 36,401 35,402 32 L402 31 Z" fill="#212121"/><path d="M445 27 C444 25,444 23,442 22 C441 21,440 20,437 20 C435 20,434 21,432 22 C431 23,431 24,431 26 C431 27,431 27,431 28 C431 29,432 30,433 30 C434 31,435 32,437 33 C439 34,440 35,441 36 C442 37,443 38,443 38 C444 39,444 40,444 41 C444 42,445 43,445 44 C445 46,444 48,443 49 C442 51,440 52,438 53 C436 54,434 54,431 54 C429 54,427 54,425 54 C423 53,421 53,419 52 L421 44 L423 44 C424 46,424 48,425 49 C427 51,429 51,431 51 C434 51,435 51,437 50 C438 49,439 47,439 45 C439 44,438 43,438 42 C438 41,437 41,436 40 C435 39,434 38,432 37 C430 36,429 35,428 34 C427 33,426 32,426 31 C425 30,425 28,425 27 C425 25,425 23,426 22 C427 20,429 19,431 18 C433 18,435 17,437 17 C440 17,442 17,444 18 C446 18,447 18,449 19 L448 27 L445 27 Z" fill="#212121"/><path d="M465 44 C466 46,466 48,466 49 C466 51,466 53,465 54 C465 55,464 57,462 58 C461 60,459 61,457 63 L455 61 C456 60,457 59,458 58 C458 57,458 56,459 54 C459 53,459 52,459 50 C459 48,459 46,459 44 L465 44 Z" fill="#212121"/><path d="M501 3 L501 5 C500 5,499 6,498 6 C498 7,497 7,497 8 C497 9,497 10,497 12 C497 12,497 13,497 15 L498 42 L498 45 L498 45 L514 16 C515 15,516 13,516 12 C516 11,517 10,517 9 C517 7,515 6,513 5 L513 3 L528 3 L528 5 C527 5,527 5,526 6 C525 6,525 7,524 8 C523 9,522 11,521 13 L497 54 L492 54 L490 13 C490 12,490 10,489 9 C489 8,489 7,488 6 C488 6,487 5,485 5 L486 3 L501 3 Z" fill="#212121"/><path d="M560 46 C557 49,555 51,552 52 C550 53,547 54,544 54 C540 54,538 53,536 51 C534 49,533 46,533 42 C533 39,533 36,534 33 C535 30,537 27,539 25 C540 22,543 20,545 19 C548 18,551 17,554 17 C557 17,559 18,561 19 C562 20,563 22,563 25 C563 29,561 32,557 34 C553 36,547 37,540 37 C539 39,539 40,539 42 C539 45,540 47,541 48 C542 49,543 50,546 50 C548 50,550 50,552 49 C553 48,555 46,557 44 L560 46 Z M540 34 C544 34,547 34,549 33 C552 33,553 32,555 30 C556 29,557 27,557 25 C557 23,556 22,556 21 C555 21,554 20,553 20 C550 20,548 21,545 24 C543 26,541 30,540 34 L540 34 Z" fill="#212121"/><path d="M594 27 C594 25,593 24,593 23 C593 22,592 21,592 21 C591 20,591 20,589 20 C587 20,585 21,583 23 C581 25,579 28,578 32 C577 36,576 39,576 42 C576 45,577 47,578 48 C579 49,580 50,582 50 C584 50,586 50,588 49 C589 48,591 47,593 45 L595 47 C593 50,591 51,588 52 C586 54,583 54,581 54 C577 54,574 53,573 51 C571 49,570 46,570 41 C570 39,570 36,571 33 C572 30,574 27,576 24 C577 22,580 20,582 19 C585 18,587 17,590 17 C594 17,596 17,599 18 L597 27 L594 27 Z" fill="#212121"/><path d="M626 46 C624 49,622 51,620 52 C618 53,616 54,614 54 C609 54,607 52,607 46 C607 45,607 43,607 41 L611 22 L605 22 L606 20 C607 20,608 20,609 20 C610 20,610 19,611 19 C611 19,612 18,612 17 C613 17,613 16,614 15 C614 14,615 12,615 9 L620 9 L618 18 L629 18 L629 22 L618 22 L614 36 C614 39,613 41,613 43 C613 44,613 45,613 45 C613 49,614 50,617 50 C618 50,619 50,620 49 C621 48,623 46,624 44 L626 46 Z" fill="#212121"/><path d="M647 54 C643 54,640 53,638 51 C636 49,635 46,635 41 C635 40,635 37,636 35 C637 31,638 28,640 25 C642 23,644 21,646 19 C649 18,652 17,655 17 C659 17,662 18,664 20 C666 23,667 26,667 30 C667 33,666 36,665 39 C664 42,663 45,661 47 C660 50,658 51,655 52 C653 54,650 54,647 54 L647 54 Z M641 43 C641 46,642 48,643 49 C644 50,646 51,648 51 C651 51,653 50,655 48 C656 46,658 43,659 39 C660 35,661 31,661 28 C661 26,660 24,659 22 C658 21,656 20,654 20 C652 20,649 21,648 23 C646 26,644 29,643 33 C642 37,641 40,641 43 L641 43 Z" fill="#212121"/><path d="M687 27 C690 23,692 21,694 19 C696 18,699 17,701 17 C703 17,704 17,705 17 L704 26 L700 26 C700 25,699 24,699 24 C699 23,699 23,698 22 C698 22,697 22,697 22 C696 22,695 22,694 23 C693 24,692 25,690 27 C689 28,688 30,687 31 C687 33,686 35,685 37 L682 53 L676 53 L681 29 C681 28,682 27,682 26 C682 25,682 25,682 24 C682 23,682 22,681 22 C681 21,681 21,680 21 C679 21,678 21,677 22 C677 23,675 24,674 25 L672 23 C674 21,676 19,678 18 C679 18,681 17,683 17 C684 17,685 18,686 19 C687 20,687 21,687 22 C687 24,687 25,687 26 L687 27 Z" fill="#212121"/><path d="M711 67 L721 67 L721 3 L711 3 L711 0 L727 0 L727 70 L711 70 L711 67 Z" fill="#212121"/><path d="M761 27 L761 22 L807 22 L807 27 L761 27 Z M761 42 L761 37 L807 37 L807 42 L761 42 Z" fill="#212121"/><path d="M817 55 L817 49 L857 32 L817 14 L817 8 L863 29 L863 34 L817 55 Z" fill="#212121"/><path d="M913 70 L897 70 L897 0 L913 0 L913 3 L903 3 L903 67 L913 67 L913 70 Z" fill="#212121"/><path d="M956 42 C955 44,955 45,955 46 C955 47,955 47,955 48 C955 49,955 50,956 50 C956 51,957 51,958 51 L958 53 L943 53 L944 51 C945 51,945 51,946 50 C946 50,947 50,947 49 C947 48,948 48,948 47 C948 46,949 44,949 42 L952 28 L932 28 L929 42 C929 43,929 44,929 45 C929 46,929 47,929 48 C929 49,929 50,929 50 C930 51,931 51,932 51 L932 53 L917 53 L917 51 C918 51,919 51,920 50 C920 50,920 50,921 49 C921 48,921 48,922 47 C922 46,922 44,923 42 L929 14 C929 13,930 12,930 11 C930 10,930 9,930 8 C930 7,930 6,929 6 C928 5,928 5,926 5 L927 3 L941 3 L941 5 C940 5,939 6,939 6 C939 6,938 6,938 7 C938 7,937 8,937 9 C937 10,936 12,936 14 L933 25 L953 25 L955 14 C956 13,956 12,956 11 C956 10,956 9,956 8 C956 7,956 6,955 6 C955 5,954 5,952 5 L953 3 L968 3 L967 5 C966 5,966 6,965 6 C965 6,964 6,964 7 C964 7,963 8,963 9 C963 10,962 12,962 14 L956 42 Z" fill="#212121"/><path d="M1001 46 C998 49,996 51,993 52 C991 53,988 54,985 54 C981 54,979 53,977 51 C975 49,974 46,974 42 C974 39,974 36,975 33 C976 30,978 27,980 25 C981 22,984 20,986 19 C989 18,992 17,995 17 C998 17,1000 18,1002 19 C1003 20,1004 22,1004 25 C1004 29,1002 32,998 34 C994 36,988 37,981 37 C980 39,980 40,980 42 C980 45,981 47,982 48 C983 49,984 50,987 50 C989 50,991 50,993 49 C994 48,996 46,998 44 L1001 46 Z M981 34 C985 34,988 34,990 33 C993 33,994 32,996 30 C997 29,998 27,998 25 C998 23,997 22,997 21 C996 21,995 20,994 20 C991 20,989 21,986 24 C984 26,982 30,981 34 L981 34 Z" fill="#212121"/><path d="M1038 20 L1042 17 L1044 18 L1039 42 C1038 44,1038 46,1038 47 C1038 48,1038 49,1038 49 C1039 50,1039 50,1040 50 C1041 50,1042 50,1042 49 C1043 49,1044 48,1046 46 L1048 48 C1046 50,1044 52,1042 53 C1041 54,1039 54,1037 54 C1036 54,1035 54,1034 53 C1033 52,1032 50,1032 49 C1032 48,1033 46,1033 45 L1033 44 C1030 48,1028 50,1026 52 C1024 53,1022 54,1019 54 C1017 54,1015 53,1013 51 C1012 49,1011 46,1011 42 C1011 38,1012 34,1013 30 C1015 26,1017 23,1020 21 C1023 18,1026 17,1030 17 C1031 17,1033 17,1034 18 C1036 18,1037 19,1038 20 L1038 20 Z M1035 31 C1035 30,1036 29,1036 28 C1036 27,1036 27,1036 26 C1036 24,1035 22,1035 21 C1034 21,1032 20,1030 20 C1028 20,1026 21,1024 23 C1022 25,1020 28,1019 32 C1018 35,1017 39,1017 42 C1017 45,1018 47,1018 48 C1019 49,1020 50,1022 50 C1024 50,1025 49,1027 48 C1028 47,1030 44,1031 42 C1033 39,1034 36,1035 32 L1035 31 Z" fill="#212121"/><path d="M1075 44 C1072 48,1070 50,1068 52 C1066 53,1064 54,1061 54 C1059 54,1057 53,1055 51 C1054 49,1053 46,1053 42 C1053 38,1054 34,1055 30 C1057 26,1059 23,1062 21 C1065 18,1068 17,1072 17 C1073 17,1075 17,1076 18 C1077 18,1078 18,1080 19 L1082 11 C1082 10,1082 9,1082 8 C1082 7,1082 7,1082 6 C1082 5,1082 5,1082 4 C1082 4,1081 4,1081 3 C1080 3,1079 3,1078 3 L1079 1 L1088 1 L1090 1 L1081 42 C1080 44,1080 46,1080 47 C1080 48,1080 49,1080 49 C1081 50,1081 50,1082 50 C1083 50,1084 50,1084 49 C1085 49,1086 48,1088 46 L1090 48 C1088 50,1086 52,1084 53 C1083 54,1081 54,1079 54 C1078 54,1077 54,1076 53 C1075 52,1074 50,1074 49 C1074 48,1075 46,1075 45 L1075 44 Z M1077 31 C1077 29,1078 27,1078 26 C1078 24,1077 22,1076 21 C1076 21,1074 20,1072 20 C1070 20,1068 21,1066 23 C1064 25,1062 28,1061 32 C1060 35,1059 39,1059 42 C1059 45,1060 47,1060 48 C1061 49,1062 50,1064 50 C1065 50,1065 50,1066 50 C1067 49,1068 49,1069 48 C1070 47,1070 46,1071 45 C1072 44,1073 43,1073 42 C1074 41,1075 40,1075 38 C1076 36,1076 35,1077 32 L1077 31 Z" fill="#212121"/><path d="M1120 27 C1119 25,1119 23,1117 22 C1116 21,1115 20,1112 20 C1110 20,1109 21,1107 22 C1106 23,1106 24,1106 26 C1106 27,1106 27,1106 28 C1106 29,1107 30,1108 30 C1109 31,1110 32,1112 33 C1114 34,1115 35,1116 36 C1117 37,1118 38,1118 38 C1119 39,1119 40,1119 41 C1119 42,1120 43,1120 44 C1120 46,1119 48,1118 49 C1117 51,1115 52,1113 53 C1111 54,1109 54,1106 54 C1104 54,1102 54,1100 54 C1098 53,1096 53,1094 52 L1096 44 L1098 44 C1099 46,1099 48,1100 49 C1102 51,1104 51,1106 51 C1109 51,1110 51,1112 50 C1113 49,1114 47,1114 45 C1114 44,1113 43,1113 42 C1113 41,1112 41,1111 40 C1110 39,1109 38,1107 37 C1105 36,1104 35,1103 34 C1102 33,1101 32,1101 31 C1100 30,1100 28,1100 27 C1100 25,1100 23,1101 22 C1102 20,1104 19,1106 18 C1108 18,1110 17,1112 17 C1115 17,1117 17,1119 18 C1121 18,1122 18,1124 19 L1123 27 L1120 27 Z" fill="#212121"/><path d="M1140 44 C1141 46,1141 48,1141 49 C1141 51,1141 53,1140 54 C1140 55,1139 57,1137 58 C1136 60,1134 61,1132 63 L1130 61 C1131 60,1132 59,1133 58 C1133 57,1133 56,1134 54 C1134 53,1134 52,1134 50 C1134 48,1134 46,1134 44 L1140 44 Z" fill="#212121"/><path d="M1166 5 L1166 3 L1181 3 L1180 5 C1180 5,1179 6,1178 6 C1178 6,1178 6,1177 7 C1177 7,1177 8,1176 9 C1176 10,1176 12,1175 14 L1171 35 C1170 36,1170 38,1170 39 C1170 40,1169 42,1169 43 C1169 46,1170 48,1171 49 C1173 50,1175 51,1177 51 C1180 51,1182 51,1184 50 C1185 49,1187 47,1188 45 C1189 43,1189 41,1190 37 L1195 14 C1196 12,1196 10,1196 8 C1196 7,1196 6,1195 6 C1195 5,1194 5,1192 5 L1193 3 L1207 3 L1206 5 C1205 5,1205 6,1204 6 C1204 6,1204 6,1203 7 C1203 7,1203 8,1202 9 C1202 10,1202 12,1201 14 L1196 36 C1195 41,1194 44,1192 47 C1190 49,1188 51,1186 52 C1183 53,1180 54,1176 54 C1172 54,1169 53,1166 51 C1164 49,1163 46,1163 42 C1163 41,1163 39,1164 37 C1164 36,1164 34,1165 31 L1169 14 C1169 12,1169 10,1169 8 C1169 7,1169 6,1169 6 C1168 5,1167 5,1166 5 L1166 5 Z" fill="#212121"/><path d="M1219 29 C1220 27,1220 25,1220 24 C1220 23,1220 22,1219 22 C1219 21,1219 21,1218 21 C1217 21,1216 21,1215 22 C1215 23,1213 24,1212 25 L1210 23 C1212 21,1214 19,1216 18 C1217 18,1219 17,1221 17 C1222 17,1223 18,1224 19 C1225 20,1226 21,1226 22 C1226 24,1225 25,1225 27 L1225 27 C1227 23,1230 21,1232 19 C1234 18,1236 17,1239 17 C1241 17,1243 18,1244 19 C1245 20,1246 22,1246 25 C1246 26,1245 29,1245 31 L1242 41 C1242 44,1241 46,1241 47 C1241 48,1241 49,1242 49 C1242 50,1243 50,1243 50 C1244 50,1245 50,1246 49 C1246 49,1248 48,1249 46 L1251 48 C1249 50,1247 52,1246 53 C1244 54,1242 54,1240 54 C1239 54,1238 53,1237 52 C1236 51,1235 50,1235 48 C1235 46,1236 44,1236 41 L1238 34 C1239 32,1239 30,1239 29 C1239 28,1239 27,1239 26 C1239 24,1239 23,1239 22 C1238 21,1237 21,1236 21 C1235 21,1234 22,1232 22 C1231 23,1230 24,1229 26 C1227 28,1226 29,1225 31 C1225 33,1224 35,1223 37 L1220 53 L1214 53 L1219 29 Z" fill="#212121"/><path d="M1273 3 L1271 10 L1265 10 L1266 3 L1273 3 Z M1261 29 C1261 26,1262 24,1262 23 C1262 22,1261 21,1261 21 C1260 20,1259 20,1258 20 L1258 18 L1267 17 L1269 17 L1264 42 C1264 44,1263 46,1263 47 C1263 48,1263 49,1264 49 C1264 50,1265 50,1265 50 C1266 50,1267 50,1268 49 C1268 49,1270 48,1271 46 L1273 48 C1271 50,1269 52,1267 53 C1266 54,1264 54,1262 54 C1261 54,1260 53,1259 52 C1258 51,1257 50,1257 48 C1257 46,1258 44,1258 41 L1261 29 Z" fill="#212121"/><path d="M1301 46 C1299 49,1297 51,1295 52 C1293 53,1291 54,1289 54 C1284 54,1282 52,1282 46 C1282 45,1282 43,1282 41 L1286 22 L1280 22 L1281 20 C1282 20,1283 20,1284 20 C1285 20,1285 19,1286 19 C1286 19,1287 18,1287 17 C1288 17,1288 16,1289 15 C1289 14,1290 12,1290 9 L1295 9 L1293 18 L1304 18 L1304 22 L1293 22 L1289 36 C1289 39,1288 41,1288 43 C1288 44,1288 45,1288 45 C1288 49,1289 50,1292 50 C1293 50,1294 50,1295 49 C1296 48,1298 46,1299 44 L1301 46 Z" fill="#212121"/><path d="M1334 27 C1333 25,1333 23,1331 22 C1330 21,1329 20,1326 20 C1324 20,1323 21,1321 22 C1320 23,1320 24,1320 26 C1320 27,1320 27,1320 28 C1320 29,1321 30,1322 30 C1323 31,1324 32,1326 33 C1328 34,1329 35,1330 36 C1331 37,1332 38,1332 38 C1333 39,1333 40,1333 41 C1333 42,1334 43,1334 44 C1334 46,1333 48,1332 49 C1331 51,1329 52,1327 53 C1325 54,1323 54,1320 54 C1318 54,1316 54,1314 54 C1312 53,1310 53,1308 52 L1310 44 L1312 44 C1313 46,1313 48,1314 49 C1316 51,1318 51,1320 51 C1323 51,1324 51,1326 50 C1327 49,1328 47,1328 45 C1328 44,1327 43,1327 42 C1327 41,1326 41,1325 40 C1324 39,1323 38,1321 37 C1319 36,1318 35,1317 34 C1316 33,1315 32,1315 31 C1314 30,1314 28,1314 27 C1314 25,1314 23,1315 22 C1316 20,1318 19,1320 18 C1322 18,1324 17,1326 17 C1329 17,1331 17,1333 18 C1335 18,1336 18,1338 19 L1337 27 L1334 27 Z" fill="#212121"/><path d="M1354 44 C1355 46,1355 48,1355 49 C1355 51,1355 53,1354 54 C1354 55,1353 57,1351 58 C1350 60,1348 61,1346 63 L1344 61 C1345 60,1346 59,1347 58 C1347 57,1347 56,1348 54 C1348 53,1348 52,1348 50 C1348 48,1348 46,1348 44 L1354 44 Z" fill="#212121"/><path d="M1390 3 L1390 5 C1389 5,1388 6,1387 6 C1387 7,1386 7,1386 8 C1386 9,1386 10,1386 12 C1386 12,1386 13,1386 15 L1387 42 L1387 45 L1387 45 L1403 16 C1404 15,1405 13,1405 12 C1405 11,1406 10,1406 9 C1406 7,1404 6,1402 5 L1402 3 L1417 3 L1417 5 C1416 5,1416 5,1415 6 C1414 6,1414 7,1413 8 C1412 9,1411 11,1410 13 L1386 54 L1381 54 L1379 13 C1379 12,1379 10,1378 9 C1378 8,1378 7,1377 6 C1377 6,1376 5,1374 5 L1375 3 L1390 3 Z" fill="#212121"/><path d="M1449 46 C1446 49,1444 51,1441 52 C1439 53,1436 54,1433 54 C1429 54,1427 53,1425 51 C1423 49,1422 46,1422 42 C1422 39,1422 36,1423 33 C1424 30,1426 27,1428 25 C1429 22,1432 20,1434 19 C1437 18,1440 17,1443 17 C1446 17,1448 18,1450 19 C1451 20,1452 22,1452 25 C1452 29,1450 32,1446 34 C1442 36,1436 37,1429 37 C1428 39,1428 40,1428 42 C1428 45,1429 47,1430 48 C1431 49,1432 50,1435 50 C1437 50,1439 50,1441 49 C1442 48,1444 46,1446 44 L1449 46 Z M1429 34 C1433 34,1436 34,1438 33 C1441 33,1442 32,1444 30 C1445 29,1446 27,1446 25 C1446 23,1445 22,1445 21 C1444 21,1443 20,1442 20 C1439 20,1437 21,1434 24 C1432 26,1430 30,1429 34 L1429 34 Z" fill="#212121"/><path d="M1483 27 C1483 25,1482 24,1482 23 C1482 22,1481 21,1481 21 C1480 20,1480 20,1478 20 C1476 20,1474 21,1472 23 C1470 25,1468 28,1467 32 C1466 36,1465 39,1465 42 C1465 45,1466 47,1467 48 C1468 49,1469 50,1471 50 C1473 50,1475 50,1477 49 C1478 48,1480 47,1482 45 L1484 47 C1482 50,1480 51,1477 52 C1475 54,1472 54,1470 54 C1466 54,1463 53,1462 51 C1460 49,1459 46,1459 41 C1459 39,1459 36,1460 33 C1461 30,1463 27,1465 24 C1466 22,1469 20,1471 19 C1474 18,1476 17,1479 17 C1483 17,1485 17,1488 18 L1486 27 L1483 27 Z" fill="#212121"/><path d="M1515 46 C1513 49,1511 51,1509 52 C1507 53,1505 54,1503 54 C1498 54,1496 52,1496 46 C1496 45,1496 43,1496 41 L1500 22 L1494 22 L1495 20 C1496 20,1497 20,1498 20 C1499 20,1499 19,1500 19 C1500 19,1501 18,1501 17 C1502 17,1502 16,1503 15 C1503 14,1504 12,1504 9 L1509 9 L1507 18 L1518 18 L1518 22 L1507 22 L1503 36 C1503 39,1502 41,1502 43 C1502 44,1502 45,1502 45 C1502 49,1503 50,1506 50 C1507 50,1508 50,1509 49 C1510 48,1512 46,1513 44 L1515 46 Z" fill="#212121"/><path d="M1536 54 C1532 54,1529 53,1527 51 C1525 49,1524 46,1524 41 C1524 40,1524 37,1525 35 C1526 31,1527 28,1529 25 C1531 23,1533 21,1535 19 C1538 18,1541 17,1544 17 C1548 17,1551 18,1553 20 C1555 23,1556 26,1556 30 C1556 33,1555 36,1554 39 C1553 42,1552 45,1550 47 C1549 50,1547 51,1544 52 C1542 54,1539 54,1536 54 L1536 54 Z M1530 43 C1530 46,1531 48,1532 49 C1533 50,1535 51,1537 51 C1540 51,1542 50,1544 48 C1545 46,1547 43,1548 39 C1549 35,1550 31,1550 28 C1550 26,1549 24,1548 22 C1547 21,1545 20,1543 20 C1541 20,1538 21,1537 23 C1535 26,1533 29,1532 33 C1531 37,1530 40,1530 43 L1530 43 Z" fill="#212121"/><path d="M1576 27 C1579 23,1581 21,1583 19 C1585 18,1588 17,1590 17 C1592 17,1593 17,1594 17 L1593 26 L1589 26 C1589 25,1588 24,1588 24 C1588 23,1588 23,1587 22 C1587 22,1586 22,1586 22 C1585 22,1584 22,1583 23 C1582 24,1581 25,1579 27 C1578 28,1577 30,1576 31 C1576 33,1575 35,1574 37 L1571 53 L1565 53 L1570 29 C1570 28,1571 27,1571 26 C1571 25,1571 25,1571 24 C1571 23,1571 22,1570 22 C1570 21,1570 21,1569 21 C1568 21,1567 21,1566 22 C1566 23,1564 24,1563 25 L1561 23 C1563 21,1565 19,1567 18 C1568 18,1570 17,1572 17 C1573 17,1574 18,1575 19 C1576 20,1576 21,1576 22 C1576 24,1576 25,1576 26 L1576 27 Z" fill="#212121"/><path d="M1600 67 L1610 67 L1610 3 L1600 3 L1600 0 L1616 0 L1616 70 L1600 70 L1600 67 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">The described procedure will be placed in a separate function </span><span class="f_Text" style="font-style: italic;">split_heads.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">def</span><span class="f_CodeExample">&nbsp;split_heads(</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">,&nbsp;x,&nbsp;batch_size):</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;x&nbsp;=&nbsp;tf.reshape(x,&nbsp;(batch_size,&nbsp;-</span><span class="f_CodeExample" style="color: #008100;">1</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iHeads,&nbsp;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iKeysSize))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;tf.transpose(x,&nbsp;perm=[</span><span class="f_CodeExample" style="color: #008100;">0</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #008100;">2</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #008100;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #008100;">3</span><span class="f_CodeExample">])</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Inside the </span><span class="f_Text" style="font-style: italic;">call</span><span class="f_Text"> method, we transform tensors and multiply them according to the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;query&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.split_heads(query,&nbsp;batch_size)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;key&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.split_heads(key,&nbsp;batch_size)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.split_heads(value,&nbsp;batch_size)&nbsp;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;score&nbsp;=&nbsp;tf.matmul(query,&nbsp;key,&nbsp;transpose_b=</span><span class="f_CodeExample" style="color: #ff0000;">True</span><span class="f_CodeExample">)</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, we need to divide the obtained dependence coefficients by the square root of the dimension of the key vector and normalize it with the </span><span class="f_Text" style="font-style: italic;">Softmax </span><span class="f_Text">function according to the last dimension of the tensor.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;score&nbsp;=&nbsp;score&nbsp;/&nbsp;tf.math.sqrt(tf.cast(</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iKeysSize,&nbsp;tf.float32))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;score&nbsp;=&nbsp;tf.nn.softmax(score,&nbsp;axis=-</span><span class="f_CodeExample" style="color: #008100;">1</span><span class="f_CodeExample">)</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Now we only need to multiply the normalized dependency coefficients by the </span><span class="f_Text" style="font-style: italic;">Value</span><span class="f_Text"> tensor.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;attention&nbsp;=&nbsp;tf.matmul(score,&nbsp;value)</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">As a result of this operation, we will get the attention block result for each attention head. To continue the algorithm, we need a concatenated tensor of all attention heads. Therefore, we need to carry out the reverse procedure of the tensor transformation. Once again, we rearrange the first and second dimensions and change the dimension of the tensor from three-dimensional to two-dimensional.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;attention&nbsp;=&nbsp;tf.transpose(attention,&nbsp;perm=[</span><span class="f_CodeExample" style="color: #008100;">0</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #008100;">2</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #008100;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #008100;">3</span><span class="f_CodeExample">])</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;attention&nbsp;=&nbsp;tf.reshape(attention,(batch_size,&nbsp;-</span><span class="f_CodeExample" style="color: #008100;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iDimension))</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After that, using the W</span><span class="f_Text" style="font-size: 7pt; vertical-align: sub;">0 </span><span class="f_Text">matrix, we convert the concatenated tensor of the results to the size of the tensor of the initial data. Add the two tensors and normalize the result.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;attention&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cW0(attention)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;attention=</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cNormAttention(data&nbsp;+&nbsp;attention)</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">This concludes the first block of the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> algorithm, followed by two consecutive fully connected layers of the </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> block. The first neural layer will be with the </span><span class="f_Text" style="font-style: italic;">Swish</span><span class="f_Text"> activation function, and the second one will have no activation function.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;output=</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cFF1(attention)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;output=</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cFF2(output)</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">At the end of the method, we add the result tensors of the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> blocks and normalize the layer. The result of the operations is returned in the form of a tensor.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;output=</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_cNormOutput(attention+output)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;output</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We have implemented a minimal set of methods of the class, sufficient to test its functionality. However, we will not be able to save the model with this class in this form. This is not good because our goal is to build and train a model with the subsequent possibility of practical use. Therefore, the ability to save the model and then restore it is one of the key requirements.</span></p>
<p class="p_Text"><span class="f_Text">First, to enable the saving of the new object, which is our neural layer, it is necessary to add it to the list of custom objects and provide serialization capabilities for the object. This allows us to make a directive </span><span class="f_Text" style="font-style: italic;">register_keras_serializable</span><span class="f_Text">, which we will add before declaring the class of our neural layer.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">#&nbsp;Multi-Head&nbsp;Self-Attention&nbsp;model</span>
<br><span class="f_CodeExample" style="color: #ff0000;">@</span><span class="f_CodeExample">tf.keras.utils.register_keras_serializable(package=</span><span class="f_CodeExample" style="color: #008080;">&quot;Custom&quot;</span><span class="f_CodeExample">,&nbsp;name=</span><span class="f_CodeExample" style="color: #008080;">'MHAttention'</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample" style="color: #0000ff;">class</span><span class="f_CodeExample">&nbsp;MHAttention(tf.keras.layers.Layer):</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">But that's not all. We still need to add the </span><span class="f_Text" style="font-style: italic;">get_config</span><span class="f_Text"> method, which will return the contents of variables to save to a file. Note that among the variables there are both those specified by the user when initializing the class object and those saved from the size of the initial data. Our weights are tuned to these dimensions.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">def</span><span class="f_CodeExample">&nbsp;get_config(</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">):</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;config={</span><span class="f_CodeExample" style="color: #008080;">'key_size'</span><span class="f_CodeExample">:&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iKeysSize,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #008080;">'heads'</span><span class="f_CodeExample">:&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iHeads,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #008080;">'dimension'</span><span class="f_CodeExample">:&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iDimension,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #008080;">'window'</span><span class="f_CodeExample">:&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iWindow</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;base_config&nbsp;=&nbsp;super(MHAttention,&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">).get_config()</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">dict</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #ff0000;">list</span><span class="f_CodeExample">(base_config.items())&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">list</span><span class="f_CodeExample">(config.items()))</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">The </span><span class="f_Text" style="font-style: italic;">from_config</span><span class="f_Text"> method is responsible for restoring data from the configuration list. However, please note the following. In the usual logic, the parameters from the class initialization method are specified in the configuration dictionary. But we also saved data that depends on the size of the initial data. And, as you remember, they are not included in the parameters of the initialization method. In its pure form, we will get an error about the presence of unknown parameters. Therefore, at the beginning of the method, we remove them from the configuration directory, but at the same time save the values to local variables. And only after that, we restore the layer.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">@</span><span class="f_CodeExample">classmethod</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">def</span><span class="f_CodeExample">&nbsp;from_config(cls,&nbsp;config):</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;dimension=config.pop(</span><span class="f_CodeExample" style="color: #008080;">'dimension'</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;window=config.pop(</span><span class="f_CodeExample" style="color: #008080;">'window'</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;layer&nbsp;=&nbsp;cls(**config)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;layer._build_from_signature(dimension,&nbsp;window)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;layer&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ffffff;">&nbsp;&nbsp;&nbsp;&nbsp;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After initializing our neural layer from the configuration dictionary, we need to pass the values we previously extracted about the configuration of the input data into the respective variables. To perform this functionality, we will call the _build_from_signature method, which we will also have to override.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">def</span><span class="f_CodeExample">&nbsp;_build_from_signature(</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">,&nbsp;dimension,&nbsp;window):</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iDimension=dimension</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">self</span><span class="f_CodeExample">.m_iWindow=window&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #ffffff;">&nbsp;&nbsp;&nbsp;&nbsp;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">With that, we conclude our work on the class of our neural layer and can move on to creating a model to test the newly created </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> neural layer.</span></p>

</div>

</body>
</html>
