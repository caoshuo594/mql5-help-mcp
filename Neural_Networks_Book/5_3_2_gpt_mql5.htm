<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>5.3.2 Building a GPT model in MQL5</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="5_transformer.htm"> 5. Attention mechanisms </a> / <a class="h_m" href="5_3_gpt.htm"> 5.3 GPT architecture </a>/ 5.3.2 Building a GPT model in MQL5
          </td>
          <td width="70" align="right">
          <a href="5_3_1_gpt_description.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="5_3_2_1_gpt_feedforward.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H3"><span class="f_H3">5.3.2 Building a GPT model using MQL5</span></p>
<p class="p_Text"><span class="f_Text">Before you start working on a </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> model, don't expect to get some kind of a beast at the end of the section that can solve any problems. We only build the model algorithms. The operation of these algorithms will be comparable to the computational resources involved. Of course, we will get and evaluate the results of these algorithms. But first things first.</span></p>
<p class="p_Text"><span class="f_Text">Let's briefly recap the algorithm:</span></p>
<ol style="list-style-type:decimal">
<li value="1" class="p_li"><span class="f_li">The Multi-Head Self-Attention block received, as </span><span class="f_Text">input</span><span class="f_li">, a tensor of initial data where each element of the sequence is represented by a token (a vector of values).</span></li>
</ol>
<p class="p_li"><span class="f_li">One sequence for all heads (threads). The actions in steps 2-5 are identical for each attention head.</span></p>
<ol style="list-style-type:decimal">
<li value="2" class="p_li"><span class="f_li">For each token, three vectors (</span><span class="f_li" style="font-style: italic;">Query, Key, Value</span><span class="f_li">) are calculated by multiplying the token vector by the corresponding trainable matrix of weights </span><span class="f_li" style="font-style: italic;">W</span><span class="f_li">.</span></li>
<li value="3" class="p_li"><span class="f_li">By multiplying the </span><span class="f_li" style="font-style: italic;">Query</span><span class="f_li"> and </span><span class="f_li" style="font-style: italic;">Key</span><span class="f_li"> vectors, we determine the pairwise dependencies between the elements of the sequence. At this step, the </span><span class="f_li" style="font-style: italic;">Query</span><span class="f_li"> vector of each element of the sequence is multiplied by the </span><span class="f_li" style="font-style: italic;">Key</span><span class="f_li"> vectors of the current and all previous elements of the sequence.</span></li>
<li value="4" class="p_li"><span class="f_li">The matrix of the obtained dependence coefficients is normalized using the </span><span class="f_li" style="font-style: italic;">Softmax</span><span class="f_li"> function in the context of each query (</span><span class="f_li" style="font-style: italic;">Query</span><span class="f_li">). In this case, a zero attention coefficient is set for subsequent elements of the sequence.</span></li>
<li value="5" class="p_li"><span class="f_li">As a result of steps 3 and 4, we get a square </span><span class="f_li" style="font-style: italic;">Score</span><span class="f_li"> matrix with a dimension equal to the number of elements in the sequence, where the sum of all elements in the context of each </span><span class="f_li" style="font-style: italic;">Query</span><span class="f_li"> is equal to one.</span></li>
<li value="6" class="p_li"><span class="f_li">Then we multiply the normalized attention coefficients by the </span><span class="f_li" style="font-style: italic;">Value</span><span class="f_li"> vectors of the corresponding elements of the sequence, add the resulting vectors, and get the attention-adjusted value for each element of the sequence.</span></li>
<li value="7" class="p_li"><span class="f_li">Next, we determine the weighted attention result. To do this, we multiply the concatenated tensor of the results of all attention heads by the trained matrix W</span><span class="f_li" style="font-size: 7pt; vertical-align: sub;">0</span><span class="f_li">.</span></li>
<li value="8" class="p_li"><span class="f_li">The resulting tensor is added to the input sequence and normalized.</span></li>
<li value="9" class="p_li"><span class="f_li">The </span><span class="f_li" style="font-style: italic;">Multi-Heads Self-Attention</span><span class="f_li"> mechanism is followed by two fully connected layers of the </span><span class="f_li" style="font-style: italic;">Feed Forward</span><span class="f_li"> block. The first (hidden) layer contains four times as many neurons as the input sequence with the </span><span class="f_li" style="font-style: italic;">ReLU</span><span class="f_li"> activation function (we used the </span><span class="f_li" style="font-style: italic;">Swish</span><span class="f_li"> function instead). The dimension of the second layer is equal to the dimension of the input sequence, and neurons do not use the activation function.</span></li>
<li value="10" class="p_li"><span class="f_Text">The result of the fully connected layers is summed up with the tensor input to the </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> block and the resulting tensor is normalized.</span></li>
</ol>
<p class="p_Text"><span class="f_Text">Now that we have refreshed the basic steps of the process, let's proceed with the implementation. To implement the new type of neural layer, let's create a new class </span><span class="f_Text" style="font-style: italic;">CNeuronGPT</span><span class="f_Text">, inheriting from the </span><span class="f_Text" style="font-style: italic;">CNeuronBase</span><span class="f_Text"> neural layer base class of our model. Despite using the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm in the model, I chose not to inherit from our existing classes of neural layers using attention mechanisms. This is due to some peculiarities in the model implementation, which we will become familiar with during the process.</span></p>
<p class="p_Text"><span class="f_Text">Perhaps one of the main differences is the ability to build multiple homogeneous layers within one class. Previously we used separate layers to implement parts of the model functionality, while now we are talking about the full-fledged creation of several copies of the layer being created, each with its own weights. To achieve this, in the body of the method, we declare not individual neural layers but entire collections of layers. Among them, you will see familiar variable names from working with previous classes, but they will now contain pointers to collections of neural layers. At the same time, we have preserved the functionality hidden behind the object names. Additionally, we have added two new variables:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li" style="font-style: italic;">m_iLayers</span><span class="f_li"> – number of neural layers in the block</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">m_iCurrentPosition</span><span class="f_li"> – number of the current element in the sequence</span></li>
</ul>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">class</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronGPT</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">public</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #0000ff;">protected</span><span class="f_CodeExample">:</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CArrayLayers</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CArrayLayers</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CArrayLayers</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CArrayLayers</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cScores</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CArrayLayers</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CArrayLayers</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CArrayLayers</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CArrayLayers</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iWindow</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_dStd</span><span class="f_CodeExample">[];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iCurrentPosition</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iScoreTemp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">NormlizeBuffer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">buffer</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">std</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample" style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;uint</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">std_shift</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">NormlizeBufferGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">output</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">gradient</span><span class="f_CodeExample">,</span><span class="f_CodeExample" style="color: #333333;">&nbsp;CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">std</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">uint</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">std_shift</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample" style="color: #0000ff;">public</span><span class="f_CodeExample">:</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronGPT</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">void</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;~</span><span class="f_CodeExample" style="color: #333333;">CNeuronGPT</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">void</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Init</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CLayerDescription</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">)&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">override</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">SetOpenCL</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CMyOpenCL</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">opencl</span><span class="f_CodeExample">)&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">override</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">override</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">override</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">read</span><span class="f_CodeExample">)&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">override</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">batch_size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;&amp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;&amp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">)&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">override</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">GetUnits</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">void</span><span class="f_CodeExample">)&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;{&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">GetLayers</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">void</span><span class="f_CodeExample">)&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;{&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;methods&nbsp;for&nbsp;operations&nbsp;with&nbsp;files</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Save</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">file_handle</span><span class="f_CodeExample">)&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">override</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Load</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">file_handle</span><span class="f_CodeExample">)&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">override</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;object&nbsp;identification&nbsp;methods</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">virtual</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Type</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">void</span><span class="f_CodeExample">)&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">override</span><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;{&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">(</span><span class="f_Definition">defNeuronGPT</span><span class="f_CodeExample">);&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;};</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">The addition of the </span><span class="f_Text" style="font-style: italic;">m_iCurrentPosition</span><span class="f_Text"> variable is the second architectural feature of this model. We have already said that </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> refers to autoregressive models. At each step, it returns one element of the sequence and feeds it as input at a new iteration. We mentioned something similar about recurrent models. However, in recurrent models, the hidden state was added to the current state of the environment, while in the case of </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text">, generating the language model involves creating a new state. Of course, concerning financial markets, we deviate slightly from this feedback and input the actual new state, but we will preserve the signal processing principles.</span></p>
<p class="p_Text"><span class="f_Text">The logic is as follows: if only one element of the sequence is updated at each new iteration, there is no need to recalculate the same values every time. It's not efficient. Let's recalculate only the new element of the sequence, and for the previous elements of the sequence, let's use the values from previous iterations. This is why we introduce the </span><span class="f_Text" style="font-style: italic;">m_iCurrentPosition</span><span class="f_Text"> variable to store the index of the current element in the sequence. We will get acquainted with its usage principles as we proceed with the implementation.</span></p>
<p class="p_Text"><span class="f_Text">Let's take things step by step. As usual, we will start working on the methods of the class with the class constructor. In it, we initialize variables with initial values. Similar to the attention mechanism classes discussed earlier, we use static objects that do not instantiate in the class constructor. The class destructor remains empty.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #333333;">CNeuronGPT</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">CNeuronGPT</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">void</span><span class="f_CodeExample">)&nbsp;:&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">8</span><span class="f_CodeExample">),</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iWindow</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iCurrentPosition</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Following our previously used pattern of working with classes, next, we will construct the initialization method of the class. This method is inherited from the parent class </span><span class="f_Text" style="font-style: italic;">CNeuronBase</span><span class="f_Text"> and is overridden in each new class.</span></p>
<p class="p_Text"><span class="f_Text">In the parameters, the method receives a pointer to an object describing the created neural layer, and we immediately perform a validity check on the received pointer, as well as verify the presence of the specified minimum necessary parameters for the correct initialization of the class instance.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronGPT</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">Init</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CLayerDescription</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;checking&nbsp;the&nbsp;initial&nbsp;data</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">&nbsp;||&nbsp;</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">type</span><span class="f_CodeExample">&nbsp;!=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Type</span><span class="f_CodeExample">()&nbsp;||&nbsp;</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">count</span><span class="f_CodeExample">&nbsp;&lt;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">&nbsp;||&nbsp;</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">&nbsp;&lt;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">window_out</span><span class="f_CodeExample">&nbsp;&lt;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">&nbsp;||&nbsp;</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">step</span><span class="f_CodeExample">&nbsp;&lt;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">&nbsp;||&nbsp;</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">layers</span><span class="f_CodeExample">&nbsp;&lt;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After successfully passing the control block, we save the received parameters to the appropriate variables of our class.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;save&nbsp;the&nbsp;constants</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iWindow</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">count</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">window_out</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">step</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">layers</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_Functions">ArrayResize</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_dStd</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">l</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">l</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">l</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_dStd</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">l</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">BufferInit</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">2</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Then, similar to the previously created classes using the attention mechanism, we will slightly adjust the description of the created neural layer and call the initialization method of the parent class. I would like to remind you that in the description of the created neural layer, we set the window size parameter of the input data to zero before calling the method of the parent class. This allows us to remove unused buffer objects from the parent class.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;call&nbsp;the&nbsp;initialization&nbsp;method&nbsp;of&nbsp;the&nbsp;parent&nbsp;class</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CLayerDescription</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">new</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CLayerDescription</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Copy</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">window_out</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">activation</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">AF_NONE</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">Init</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After that, we create a loop with the number of iterations equal to the number of homogeneous neural layers created. All other objects will be created in the body of this loop.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;run&nbsp;a&nbsp;loop&nbsp;to&nbsp;create&nbsp;objects&nbsp;of&nbsp;internal&nbsp;layers</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">The operations in the loop body are very similar to the operations performed in the class initialization methods using the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> mechanism, but there are still differences.</span></p>
<p class="p_Text"><span class="f_Text">Firstly, within the loop body, we create an instance of the </span><span class="f_Text" style="font-style: italic;">CLayerDescription</span><span class="f_Text"> object to describe the neural layers being created and fill it with the necessary data. Since we have decided to input only the state update to the neural network, rather than the entire pattern information, I chose to forgo using convolutional neural layers and opted for a basic fully connected neural layer. Therefore, in the </span><span class="f_Text" style="font-style: italic;">type</span><span class="f_Text"> field of the layer description object, we set the constant </span><span class="f_Text" style="font-style: italic;">defNeuronBase</span><span class="f_Text">. In this case, the window size of the input data will be equal to the size of the vector describing one element of the sequence. In this case, the entire volume of input data is perceived as the description of one element of the sequence. &nbsp; </span></p>
<p class="p_Text"><span class="f_Text">Next, we recall that the model uses the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> mechanism, so we need to create three vectors (</span><span class="f_Text" style="font-style: italic;">Query, Key, Value</span><span class="f_Text">) for each attention head from one vector of the initial data. I would like to remind you of another detail: when implementing the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> mechanism, we used concatenated vectors. Now we are going further: we will no only create a single tensor for all attention heads but we also combine all three entities mentioned above at once (</span><span class="f_Text" style="font-style: italic;">Query, Key, Value</span><span class="f_Text">). However, since it will contain only one element of the sequence, its size will not be so large. In the </span><span class="f_Text" style="font-style: italic;">count</span><span class="f_Text"> field specify a size equal to the three vectors of one element of the key tensor sequence for each attention head. The newly created layer will not have an activation function, just like before. We will use the parameter optimization method specified by the user in the neural layer description from the method parameters.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">new</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CLayerDescription</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">type</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">defNeuronBase</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iWindow</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">count</span><span class="f_CodeExample">&nbsp;=&nbsp;(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">)(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">3</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">activation</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">AF_NONE</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">optimization</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">desc</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">optimization</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After creating the neural layer description object and specifying all the necessary parameters, we create the first internal neural layer </span><span class="f_Text" style="font-style: italic;">Queries</span><span class="f_Text">. We initialize it using a pre-created neural layer description object. It is essential to monitor the process of performing operations. After successfully completing the first two operations, we add the layer to the corresponding collection.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;initialize&nbsp;Querys</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">new</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Init</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Add</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Despite creating a concatenated tensor, we have kept the name </span><span class="f_Text" style="font-style: italic;">Querys</span><span class="f_Text"> for the neural layer, maintaining continuity with the previously created attention mechanism classes. However, we will also create internal neural layers for </span><span class="f_Text" style="font-style: italic;">Keys </span><span class="f_Text">and</span><span class="f_Text" style="font-style: italic;"> Values</span><span class="f_Text">, although with different parameters. </span></p>
<p class="p_Text"><span class="f_Text">We will use the internal neural layers </span><span class="f_Text" style="font-style: italic;">Keys</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">Values</span><span class="f_Text"> to accumulate historical data on the received current states. It is, so to speak, the memory of our neural layer, and it should be sufficient to store the entire pattern being analyzed. However, since we have already calculated the state of these vectors in the fully connected neural layer </span><span class="f_Text" style="font-style: italic;">Querys</span><span class="f_Text">, we do not need matrices of weights in them. Therefore, before initializing the mentioned internal neural layers, we will make a change to the description object of the neural layer: we will set the size of the input data window to zero and ensure that the neural layer has enough elements to store the entire pattern description tensor.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;initialize&nbsp;Keys</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">new</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">count</span><span class="f_CodeExample">&nbsp;=&nbsp;(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">)(</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Init</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Add</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">The rest of the algorithm for creating an internal neural layer is similar to creating the </span><span class="f_Text" style="font-style: italic;">Querys </span><span class="f_Text">layer:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li">Create a new instance of the neural layer object.</span></li>
<li class="p_li"><span class="f_li">Initialize the neural layer.</span></li>
<li class="p_li"><span class="f_li">Add the neural layer to the corresponding collection.</span></li>
</ul>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;initialize&nbsp;Values</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">new</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Init</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Add</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After creating the neural layers </span><span class="f_Text" style="font-style: italic;">Query, Keys</span><span class="f_Text">, and </span><span class="f_Text" style="font-style: italic;">Values</span><span class="f_Text">, we proceed to create the dependency coefficient matrix </span><span class="f_Text" style="font-style: italic;">Score</span><span class="f_Text">. There are implementation nuances here as well. This matrix in the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> implementation algorithm has a square size with each side of the square equal to the number of elements of the sequence. Each element of the matrix represents the coefficient of the pairwise relationship between the elements of the sequence, where the rows of the matrix correspond to the vectors of the tensor of the </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text"> queries and the columns of the matrix correspond to the vectors of the </span><span class="f_Text" style="font-style: italic;">Key</span><span class="f_Text"> tensor.</span></p>
<p class="p_Text"><span class="f_Text">Now, let's think about how we can implement such a matrix if we have one </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text"> vector that describes only the last state. Therefore, the </span><span class="f_Text" style="font-style: italic;">Score </span><span class="f_Text">matrix in this case degenerates into a vector. Of course, for each attention head. Certainly, the neural layer of the </span><span class="f_Text" style="font-style: italic;">Score</span><span class="f_Text"> dependency coefficient vector does not contain a matrix of weights. Therefore, we adjust the number of elements in the neural layer and create a new internal neural layer using the algorithm mentioned above. Let's take advantage of the opportunity and make the matrix rectangular. The rows of the matrix will correspond to the attention heads.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;initialize&nbsp;Scores</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Scores</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">new</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Scores</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">count</span><span class="f_CodeExample">&nbsp;=&nbsp;(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">)(</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Scores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Init</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Scores</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Scores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cScores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Add</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">Scores</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Scores</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">The next object we will create is a neural layer for the concatenated output of the </span><span class="f_Text" style="font-style: italic;">AttentionOut</span><span class="f_Text"> attention heads. Here, the situation is similar to the dependency coefficient matrix. We have already discussed the reasons for the degeneration of the matrix of dependence coefficients into a vector, and to obtain the result of the work of the attention head according to the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm, we need to multiply the matrix of dependence coefficients by the </span><span class="f_Text" style="font-style: italic;">Value</span><span class="f_Text"> tensor.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:372px;height:91px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 1488 364"><path d="M31 140 C36 140,39 142,42 144 C45 147,46 151,46 156 C46 160,45 165,44 169 C43 174,41 178,38 182 C36 185,33 188,30 190 C27 191,23 192,20 192 C15 192,11 191,9 188 C6 185,5 181,5 176 C5 172,6 168,7 163 C8 159,10 155,12 151 C15 147,17 145,21 143 C24 141,27 140,31 140 L31 140 Z M39 155 C39 147,36 143,31 143 C28 143,26 144,23 146 C21 148,19 151,17 154 C16 158,14 162,13 166 C12 170,12 174,12 177 C12 185,15 189,20 189 C23 189,27 188,29 184 C32 181,34 177,36 171 C38 165,39 160,39 155 L39 155 Z" fill="#212121"/><path d="M84 180 C84 182,83 184,83 185 C83 186,83 187,84 187 C84 188,85 188,85 188 C86 188,87 188,88 187 C88 187,90 186,91 184 L93 186 C91 188,89 190,88 191 C86 192,84 192,83 192 C81 192,80 192,79 191 C78 190,78 188,78 187 C78 186,78 184,78 183 L78 182 C76 186,73 188,71 190 C69 191,67 192,64 192 C62 192,60 191,59 190 C58 189,57 187,57 184 C57 183,58 181,58 178 L61 168 C61 165,62 163,62 162 C62 161,62 160,61 160 C61 159,61 159,60 159 C59 159,58 159,57 160 C57 161,55 162,54 163 L52 161 C54 159,56 158,57 157 C58 156,59 156,60 156 C61 155,62 155,63 155 C64 155,66 156,67 157 C67 158,68 159,68 161 C68 163,68 166,67 169 L65 175 C65 177,64 178,64 179 C64 180,64 181,64 181 C64 182,64 183,64 183 C64 185,64 186,65 187 C65 188,66 188,67 188 C68 188,69 188,71 187 C72 186,73 185,74 183 C76 181,77 180,78 178 C78 177,79 175,80 172 L83 156 L89 156 L84 180 Z" fill="#212121"/><path d="M119 184 C117 187,115 189,113 190 C111 191,109 192,107 192 C102 192,100 190,100 184 C100 183,100 181,100 179 L104 160 L98 160 L99 158 C100 158,101 158,102 158 C103 158,103 157,104 157 C104 157,105 156,105 155 C106 155,106 154,107 153 C107 152,108 150,108 147 L113 147 L111 156 L122 156 L122 160 L111 160 L107 174 C107 177,106 179,106 181 C106 182,106 183,106 183 C106 187,107 188,110 188 C111 188,112 188,113 187 C114 186,116 184,117 182 L119 184 Z" fill="#212121"/><path d="M134 168 C135 165,135 163,135 162 C135 161,135 160,134 160 C134 159,134 159,133 159 C132 159,131 159,130 160 C130 161,128 162,127 163 L125 161 C127 159,129 157,130 156 C132 156,134 155,136 155 C137 155,138 156,139 157 C140 158,140 159,140 160 C140 162,140 163,140 165 L140 165 C145 158,149 155,154 155 C156 155,158 156,160 158 C161 160,162 163,162 167 C162 171,161 175,160 179 C158 183,156 186,153 189 C150 191,147 192,143 192 C140 192,137 191,135 190 L133 198 C133 200,133 201,133 202 C133 203,133 204,134 204 C134 205,135 205,137 205 L137 207 L125 207 L134 168 Z M138 178 C137 180,137 181,137 181 C137 182,137 183,137 183 C137 185,138 187,138 188 C139 189,141 189,143 189 C144 189,145 189,146 188 C147 188,148 187,149 186 C150 185,151 184,152 183 C152 181,153 180,154 178 C154 176,155 174,155 172 C155 170,155 169,155 167 C155 164,155 162,154 161 C154 160,153 159,151 159 C150 159,149 160,147 160 C146 161,145 163,143 165 C142 167,141 168,140 170 C139 172,139 174,138 177 L138 178 Z" fill="#212121"/><path d="M199 180 C199 182,198 184,198 185 C198 186,198 187,199 187 C199 188,200 188,200 188 C201 188,202 188,203 187 C203 187,205 186,206 184 L208 186 C206 188,204 190,203 191 C201 192,199 192,198 192 C196 192,195 192,194 191 C193 190,193 188,193 187 C193 186,193 184,193 183 L193 182 C191 186,188 188,186 190 C184 191,182 192,179 192 C177 192,175 191,174 190 C173 189,172 187,172 184 C172 183,173 181,173 178 L176 168 C176 165,177 163,177 162 C177 161,177 160,176 160 C176 159,176 159,175 159 C174 159,173 159,172 160 C172 161,170 162,169 163 L167 161 C169 159,171 158,172 157 C173 156,174 156,175 156 C176 155,177 155,178 155 C179 155,181 156,182 157 C182 158,183 159,183 161 C183 163,183 166,182 169 L180 175 C180 177,179 178,179 179 C179 180,179 181,179 181 C179 182,179 183,179 183 C179 185,179 186,180 187 C180 188,181 188,182 188 C183 188,184 188,186 187 C187 186,188 185,189 183 C191 181,192 180,193 178 C193 177,194 175,195 172 L198 156 L204 156 L199 180 Z" fill="#212121"/><path d="M234 184 C232 187,230 189,228 190 C226 191,224 192,222 192 C217 192,215 190,215 184 C215 183,215 181,215 179 L219 160 L213 160 L214 158 C215 158,216 158,217 158 C218 158,218 157,219 157 C219 157,220 156,220 155 C221 155,221 154,222 153 C222 152,223 150,223 147 L228 147 L226 156 L237 156 L237 160 L226 160 L222 174 C222 177,221 179,221 181 C221 182,221 183,221 183 C221 187,222 188,225 188 C226 188,227 188,228 187 C229 186,231 184,232 182 L234 184 Z" fill="#212121"/><path d="M267 165 L267 160 L313 160 L313 165 L267 165 Z M267 180 L267 175 L313 175 L313 180 L267 180 Z" fill="#212121"/><path d="M345 180 C345 183,346 185,347 187 C349 188,351 189,354 189 C357 189,360 188,362 186 C363 184,364 182,364 178 C364 177,364 175,364 174 C363 173,362 172,361 171 C360 170,359 169,357 167 C355 166,353 165,352 163 C351 162,350 161,349 160 C349 158,348 156,348 155 C348 152,349 149,350 147 C352 145,354 143,356 142 C359 141,362 140,365 140 C367 140,369 140,371 141 C373 141,375 141,378 142 L376 151 L372 151 C372 149,372 148,371 146 C371 145,370 144,369 144 C368 143,366 143,365 143 C363 143,361 144,359 144 C358 145,357 146,356 148 C355 149,355 151,355 153 C355 154,355 156,356 157 C357 159,359 161,362 162 C364 164,366 166,367 167 C368 168,369 170,370 171 C371 173,371 175,371 176 C371 180,370 182,369 185 C367 187,365 189,363 190 C360 191,357 192,354 192 C352 192,349 192,347 192 C344 191,342 191,340 190 L342 180 L345 180 Z" fill="#212121"/><path d="M395 192 C391 192,388 191,386 189 C384 187,383 184,383 179 C383 178,383 175,384 173 C385 169,386 166,388 163 C390 161,392 159,394 157 C397 156,400 155,403 155 C407 155,410 156,412 158 C414 161,415 164,415 168 C415 171,414 174,413 177 C412 180,411 183,409 185 C408 188,406 189,403 190 C401 192,398 192,395 192 L395 192 Z M389 181 C389 184,390 186,391 187 C392 188,394 189,396 189 C399 189,401 188,403 186 C404 184,406 181,407 177 C408 173,409 169,409 166 C409 164,408 162,407 160 C406 159,404 158,402 158 C400 158,397 159,396 161 C394 164,392 167,391 171 C390 175,389 178,389 181 L389 181 Z" fill="#212121"/><path d="M435 192 C434 197,432 201,430 203 C427 206,424 207,421 207 C420 207,419 207,418 207 L419 204 C419 204,420 204,421 204 C422 204,423 204,424 203 C424 203,425 202,426 200 C426 199,427 197,428 195 L436 160 L429 160 L430 158 C432 158,433 158,433 158 C434 158,435 157,435 157 C436 157,436 156,436 156 C436 155,437 154,437 152 C439 148,441 145,443 142 C446 140,449 139,454 139 C456 139,458 139,460 140 L458 146 L455 146 C455 145,454 144,454 143 C453 142,452 142,451 142 C450 142,449 142,448 143 C447 144,446 145,445 146 C445 148,444 150,443 152 L442 156 L452 156 L451 160 L442 160 L435 192 Z" fill="#212121"/><path d="M485 184 C483 187,481 189,479 190 C477 191,475 192,473 192 C468 192,466 190,466 184 C466 183,466 181,466 179 L470 160 L464 160 L465 158 C466 158,467 158,468 158 C469 158,469 157,470 157 C470 157,471 156,471 155 C472 155,472 154,473 153 C473 152,474 150,474 147 L479 147 L477 156 L488 156 L488 160 L477 160 L473 174 C473 177,472 179,472 181 C472 182,472 183,472 183 C472 187,473 188,476 188 C477 188,478 188,479 187 C480 186,482 184,483 182 L485 184 Z" fill="#212121"/><path d="M506 165 C508 161,511 159,513 157 C515 156,517 155,520 155 C522 155,524 156,525 157 C526 158,527 160,527 163 L527 163 C527 163,527 163,527 164 C529 161,531 159,533 157 C535 156,537 155,539 155 C542 155,543 156,545 157 C546 158,547 160,547 163 C547 164,546 167,545 169 L543 179 C542 182,542 184,542 185 C542 186,542 187,543 187 C543 188,543 188,544 188 C545 188,546 188,546 187 C547 187,548 186,550 184 L552 186 C550 188,548 190,547 191 C545 192,543 192,541 192 C540 192,538 191,537 190 C536 189,536 188,536 186 C536 184,536 182,537 179 L539 172 C539 170,540 168,540 167 C540 166,540 165,540 164 C540 162,540 161,539 160 C539 159,538 159,537 159 C536 159,534 160,533 160 C532 161,531 162,530 164 C528 166,527 167,526 169 C526 170,525 172,524 175 L521 191 L515 191 L519 172 C520 170,520 168,520 167 C520 166,520 165,520 164 C520 162,520 161,519 160 C519 159,518 159,516 159 C516 159,515 160,513 160 C512 161,511 162,510 164 C508 166,507 167,506 169 C506 171,505 173,504 175 L501 191 L495 191 L500 167 C501 165,501 163,501 162 C501 161,501 160,500 160 C500 159,500 159,499 159 C498 159,497 159,496 160 C496 161,494 162,493 163 L491 161 C493 159,495 158,496 157 C498 156,500 155,502 155 C503 155,504 156,505 157 C506 158,507 159,507 160 C507 162,506 163,506 165 L506 165 Z" fill="#212121"/><path d="M584 158 L588 155 L590 156 L585 180 C584 182,584 184,584 185 C584 186,584 187,584 187 C585 188,585 188,586 188 C587 188,588 188,588 187 C589 187,590 186,592 184 L594 186 C592 188,590 190,588 191 C587 192,585 192,583 192 C582 192,581 192,580 191 C579 190,578 188,578 187 C578 186,579 184,579 183 L579 182 C576 186,574 188,572 190 C570 191,568 192,565 192 C563 192,561 191,559 189 C558 187,557 184,557 180 C557 176,558 172,559 168 C561 164,563 161,566 159 C569 156,572 155,576 155 C577 155,579 155,580 156 C582 156,583 157,584 158 L584 158 Z M581 169 C581 168,582 167,582 166 C582 165,582 165,582 164 C582 162,581 160,581 159 C580 159,578 158,576 158 C574 158,572 159,570 161 C568 163,566 166,565 170 C564 173,563 177,563 180 C563 183,564 185,564 186 C565 187,566 188,568 188 C570 188,571 187,573 186 C574 185,576 182,577 180 C579 177,580 174,581 170 L581 169 Z" fill="#212121"/><path d="M612 174 C612 172,612 171,611 169 C611 167,611 165,610 164 C610 162,609 161,609 160 C609 160,609 160,608 159 C608 159,608 159,607 159 C607 159,606 159,606 159 C605 160,605 160,604 161 C604 161,603 162,602 163 L600 161 C602 159,603 158,604 157 C606 156,608 155,609 155 C610 155,611 155,611 155 C612 156,613 156,613 156 C613 157,614 157,614 158 C615 158,615 159,615 160 C616 161,616 162,616 164 C616 165,617 167,617 168 L617 168 C620 165,621 162,622 161 C624 159,625 158,626 157 C626 156,627 156,628 156 C629 155,630 155,631 155 C632 155,633 155,634 156 L632 163 L630 163 C630 162,629 161,629 161 C628 161,628 161,628 161 C628 161,627 161,627 162 C627 162,626 162,625 163 C625 164,624 165,623 166 C622 167,621 168,620 169 L618 172 C618 175,619 177,619 178 C620 180,620 182,620 183 C621 184,621 185,621 186 C621 186,622 187,622 187 C622 188,622 188,623 188 C623 188,623 188,624 188 C624 188,625 188,625 187 C626 187,627 186,628 184 L631 186 C629 188,627 190,626 191 C625 192,623 192,621 192 C620 192,619 192,618 191 C618 191,617 190,616 190 C616 189,615 187,615 186 C614 182,614 180,614 178 L613 178 C611 182,609 185,607 186 C606 188,605 189,604 190 C604 191,603 191,602 192 C601 192,600 192,599 192 C598 192,597 192,596 192 L598 184 L600 184 C600 185,601 186,601 186 C602 186,602 186,603 186 C603 186,603 185,604 184 C605 184,606 183,607 181 C608 180,610 177,612 174 L612 174 Z" fill="#212121"/><path d="M654 165 L654 176 C654 208,658 238,664 267 C671 296,681 319,693 337 L690 339 C677 322,666 298,658 269 C650 241,646 209,646 176 L646 165 L654 165 Z M646 173 L646 162 C646 129,650 98,658 69 C666 41,677 18,690 0 L693 2 C681 20,671 43,664 72 C658 101,654 131,654 162 L654 173 L646 173 Z" fill="#212121"/><path d="M757 134 C758 136,758 138,759 139 C759 140,760 141,760 142 C761 142,761 143,761 143 C762 144,762 144,763 144 C763 144,764 144,764 144 C765 144,766 144,767 143 C768 143,769 142,770 141 L772 143 C770 145,768 146,767 147 C765 148,763 148,762 148 C760 148,758 148,757 147 C755 146,754 144,753 143 C752 141,751 138,751 135 C746 135,743 133,740 131 C738 128,737 124,737 119 C737 115,738 111,739 106 C740 102,742 98,744 94 C747 90,749 88,753 86 C756 84,759 83,763 83 C768 83,771 85,774 87 C777 90,778 94,778 99 C778 103,777 107,776 111 C775 115,774 119,772 122 C770 126,768 128,765 130 C763 132,760 134,757 134 L757 134 Z M771 98 C771 94,770 91,769 89 C768 87,765 86,763 86 C760 86,758 87,755 89 C753 91,751 94,749 97 C748 101,746 105,745 109 C744 113,744 117,744 120 C744 128,747 132,752 132 C755 132,759 130,761 127 C764 124,767 119,768 114 C770 108,771 103,771 98 L771 98 Z" fill="#212121"/><path d="M816 123 C816 125,815 127,815 128 C815 129,815 130,816 130 C816 131,817 131,817 131 C818 131,819 131,820 130 C820 130,822 129,823 127 L825 129 C823 131,821 133,820 134 C818 135,816 135,815 135 C813 135,812 135,811 134 C810 133,810 131,810 130 C810 129,810 127,810 126 L810 125 C808 129,805 131,803 133 C801 134,799 135,796 135 C794 135,792 134,791 133 C790 132,789 130,789 127 C789 126,790 124,790 121 L793 111 C793 108,794 106,794 105 C794 104,794 103,793 103 C793 102,793 102,792 102 C791 102,790 102,789 103 C789 104,787 105,786 106 L784 104 C786 102,788 101,789 100 C790 99,791 99,792 99 C793 98,794 98,795 98 C796 98,798 99,799 100 C799 101,800 102,800 104 C800 106,800 109,799 112 L797 118 C797 120,796 121,796 122 C796 123,796 124,796 124 C796 125,796 126,796 126 C796 128,796 129,797 130 C797 131,798 131,799 131 C800 131,801 131,803 130 C804 129,805 128,806 126 C808 124,809 123,810 121 C810 120,811 118,812 115 L815 99 L821 99 L816 123 Z" fill="#212121"/><path d="M857 127 C854 130,852 132,849 133 C847 134,844 135,841 135 C837 135,835 134,833 132 C831 130,830 127,830 123 C830 120,830 117,831 114 C832 111,834 108,836 106 C837 103,840 101,842 100 C845 99,848 98,851 98 C854 98,856 99,858 100 C859 101,860 103,860 106 C860 110,858 113,854 115 C850 117,844 118,837 118 C836 120,836 121,836 123 C836 126,837 128,838 129 C839 130,840 131,843 131 C845 131,847 131,849 130 C850 129,852 127,854 125 L857 127 Z M837 115 C841 115,844 115,846 114 C849 114,850 113,852 111 C853 110,854 108,854 106 C854 104,853 103,853 102 C852 102,851 101,850 101 C847 101,845 102,842 105 C840 107,838 111,837 115 L837 115 Z" fill="#212121"/><path d="M879 108 C882 104,884 102,886 100 C888 99,891 98,893 98 C895 98,896 98,897 98 L896 107 L892 107 C892 106,891 105,891 105 C891 104,891 104,890 103 C890 103,889 103,889 103 C888 103,887 103,886 104 C885 105,884 106,882 108 C881 109,880 111,879 112 C879 114,878 116,877 118 L874 134 L868 134 L873 110 C873 109,874 108,874 107 C874 106,874 106,874 105 C874 104,874 103,873 103 C873 102,873 102,872 102 C871 102,870 102,869 103 C869 104,867 105,866 106 L864 104 C866 102,868 100,870 99 C871 99,873 98,875 98 C876 98,877 99,878 100 C879 101,879 102,879 103 C879 105,879 106,879 107 L879 108 Z" fill="#212121"/><path d="M917 134 C916 130,915 125,914 119 C913 112,912 108,911 106 C911 104,910 103,910 103 C910 102,909 102,909 102 C908 102,907 102,906 103 C906 104,905 105,904 106 L901 104 C903 102,905 101,906 100 C908 99,909 98,911 98 C912 98,913 98,914 98 C914 99,915 99,915 100 C916 100,916 101,916 101 C917 102,917 103,917 104 C918 105,918 107,918 109 C919 112,920 115,920 119 C921 122,921 126,921 129 C924 125,926 122,928 119 C929 115,930 113,931 110 C932 108,932 106,932 104 C932 103,932 102,931 102 C931 101,930 101,929 101 L929 99 L939 99 L940 101 C938 106,936 111,932 117 C929 123,926 129,922 134 C918 139,915 143,912 146 C910 147,908 149,907 149 C906 150,904 150,903 150 C902 150,902 150,901 150 C900 150,900 150,899 150 L901 143 L904 143 C904 144,904 145,905 145 C907 145,908 144,910 142 C912 141,914 138,917 134 L917 134 Z" fill="#212121"/><path d="M990 118 L988 122 L978 115 L980 127 L975 127 L976 115 L966 122 L964 118 L974 113 L964 108 L966 104 L976 111 L975 99 L980 99 L978 111 L988 104 L990 108 L980 113 L990 118 Z" fill="#212121"/><path d="M1030 106 L1031 106 C1032 106,1033 106,1034 106 C1035 105,1036 105,1037 104 C1038 103,1040 101,1042 99 C1045 97,1047 95,1048 94 C1049 93,1049 91,1050 91 C1050 90,1051 89,1051 88 C1051 88,1050 87,1050 87 C1049 86,1049 86,1048 86 L1048 84 L1064 84 L1063 86 C1062 86,1061 87,1060 88 C1059 88,1057 90,1054 92 L1040 106 L1048 125 C1049 127,1050 128,1050 129 C1051 130,1051 131,1052 131 C1053 132,1054 132,1055 132 L1054 134 L1040 134 L1040 132 C1042 132,1042 131,1042 130 C1042 130,1042 129,1042 128 C1042 127,1041 126,1041 125 L1036 114 C1036 112,1035 111,1035 111 C1035 110,1034 110,1034 109 C1033 109,1032 109,1031 109 L1029 109 L1026 123 C1026 125,1026 126,1026 127 C1026 127,1026 128,1026 129 C1026 130,1026 131,1026 131 C1027 132,1028 132,1029 132 L1029 134 L1014 134 L1014 132 C1015 132,1016 132,1017 131 C1017 131,1017 131,1018 130 C1018 129,1018 129,1019 128 C1019 127,1019 125,1020 123 L1026 95 C1027 93,1027 91,1027 89 C1027 88,1027 87,1026 87 C1025 86,1025 86,1023 86 L1024 84 L1038 84 L1038 86 C1037 86,1036 87,1036 87 C1036 87,1035 87,1035 88 C1035 88,1034 89,1034 90 C1034 91,1033 93,1033 95 L1030 106 Z" fill="#212121"/><path d="M1095 127 C1092 130,1090 132,1087 133 C1085 134,1082 135,1079 135 C1075 135,1073 134,1071 132 C1069 130,1068 127,1068 123 C1068 120,1068 117,1069 114 C1070 111,1072 108,1074 106 C1075 103,1078 101,1080 100 C1083 99,1086 98,1089 98 C1092 98,1094 99,1096 100 C1097 101,1098 103,1098 106 C1098 110,1096 113,1092 115 C1088 117,1082 118,1075 118 C1074 120,1074 121,1074 123 C1074 126,1075 128,1076 129 C1077 130,1078 131,1081 131 C1083 131,1085 131,1087 130 C1088 129,1090 127,1092 125 L1095 127 Z M1075 115 C1079 115,1082 115,1084 114 C1087 114,1088 113,1090 111 C1091 110,1092 108,1092 106 C1092 104,1091 103,1091 102 C1090 102,1089 101,1088 101 C1085 101,1083 102,1080 105 C1078 107,1076 111,1075 115 L1075 115 Z" fill="#212121"/><path d="M1119 134 C1118 130,1117 125,1116 119 C1115 112,1114 108,1113 106 C1113 104,1112 103,1112 103 C1112 102,1111 102,1111 102 C1110 102,1109 102,1108 103 C1108 104,1107 105,1106 106 L1103 104 C1105 102,1107 101,1108 100 C1110 99,1111 98,1113 98 C1114 98,1115 98,1116 98 C1116 99,1117 99,1117 100 C1118 100,1118 101,1118 101 C1119 102,1119 103,1119 104 C1120 105,1120 107,1120 109 C1121 112,1122 115,1122 119 C1123 122,1123 126,1123 129 C1126 125,1128 122,1130 119 C1131 115,1132 113,1133 110 C1134 108,1134 106,1134 104 C1134 103,1134 102,1133 102 C1133 101,1132 101,1131 101 L1131 99 L1141 99 L1142 101 C1140 106,1138 111,1134 117 C1131 123,1128 129,1124 134 C1120 139,1117 143,1114 146 C1112 147,1110 149,1109 149 C1108 150,1106 150,1105 150 C1104 150,1104 150,1103 150 C1102 150,1102 150,1101 150 L1103 143 L1106 143 C1106 144,1106 145,1107 145 C1109 145,1110 144,1112 142 C1114 141,1116 138,1119 134 L1119 134 Z" fill="#212121"/><path d="M1149 101 L1150 99 C1150 99,1150 99,1151 99 C1151 99,1151 99,1152 99 C1152 98,1152 98,1152 98 C1152 98,1153 97,1153 97 C1153 97,1153 96,1153 95 C1153 95,1154 94,1154 93 L1159 69 L1155 69 C1155 69,1154 69,1154 69 C1154 69,1153 69,1153 69 C1152 70,1152 70,1152 70 C1152 70,1151 71,1151 71 C1151 72,1150 72,1150 73 C1150 74,1149 74,1149 75 L1146 75 L1148 66 L1177 66 L1175 76 L1172 76 L1172 74 C1172 74,1172 74,1172 73 C1172 73,1172 73,1172 72 C1172 72,1172 71,1172 71 C1172 71,1172 71,1172 70 C1171 70,1171 70,1171 70 C1171 70,1171 69,1171 69 C1170 69,1170 69,1170 69 C1170 69,1169 69,1169 69 L1164 69 L1159 93 C1159 94,1159 95,1159 95 C1158 96,1158 97,1158 97 C1158 97,1158 98,1159 98 C1159 98,1159 98,1159 99 C1159 99,1160 99,1160 99 C1160 99,1161 99,1161 99 L1161 101 L1149 101 Z" fill="#212121"/><path d="M708 307 L726 341 L742 187 L753 187 L753 192 L746 192 L729 354 L726 354 L703 313 L697 316 L695 314 L708 307 Z" fill="#212121"/><path d="M776 234 C779 234,782 234,785 235 C787 236,789 237,791 238 C793 239,794 241,795 244 C796 246,797 249,797 252 C797 256,797 259,796 262 C795 265,794 268,793 270 C791 273,790 275,788 277 C786 279,784 280,782 281 C780 282,778 283,776 283 C773 284,771 284,767 284 L752 284 L752 282 C753 282,754 282,755 282 C755 281,755 281,756 280 C756 280,756 279,757 278 C757 277,757 276,758 273 L764 245 C765 243,765 241,765 239 C765 238,765 237,764 237 C763 236,763 236,761 236 L762 234 L776 234 Z M763 281 C763 281,765 281,767 281 C771 281,774 280,777 279 C780 278,782 275,784 272 C786 270,787 266,788 263 C789 260,790 256,790 253 C790 247,789 244,786 241 C784 238,780 237,776 237 C775 237,773 237,772 237 L763 281 Z" fill="#212121"/><path d="M823 234 L821 241 L815 241 L816 234 L823 234 Z M811 260 C811 257,812 255,812 254 C812 253,811 252,811 252 C810 251,809 251,808 251 L808 249 L817 248 L819 248 L814 273 C814 275,813 277,813 278 C813 279,813 280,814 280 C814 281,815 281,815 281 C816 281,817 281,818 280 C818 280,820 279,821 277 L823 279 C821 281,819 283,817 284 C816 285,814 285,812 285 C811 285,810 284,809 283 C808 282,807 281,807 279 C807 277,808 275,808 272 L811 260 Z" fill="#212121"/><path d="M842 258 C844 254,847 252,849 250 C851 249,853 248,856 248 C858 248,860 249,861 250 C862 251,863 253,863 256 L863 256 C863 256,863 256,863 257 C865 254,867 252,869 250 C871 249,873 248,875 248 C878 248,879 249,881 250 C882 251,883 253,883 256 C883 257,882 260,881 262 L879 272 C878 275,878 277,878 278 C878 279,878 280,879 280 C879 281,879 281,880 281 C881 281,882 281,882 280 C883 280,884 279,886 277 L888 279 C886 281,884 283,883 284 C881 285,879 285,877 285 C876 285,874 284,873 283 C872 282,872 281,872 279 C872 277,872 275,873 272 L875 265 C875 263,876 261,876 260 C876 259,876 258,876 257 C876 255,876 254,875 253 C875 252,874 252,873 252 C872 252,870 253,869 253 C868 254,867 255,866 257 C864 259,863 260,862 262 C862 263,861 265,860 268 L857 284 L851 284 L855 265 C856 263,856 261,856 260 C856 259,856 258,856 257 C856 255,856 254,855 253 C855 252,854 252,852 252 C852 252,851 253,849 253 C848 254,847 255,846 257 C844 259,843 260,842 262 C842 264,841 266,840 268 L837 284 L831 284 L836 260 C837 258,837 256,837 255 C837 254,837 253,836 253 C836 252,836 252,835 252 C834 252,833 252,832 253 C832 254,830 255,829 256 L827 254 C829 252,831 251,832 250 C834 249,836 248,838 248 C839 248,840 249,841 250 C842 251,843 252,843 253 C843 255,842 256,842 258 L842 258 Z" fill="#212121"/><path d="M920 277 C917 280,915 282,912 283 C910 284,907 285,904 285 C900 285,898 284,896 282 C894 280,893 277,893 273 C893 270,893 267,894 264 C895 261,897 258,899 256 C900 253,903 251,905 250 C908 249,911 248,914 248 C917 248,919 249,921 250 C922 251,923 253,923 256 C923 260,921 263,917 265 C913 267,907 268,900 268 C899 270,899 271,899 273 C899 276,900 278,901 279 C902 280,903 281,906 281 C908 281,910 281,912 280 C913 279,915 277,917 275 L920 277 Z M900 265 C904 265,907 265,909 264 C912 264,913 263,915 261 C916 260,917 258,917 256 C917 254,916 253,916 252 C915 252,914 251,913 251 C910 251,908 252,905 255 C903 257,901 261,900 265 L900 265 Z" fill="#212121"/><path d="M936 260 C937 258,937 256,937 255 C937 254,937 253,936 253 C936 252,936 252,935 252 C934 252,933 252,932 253 C932 254,930 255,929 256 L927 254 C929 252,931 250,933 249 C934 249,936 248,938 248 C939 248,940 249,941 250 C942 251,943 252,943 253 C943 255,942 256,942 258 L942 258 C944 254,947 252,949 250 C951 249,953 248,956 248 C958 248,960 249,961 250 C962 251,963 253,963 256 C963 257,962 260,962 262 L959 272 C959 275,958 277,958 278 C958 279,958 280,959 280 C959 281,960 281,960 281 C961 281,962 281,963 280 C963 280,965 279,966 277 L968 279 C966 281,964 283,963 284 C961 285,959 285,957 285 C956 285,955 284,954 283 C953 282,952 281,952 279 C952 277,953 275,953 272 L955 265 C956 263,956 261,956 260 C956 259,956 258,956 257 C956 255,956 254,956 253 C955 252,954 252,953 252 C952 252,951 253,949 253 C948 254,947 255,946 257 C944 259,943 260,942 262 C942 264,941 266,940 268 L937 284 L931 284 L936 260 Z" fill="#212121"/><path d="M997 258 C996 256,996 254,994 253 C993 252,992 251,989 251 C987 251,986 252,984 253 C983 254,983 255,983 257 C983 258,983 258,983 259 C983 260,984 261,985 261 C986 262,987 263,989 264 C991 265,992 266,993 267 C994 268,995 269,995 269 C996 270,996 271,996 272 C996 273,997 274,997 275 C997 277,996 279,995 280 C994 282,992 283,990 284 C988 285,986 285,983 285 C981 285,979 285,977 285 C975 284,973 284,971 283 L973 275 L975 275 C976 277,976 279,977 280 C979 282,981 282,983 282 C986 282,987 282,989 281 C990 280,991 278,991 276 C991 275,990 274,990 273 C990 272,989 272,988 271 C987 270,986 269,984 268 C982 267,981 266,980 265 C979 264,978 263,978 262 C977 261,977 259,977 258 C977 256,977 254,978 253 C979 251,981 250,983 249 C985 249,987 248,989 248 C992 248,994 248,996 249 C998 249,999 249,1001 250 L1000 258 L997 258 Z" fill="#212121"/><path d="M1025 234 L1023 241 L1017 241 L1018 234 L1025 234 Z M1013 260 C1013 257,1014 255,1014 254 C1014 253,1013 252,1013 252 C1012 251,1011 251,1010 251 L1010 249 L1019 248 L1021 248 L1016 273 C1016 275,1015 277,1015 278 C1015 279,1015 280,1016 280 C1016 281,1017 281,1017 281 C1018 281,1019 281,1020 280 C1020 280,1022 279,1023 277 L1025 279 C1023 281,1021 283,1019 284 C1018 285,1016 285,1014 285 C1013 285,1012 284,1011 283 C1010 282,1009 281,1009 279 C1009 277,1010 275,1010 272 L1013 260 Z" fill="#212121"/><path d="M1044 285 C1040 285,1037 284,1035 282 C1033 280,1032 277,1032 272 C1032 271,1032 268,1033 266 C1034 262,1035 259,1037 256 C1039 254,1041 252,1043 250 C1046 249,1049 248,1052 248 C1056 248,1059 249,1061 251 C1063 254,1064 257,1064 261 C1064 264,1063 267,1062 270 C1061 273,1060 276,1058 278 C1057 281,1055 282,1052 283 C1050 285,1047 285,1044 285 L1044 285 Z M1038 274 C1038 277,1039 279,1040 280 C1041 281,1043 282,1045 282 C1048 282,1050 281,1052 279 C1053 277,1055 274,1056 270 C1057 266,1058 262,1058 259 C1058 257,1057 255,1056 253 C1055 252,1053 251,1051 251 C1049 251,1046 252,1045 254 C1043 257,1041 260,1040 264 C1039 268,1038 271,1038 274 L1038 274 Z" fill="#212121"/><path d="M1078 260 C1079 258,1079 256,1079 255 C1079 254,1079 253,1078 253 C1078 252,1078 252,1077 252 C1076 252,1075 252,1074 253 C1074 254,1072 255,1071 256 L1069 254 C1071 252,1073 250,1075 249 C1076 249,1078 248,1080 248 C1081 248,1082 249,1083 250 C1084 251,1085 252,1085 253 C1085 255,1084 256,1084 258 L1084 258 C1086 254,1089 252,1091 250 C1093 249,1095 248,1098 248 C1100 248,1102 249,1103 250 C1104 251,1105 253,1105 256 C1105 257,1104 260,1104 262 L1101 272 C1101 275,1100 277,1100 278 C1100 279,1100 280,1101 280 C1101 281,1102 281,1102 281 C1103 281,1104 281,1105 280 C1105 280,1107 279,1108 277 L1110 279 C1108 281,1106 283,1105 284 C1103 285,1101 285,1099 285 C1098 285,1097 284,1096 283 C1095 282,1094 281,1094 279 C1094 277,1095 275,1095 272 L1097 265 C1098 263,1098 261,1098 260 C1098 259,1098 258,1098 257 C1098 255,1098 254,1098 253 C1097 252,1096 252,1095 252 C1094 252,1093 253,1091 253 C1090 254,1089 255,1088 257 C1086 259,1085 260,1084 262 C1084 264,1083 266,1082 268 L1079 284 L1073 284 L1078 260 Z" fill="#212121"/><path d="M1125 288 L1126 288 C1126 288,1126 288,1127 288 C1127 288,1127 288,1128 288 C1128 288,1128 287,1129 287 C1129 287,1129 287,1130 286 C1132 285,1133 284,1135 282 C1136 281,1137 280,1138 279 C1138 278,1139 278,1139 277 C1140 276,1140 276,1140 275 C1140 275,1140 275,1139 274 C1139 274,1138 274,1138 274 L1138 272 L1150 272 L1150 274 C1149 274,1149 274,1149 274 C1148 274,1148 275,1147 275 C1147 275,1146 276,1146 276 C1145 277,1144 278,1143 279 L1133 288 L1139 300 C1139 301,1140 302,1140 303 C1140 303,1141 304,1141 304 C1141 304,1142 305,1142 305 C1143 305,1143 305,1144 305 L1143 307 L1132 307 L1133 305 C1133 305,1134 305,1134 305 C1134 305,1134 304,1134 304 C1134 304,1134 304,1134 303 C1134 303,1134 303,1134 303 C1134 303,1134 302,1134 302 C1133 302,1133 301,1133 301 L1129 293 C1129 292,1129 292,1129 291 C1128 291,1128 291,1128 291 C1128 290,1127 290,1127 290 C1127 290,1126 290,1126 290 L1124 290 C1124 291,1124 292,1124 292 C1124 293,1123 294,1123 295 C1123 296,1123 297,1123 297 C1122 298,1122 299,1122 300 C1122 301,1122 301,1122 302 C1122 302,1122 303,1122 303 C1122 304,1122 304,1122 305 C1123 305,1123 305,1125 305 L1124 307 L1113 307 L1113 305 C1114 305,1114 305,1114 305 C1115 305,1115 304,1115 304 C1116 304,1116 303,1116 302 C1116 302,1117 301,1117 299 L1121 280 C1122 279,1122 278,1122 278 C1122 277,1122 277,1122 276 C1122 275,1122 275,1121 275 C1121 274,1120 274,1119 274 L1119 272 L1131 272 L1130 274 C1130 274,1130 274,1129 274 C1129 274,1129 275,1129 275 C1128 275,1128 275,1128 275 C1128 276,1128 276,1128 276 C1127 277,1127 277,1127 278 C1127 279,1127 279,1127 280 L1125 288 Z" fill="#212121"/><path d="M1176 301 C1174 303,1172 305,1170 306 C1168 307,1166 307,1163 307 C1162 307,1161 307,1159 306 C1158 306,1157 306,1157 305 C1156 304,1155 303,1155 302 C1155 301,1154 300,1154 298 C1154 297,1155 296,1155 295 C1155 294,1155 293,1156 292 C1156 291,1156 290,1157 289 C1158 288,1158 287,1159 286 C1160 284,1162 283,1164 282 C1166 281,1169 280,1171 280 C1174 280,1175 281,1177 282 C1178 283,1178 284,1178 286 C1178 292,1172 295,1160 295 C1160 296,1160 296,1160 297 C1160 297,1160 298,1160 298 C1160 300,1160 302,1161 303 C1162 304,1163 304,1165 304 C1165 304,1166 304,1167 304 C1168 304,1169 303,1169 303 C1170 303,1171 302,1172 301 C1172 301,1173 300,1174 299 L1176 301 Z M1160 292 C1163 292,1165 292,1166 292 C1168 291,1169 291,1170 290 C1171 290,1172 289,1173 289 C1173 288,1173 287,1173 286 C1173 285,1173 285,1173 284 C1172 283,1171 283,1170 283 C1169 283,1168 283,1167 284 C1166 284,1165 285,1165 286 C1164 286,1163 287,1162 288 C1162 289,1161 291,1160 292 L1160 292 Z" fill="#212121"/><path d="M1196 307 C1195 304,1195 301,1194 299 C1194 296,1193 294,1193 292 C1193 291,1192 289,1192 288 C1192 287,1191 286,1191 286 C1191 285,1191 285,1190 284 C1190 284,1190 284,1189 284 C1189 284,1189 284,1188 284 C1188 284,1188 284,1188 285 C1187 285,1187 285,1187 286 C1186 286,1186 287,1185 288 L1183 286 C1184 285,1185 284,1185 283 C1186 283,1187 282,1187 281 C1188 281,1189 281,1189 280 C1190 280,1191 280,1192 280 C1192 280,1193 280,1193 280 C1194 280,1194 281,1194 281 C1195 281,1195 282,1195 282 C1196 283,1196 283,1196 284 C1197 284,1197 285,1197 285 C1197 286,1197 287,1197 288 C1198 289,1198 290,1198 291 C1198 292,1198 294,1199 295 C1199 296,1199 297,1199 299 C1199 300,1200 301,1200 303 C1201 301,1202 299,1203 297 C1204 295,1205 294,1206 292 C1207 290,1207 289,1208 288 C1208 286,1208 285,1208 284 C1208 284,1208 283,1208 283 C1207 282,1207 282,1206 282 L1206 280 L1214 280 L1215 282 C1213 286,1211 291,1208 295 C1206 299,1203 303,1200 307 C1199 308,1198 310,1197 311 C1196 312,1195 313,1194 314 C1193 315,1192 315,1191 316 C1191 316,1190 317,1189 317 C1188 317,1188 318,1187 318 C1186 318,1185 318,1185 318 C1184 318,1184 318,1183 318 C1183 318,1182 318,1182 318 L1183 312 L1185 312 L1185 312 C1185 313,1186 314,1187 314 C1188 314,1190 314,1191 312 C1192 311,1194 310,1196 307 L1196 307 Z" fill="#212121"/><rect x="750" y="187" width="469" height="5" fill="#212121"/><rect x="694" y="167" width="525" height="5" fill="#212121"/><path d="M1267 165 L1267 176 C1267 209,1263 241,1255 269 C1247 298,1236 322,1223 339 L1220 337 C1232 319,1242 296,1249 267 C1256 238,1259 208,1259 176 L1259 165 L1267 165 Z M1259 173 L1259 162 C1259 131,1256 101,1249 72 C1242 44,1232 20,1220 2 L1223 0 C1236 18,1247 41,1255 69 C1263 98,1267 129,1267 162 L1267 173 L1259 173 Z" fill="#212121"/><path d="M1295 141 L1295 143 C1294 143,1293 144,1292 144 C1292 145,1291 145,1291 146 C1291 147,1291 148,1291 150 C1291 150,1291 151,1291 153 L1292 180 L1292 183 L1292 183 L1308 154 C1309 153,1310 151,1310 150 C1310 149,1311 148,1311 147 C1311 145,1309 144,1307 143 L1307 141 L1322 141 L1322 143 C1321 143,1321 143,1320 144 C1319 144,1319 145,1318 146 C1317 147,1316 149,1315 151 L1291 192 L1286 192 L1284 151 C1284 150,1284 148,1283 147 C1283 146,1283 145,1282 144 C1282 144,1281 143,1279 143 L1280 141 L1295 141 Z" fill="#212121"/><path d="M1354 158 L1358 155 L1360 156 L1355 180 C1354 182,1354 184,1354 185 C1354 186,1354 187,1354 187 C1355 188,1355 188,1356 188 C1357 188,1358 188,1358 187 C1359 187,1360 186,1362 184 L1364 186 C1362 188,1360 190,1358 191 C1357 192,1355 192,1353 192 C1352 192,1351 192,1350 191 C1349 190,1348 188,1348 187 C1348 186,1349 184,1349 183 L1349 182 C1346 186,1344 188,1342 190 C1340 191,1338 192,1335 192 C1333 192,1331 191,1329 189 C1328 187,1327 184,1327 180 C1327 176,1328 172,1329 168 C1331 164,1333 161,1336 159 C1339 156,1342 155,1346 155 C1347 155,1349 155,1350 156 C1352 156,1353 157,1354 158 L1354 158 Z M1351 169 C1351 168,1352 167,1352 166 C1352 165,1352 165,1352 164 C1352 162,1351 160,1351 159 C1350 159,1348 158,1346 158 C1344 158,1342 159,1340 161 C1338 163,1336 166,1335 170 C1334 173,1333 177,1333 180 C1333 183,1334 185,1334 186 C1335 187,1336 188,1338 188 C1340 188,1341 187,1343 186 C1344 185,1346 182,1347 180 C1349 177,1350 174,1351 170 L1351 169 Z" fill="#212121"/><path d="M1378 149 C1378 148,1378 147,1378 146 C1378 145,1378 145,1378 144 C1378 143,1378 143,1378 142 C1378 142,1377 142,1377 141 C1376 141,1376 141,1374 141 L1375 139 L1384 139 L1386 139 L1377 180 C1377 182,1376 184,1376 185 C1376 186,1376 187,1377 187 C1377 188,1378 188,1378 188 C1379 188,1380 188,1381 187 C1381 187,1383 186,1384 184 L1386 186 C1384 188,1382 190,1380 191 C1379 192,1377 192,1375 192 C1374 192,1372 191,1371 190 C1371 189,1370 188,1370 186 C1370 184,1370 182,1371 179 L1378 149 Z" fill="#212121"/><path d="M1422 180 C1422 182,1421 184,1421 185 C1421 186,1421 187,1422 187 C1422 188,1423 188,1423 188 C1424 188,1425 188,1426 187 C1426 187,1428 186,1429 184 L1431 186 C1429 188,1427 190,1426 191 C1424 192,1422 192,1421 192 C1419 192,1418 192,1417 191 C1416 190,1416 188,1416 187 C1416 186,1416 184,1416 183 L1416 182 C1414 186,1411 188,1409 190 C1407 191,1405 192,1402 192 C1400 192,1398 191,1397 190 C1396 189,1395 187,1395 184 C1395 183,1396 181,1396 178 L1399 168 C1399 165,1400 163,1400 162 C1400 161,1400 160,1399 160 C1399 159,1399 159,1398 159 C1397 159,1396 159,1395 160 C1395 161,1393 162,1392 163 L1390 161 C1392 159,1394 158,1395 157 C1396 156,1397 156,1398 156 C1399 155,1400 155,1401 155 C1402 155,1404 156,1405 157 C1405 158,1406 159,1406 161 C1406 163,1406 166,1405 169 L1403 175 C1403 177,1402 178,1402 179 C1402 180,1402 181,1402 181 C1402 182,1402 183,1402 183 C1402 185,1402 186,1403 187 C1403 188,1404 188,1405 188 C1406 188,1407 188,1409 187 C1410 186,1411 185,1412 183 C1414 181,1415 180,1416 178 C1416 177,1417 175,1418 172 L1421 156 L1427 156 L1422 180 Z" fill="#212121"/><path d="M1463 184 C1460 187,1458 189,1455 190 C1453 191,1450 192,1447 192 C1443 192,1441 191,1439 189 C1437 187,1436 184,1436 180 C1436 177,1436 174,1437 171 C1438 168,1440 165,1442 163 C1443 160,1446 158,1448 157 C1451 156,1454 155,1457 155 C1460 155,1462 156,1464 157 C1465 158,1466 160,1466 163 C1466 167,1464 170,1460 172 C1456 174,1450 175,1443 175 C1442 177,1442 178,1442 180 C1442 183,1443 185,1444 186 C1445 187,1446 188,1449 188 C1451 188,1453 188,1455 187 C1456 186,1458 184,1460 182 L1463 184 Z M1443 172 C1447 172,1450 172,1452 171 C1455 171,1456 170,1458 168 C1459 167,1460 165,1460 163 C1460 161,1459 160,1459 159 C1458 159,1457 158,1456 158 C1453 158,1451 159,1448 162 C1446 164,1444 168,1443 172 L1443 172 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">But in our case, with one </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text"> vector at the output, we also get one vector for each attention head. Therefore, we will specify the correct layer size and execute the algorithm for its initialization.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;initialize&nbsp;AttentionOut</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">new</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">count</span><span class="f_CodeExample">&nbsp;=&nbsp;(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">)(</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Init</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Add</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Following the multi-head attention algorithm, our next step will be to organize the results of all attention heads into a unified vector and adjust its size to match the size of the input data vector. In the algorithm of the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> mechanism, this operation is performed using the W</span><span class="f_Text" style="font-size: 7pt; vertical-align: sub;">0</span><span class="f_Text"> matrix. However, we will perform this operation using a basic fully connected neural layer without an activation function.</span></p>
<p class="p_Text"><span class="f_Text">Again, we will create a new instance of the neural layer object. Do not forget to check the result of the operation.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;initialize&nbsp;W0</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">new</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">In the neural layer description object, we enter the necessary parameters:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li">The size of the input data window is equal to the size of the previously created layer for the concatenated results of attention heads.</span></li>
<li class="p_li"><span class="f_li">The number of elements at the output of the neural layer is equal to the size of the source data vector.</span></li>
<li class="p_li"><span class="f_li">The activation function is not used.</span></li>
</ul>
<p class="p_Text"><span class="f_Text">We initialize the neural layer using the neural layer description object.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">count</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">count</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iWindow</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">activation</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">AF_NONE</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Init</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Add</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After the successful initialization of the neural layer object, we add it to the appropriate collection.</span></p>
<p class="p_Text"><span class="f_Text">This concludes the work on initializing the objects of the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> mechanism, and we just have to create two neural layers of the </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> block. The first neural layer has four times as many neurons in its output as the tensor received as input, and it is activated using the </span><span class="f_Text" style="font-style: italic;">Swish</span><span class="f_Text"> function.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;initialize&nbsp;FF1</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">FF1</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">new</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">FF1</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iWindow</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">count</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">4</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">activation</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">AF_SWISH</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">activation_params</span><span class="f_CodeExample">[</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">]&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">activation_params</span><span class="f_CodeExample">[</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">]&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">FF1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Init</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">FF1</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Add</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">FF1</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">FF1</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">The second neural layer of the </span><span class="f_Text" style="font-style: italic;">Feed Forward </span><span class="f_Text">block does not have the activation function. It returns the size of the tensor to the size of the initial data. Here we also use a basic fully connected neural layer. We will make the necessary adjustments to the description object of the neural layer and initialize the neural layer.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;initialize&nbsp;FF2</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">new</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">count</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">count</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iWindow</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">activation</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">AF_NONE</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Init</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Add</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We check the results of the operations at each step and add the created neural layer to the appropriate collection.</span></p>
<p class="p_Text"><span class="f_Text">At this stage, we have created all the objects necessary for the operation of a single neural layer. We remove the description object of the neural layer and proceed to the next iteration of our loop, where we will create objects for the operation of the next layer.</span></p>
<p class="p_Text"><span class="f_Text">Thus, upon completing all iterations of the loop, we will obtain objects for the operation of as many neural layers as the user specified when calling the initialization method of this neural layer.</span></p>
<p class="p_Text"><span class="f_Text">Furthermore, to avoid copying data between the buffers of internal neural layers and the current layer, we will replace the pointers to the result and gradient buffers of the current layer.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;to&nbsp;avoid&nbsp;copying&nbsp;buffers,&nbsp;we&nbsp;will&nbsp;replace&nbsp;them</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Total</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">neuron</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">&nbsp;-&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">neuron</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">neuron</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">delete</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">neuron</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">();</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">In conclusion, we call the method for distributing pointers to the OpenCL context among the class object and exit the initialization method.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">SetOpenCL</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">To fully address the issue of class initialization, I suggest considering a method for distributing the OpenCL context object pointer among the internal layer objects.</span></p>
<p class="p_Text"><span class="f_Text">Despite the change in the type of internal objects, from a neural layer to a collection of neural layers, the structure and algorithm of the pointer propagation method to the OpenCL context have not changed much. This became possible thanks to the similar method we previously wrote in the neural layer collection class.</span></p>
<p class="p_Text"><span class="f_Text">In the parameters, our </span><span class="f_Text" style="font-style: italic;">SetOpenCL</span><span class="f_Text"> method gets a pointer to the OpenCL context object. In the body of the method, we first call the relevant method of the parent class, where all the necessary controls are already implemented, and the pointer is saved in the corresponding class variable. After that, we alternately check the pointers of all the internal objects of the neural layer and call a similar method for them.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronGPT</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">SetOpenCL</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CMyOpenCL</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">opencl</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">SetOpenCL</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">opencl</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetOpencl</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetOpencl</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetOpencl</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cScores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetOpencl</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetOpencl</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetOpencl</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetOpencl</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetOpencl</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">uint</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">size</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">sizeof</span><span class="f_CodeExample">(</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">)&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iScoreTemp</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">AddBuffer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Definition">CL_MEM_READ_WRITE</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">l</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">l</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">l</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_dStd</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">l</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">BufferCreate</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">l</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">l</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">l</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_dStd</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">l</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">BufferFree</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">(!!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Thus, we conclude the class initialization and proceed directly to implementing the neural layer operational algorithm. As always, we will start with the implementation of the feed-forward method.</span></p>

</div>

</body>
</html>
