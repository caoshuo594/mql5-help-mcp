<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>5.3.1 Description of the architecture and implementation principles</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="5_transformer.htm"> 5. Attention mechanisms </a> / <a class="h_m" href="5_3_gpt.htm"> 5.3 GPT architecture </a>/ 5.3.1 Description of the architecture 
          </td>
          <td width="70" align="right">
          <a href="5_3_gpt.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="5_3_2_gpt_mql5.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H3"><span class="f_H3">5.3.1 Description of the architecture and implementation principles</span></p>
<p class="p_Text"><span class="f_Text">Let's consider the differences between </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> models and the previously considered </span><span class="f_Text" style="font-style: italic;">Transformer</span><span class="f_Text">. First, </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> models do not use the encoder while only using the decoder. This has led to the disappearance of the inner layer of </span><span class="f_Text" style="font-style: italic;">Encoder-Decoder Self-Attention</span><span class="f_Text">. The figure below shows the transformer block in </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text">. </span></p>
<div class="p_Text" style="text-align: center;"><div style="margin:0 auto 0 auto;width:189px"><img class="help" alt="GPT Block" title="GPT Block" width="189" height="189" style="width:189px;height:189px;border:none" src="683319905338.png"/><p style="text-align:center"><span class="f_Text">GPT Block</span></p></div></div>
<p class="p_Text"><span class="f_Text">As in the classic </span><span class="f_Text" style="font-style: italic;">Transformer</span><span class="f_Text">, these blocks in </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> models are lined up on top of each other. Each block has its own weigh matrices for the attention engine and fully connected </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> layers. The number of such blocks determines the size of the model. As it turns out, the stack of blocks can be quite large. There are 12 of them in </span><span class="f_Text" style="font-style: italic;">GPT-1</span><span class="f_Text"> and the smallest of </span><span class="f_Text" style="font-style: italic;">GPT-2</span><span class="f_Text"> (</span><span class="f_Text" style="font-style: italic;">GPT-2 Small</span><span class="f_Text">), 48 in </span><span class="f_Text" style="font-style: italic;">GPT-2 Extra Large</span><span class="f_Text">, and 96 in </span><span class="f_Text" style="font-style: italic;">GPT-3</span><span class="f_Text">.</span></p>
<p class="p_Text"><span class="f_Text">Like traditional language models, </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> allows you to find relationships only with the previous elements of the sequence, not allowing you to look into the future. However, unlike the Transformer, it doesn't use masking of elements but rather introduces changes to the computation process. In </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text">, the attention coefficients in the </span><span class="f_Text" style="font-style: italic;">Score</span><span class="f_Text"> matrix for subsequent elements are zeroed.</span></p>
<p class="p_Text"><span class="f_Text">At the same time, </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> can be attributed to autoregressive models. Generating one token of the sequence at a time, the generated token is added to the input sequence and fed into the model for the next iteration.</span></p>
<p class="p_Text"><span class="f_Text">As in the classic transformer, three vectors are generated inside the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> mechanism for each token: </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text">, </span><span class="f_Text" style="font-style: italic;">Key</span><span class="f_Text">, and </span><span class="f_Text" style="font-style: italic;">Value</span><span class="f_Text">. In an autoregressive model, when the input sequence changes by only 1 token on each new iteration, there is no need to recalculate the vectors for each token from scratch. That's why in GPT, each layer calculates vectors only for the new elements in the sequence and stores them for each element in the sequence. Each transformer block saves its vectors for later use. </span></p>
<p class="p_Text"><span class="f_Text">This approach allows the model to generate text word by word until it reaches the end token. </span></p>
<p class="p_Text"><span class="f_Text">And of course, </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> models use the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> mechanism.</span></p>
<p class="p_Text"><span class="f_Text">&nbsp;</span></p>

</div>

</body>
</html>
