<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>4.1.1 Description of architecture and implementation principles</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="4_main_layer_types.htm"> 4. Basic types of neural layers </a> / <a class="h_m" href="4_1_cnn.htm"> 4.1 Convolutional neural networks </a>/ 4.1.1 Description of architecture and implementation principles
          </td>
          <td width="70" align="right">
          <a href="4_1_cnn.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="4_1_2_cnn_mql5.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H3"><span class="f_H3">4.1.1 Description of architecture and implementation principles</span></p>
<p class="p_Text"><span class="f_Text">Convolutional networks, in comparison with a fully connected perceptron, have two new types of layers: convolutional (filter) and pooling (subsampling). Alternating, the specified layers are designed to highlight the main components and filter out noise in the original data while simultaneously reducing the dimensionality (volume) of the data, which is then fed into a fully connected perceptron for decision-making. Depending on the tasks to be solved, it is possible to consistently use several groups of alternating convolutional and subsample layers.</span></p>
<div class="p_Text" style="text-align: center;"><div style="margin:0 auto 0 auto;width:600px"><img class="help" id="convolution_nn" alt="Convolutional neural network" title="Convolutional neural network" width="600" height="388" style="width:600px;height:388px;border:none" src="cnn.png"/><p style="text-align:center"><span class="f_ImageCaption">Convolutional neural network</span></p></div></div>
<p class="p_Text"><span class="f_Text">The Convolution layer is responsible for recognizing objects in the source data set. In this layer, sequential operations of mathematical convolution of the original data with a small template (filter) are carried out, acting as the convolution kernel.</span></p>
<p class="p_Quote"><span class="f_Quote">Convolution is an operation in functional analysis that, when applied to two functions f and g, returns a third function corresponding to the cross-correlation function of f(x) and g(-x). The operation of convolution can be interpreted as the &quot;similarity&quot; of one function to the mirrored and shifted copy of another.</span></p>
<p class="p_Text"><span class="f_Text">In other words, the convolutional layer searches for a template element in the entire original sample. At the same time, on each iteration, the template is shifted across the array of original data with a specified step, which can be from 1 to the size of the template. If the magnitude of the shift step is smaller than the size of the template, then such convolution is called overlapping.</span></p>
<p class="p_Text"><span class="f_Text">As a result of the convolution operation, we obtain a feature array that shows the similarity of the original data to the desired template at each iteration. Activation functions are used for data normalization. The size of the obtained array will be smaller than the array of original data, and the number of such arrays is equal to the number of templates (filters).</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:291px;height:47px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 1164 188"><path d="M6 107 C6 110,7 112,8 114 C10 115,12 116,15 116 C18 116,21 115,23 113 C24 111,25 109,25 105 C25 104,25 102,25 101 C24 100,23 99,22 98 C21 97,20 96,18 94 C16 93,14 92,13 90 C12 89,11 88,10 87 C10 85,9 83,9 82 C9 79,10 76,11 74 C13 72,15 70,17 69 C20 68,23 67,26 67 C28 67,30 67,32 68 C34 68,36 68,39 69 L37 78 L33 78 C33 76,33 75,32 73 C32 72,31 71,30 71 C29 70,27 70,26 70 C24 70,22 71,20 71 C19 72,18 73,17 75 C16 76,16 78,16 80 C16 81,16 83,17 84 C18 86,20 88,23 89 C25 91,27 93,28 94 C29 95,30 97,31 98 C32 100,32 102,32 103 C32 107,31 109,30 112 C28 114,26 116,24 117 C21 118,18 119,15 119 C13 119,10 119,8 119 C5 118,3 118,1 117 L3 107 L6 107 Z" fill="#212121"/><path d="M61 68 L59 75 L53 75 L54 68 L61 68 Z M49 94 C49 91,50 89,50 88 C50 87,49 86,49 86 C48 85,47 85,46 85 L46 83 L55 82 L57 82 L52 107 C52 109,51 111,51 112 C51 113,51 114,52 114 C52 115,53 115,53 115 C54 115,55 115,56 114 C56 114,58 113,59 111 L61 113 C59 115,57 117,55 118 C54 119,52 119,50 119 C49 119,48 118,47 117 C46 116,45 115,45 113 C45 111,46 109,46 106 L49 94 Z" fill="#212121"/><path d="M94 108 L92 118 L66 118 L65 116 L85 93 C88 90,90 88,91 86 L91 86 L82 86 C81 86,80 86,80 86 C79 87,78 87,78 87 C77 88,77 88,76 89 C75 90,75 91,74 93 L70 93 L73 83 L99 83 L99 85 L82 105 C79 109,76 112,73 115 L73 115 L82 115 C83 115,84 115,85 115 C86 115,86 114,87 114 C88 114,88 113,89 112 C89 111,90 110,91 108 L94 108 Z" fill="#212121"/><path d="M131 111 C128 114,126 116,123 117 C121 118,118 119,115 119 C111 119,109 118,107 116 C105 114,104 111,104 107 C104 104,104 101,105 98 C106 95,108 92,110 90 C111 87,114 85,116 84 C119 83,122 82,125 82 C128 82,130 83,132 84 C133 85,134 87,134 90 C134 94,132 97,128 99 C124 101,118 102,111 102 C110 104,110 105,110 107 C110 110,111 112,112 113 C113 114,114 115,117 115 C119 115,121 115,123 114 C124 113,126 111,128 109 L131 111 Z M111 99 C115 99,118 99,120 98 C123 98,124 97,126 95 C127 94,128 92,128 90 C128 88,127 87,127 86 C126 86,125 85,124 85 C121 85,119 86,116 89 C114 91,112 95,111 99 L111 99 Z" fill="#212121"/><path d="M164 115 L161 115 C161 114,161 113,161 112 C160 112,160 111,160 111 C159 110,159 110,158 109 C157 109,157 109,156 109 C154 109,153 109,152 110 C151 111,150 112,149 113 C148 114,147 115,146 117 C145 118,145 120,144 121 C143 123,143 125,143 126 C142 128,142 130,142 131 C142 134,143 136,144 137 C145 139,146 139,148 139 C150 139,150 139,151 139 C152 139,153 138,154 138 C155 137,155 137,156 136 C156 135,157 134,158 133 L161 133 L159 140 C158 141,157 141,156 141 C155 141,154 142,153 142 C153 142,152 142,151 142 C150 142,149 142,148 142 C146 142,145 142,143 141 C142 141,141 140,140 139 C139 138,138 137,137 135 C137 134,137 132,137 130 C137 128,137 126,137 124 C138 122,139 120,139 118 C140 116,141 114,143 113 C144 112,145 110,146 109 C148 108,149 108,151 107 C153 106,154 106,156 106 C157 106,158 106,158 106 C159 106,160 106,161 106 C161 107,162 107,163 107 C164 107,165 107,166 108 L164 115 Z" fill="#212121"/><path d="M179 141 C178 141,176 141,175 140 C174 140,173 139,172 139 C171 138,171 137,170 136 C170 135,169 133,169 132 C169 129,170 127,171 125 C172 123,173 121,174 119 C175 118,177 116,179 115 C181 115,183 114,185 114 C187 114,188 114,189 115 C191 115,192 116,193 117 C193 118,194 119,194 120 C195 121,195 122,195 124 C195 126,195 129,194 131 C193 133,192 135,190 136 C189 138,187 139,185 140 C183 141,181 141,179 141 L179 141 Z M175 132 C175 134,175 136,176 137 C177 138,178 138,180 138 C181 138,182 138,183 137 C184 137,184 136,185 136 C186 135,186 134,187 133 C188 132,188 131,188 130 C189 129,189 128,189 126 C190 125,190 124,190 123 C190 121,189 120,188 119 C188 118,186 117,185 117 C184 117,183 117,182 118 C181 118,180 119,180 119 C179 120,178 121,178 122 C177 123,176 124,176 125 C176 126,175 128,175 129 C175 130,175 131,175 132 L175 132 Z" fill="#212121"/><path d="M212 121 C214 119,215 117,217 116 C219 115,221 114,222 114 C224 114,226 115,227 116 C228 117,228 118,228 120 C228 120,228 121,228 122 C228 123,227 124,227 125 C227 126,227 127,226 128 C226 129,226 131,225 131 C225 132,225 133,225 134 C225 135,225 135,225 135 C225 136,225 136,225 137 C225 137,226 137,226 137 C227 137,227 137,227 137 C228 137,228 137,228 136 C229 136,229 136,229 135 C230 135,230 134,231 134 L233 135 C232 137,231 137,230 138 C230 139,229 139,228 140 C228 140,227 141,226 141 C225 141,225 141,224 141 C223 141,223 141,222 141 C222 141,221 140,221 140 C220 139,220 139,220 138 C220 138,220 137,220 136 C220 136,220 135,220 134 C220 133,220 132,220 131 C221 130,221 129,221 128 C222 127,222 126,222 125 C222 124,222 124,223 123 C223 122,223 122,223 121 C223 120,223 119,222 119 C222 118,221 118,220 118 C220 118,219 118,218 118 C218 119,217 119,216 120 C216 120,215 121,215 121 C214 122,213 123,213 123 C212 124,212 125,212 126 C211 127,211 127,211 128 L208 141 L203 141 L207 124 C207 123,207 123,207 122 C207 121,207 120,207 120 C207 119,207 118,206 118 C205 118,205 118,205 118 C204 118,204 119,204 119 C203 119,203 120,202 120 C202 120,202 121,201 121 L199 120 C200 119,201 118,202 117 C202 116,203 116,204 115 C204 115,205 115,206 114 C206 114,207 114,208 114 C209 114,210 115,211 115 C212 116,212 117,212 119 C212 119,212 120,212 120 C212 120,212 121,212 121 L212 121 Z" fill="#212121"/><path d="M244 141 C243 141,242 141,242 141 C242 140,241 140,241 140 C241 139,241 139,241 138 C240 137,240 137,240 136 C240 135,240 134,241 133 C241 132,241 131,241 129 C242 124,242 121,242 120 C243 119,242 119,242 118 C242 118,242 118,241 118 C241 118,241 118,240 118 C240 119,240 119,239 119 C239 119,238 120,238 120 C238 120,237 121,237 121 L235 119 C236 119,237 118,237 117 C238 117,239 116,239 116 C240 115,241 115,242 115 C243 114,243 114,244 114 C245 114,246 114,246 115 C247 115,247 115,247 116 C248 116,248 117,248 117 C248 118,248 118,248 119 C248 119,248 120,248 120 L246 130 C246 131,246 132,246 133 C246 133,246 134,246 135 C246 136,246 136,246 136 C246 137,246 137,246 137 C247 137,247 137,248 136 C249 136,250 135,251 135 C252 134,253 133,254 132 C255 131,256 130,257 128 C257 127,258 126,259 125 C259 123,259 122,259 120 C259 120,259 119,259 119 C259 118,258 118,258 118 C257 118,256 118,256 119 C255 120,254 120,254 121 L252 119 C253 119,253 118,254 117 C254 117,255 116,256 116 C256 115,257 115,258 115 C258 114,259 114,260 114 C261 114,262 115,263 115 C263 116,264 117,264 119 C264 120,264 121,263 122 C263 123,263 124,262 125 C261 126,261 128,260 129 C259 130,258 131,257 132 C256 134,255 135,254 136 C253 137,252 138,251 139 C250 139,248 140,247 140 C246 141,245 141,244 141 L244 141 Z" fill="#212121"/><path d="M294 92 L294 87 L340 87 L340 92 L294 92 Z M294 107 L294 102 L340 102 L340 107 L294 107 Z" fill="#212121"/><path d="M372 40 C372 43,373 45,374 47 C376 48,378 49,381 49 C384 49,387 48,389 46 C390 44,391 42,391 38 C391 37,391 35,391 34 C390 33,389 32,388 31 C387 30,386 29,384 27 C382 26,380 25,379 23 C378 22,377 21,376 20 C376 18,375 16,375 15 C375 12,376 9,377 7 C379 5,381 3,383 2 C386 1,389 0,392 0 C394 0,396 0,398 1 C400 1,402 1,405 2 L403 11 L399 11 C399 9,399 8,398 6 C398 5,397 4,396 4 C395 3,393 3,392 3 C390 3,388 4,386 4 C385 5,384 6,383 8 C382 9,382 11,382 13 C382 14,382 16,383 17 C384 19,386 21,389 22 C391 24,393 26,394 27 C395 28,396 30,397 31 C398 33,398 35,398 36 C398 40,397 42,396 45 C394 47,392 49,390 50 C387 51,384 52,381 52 C379 52,376 52,374 52 C371 51,369 51,367 50 L369 40 L372 40 Z" fill="#212121"/><path d="M427 1 L425 8 L419 8 L420 1 L427 1 Z M415 27 C415 24,416 22,416 21 C416 20,415 19,415 19 C414 18,413 18,412 18 L412 16 L421 15 L423 15 L418 40 C418 42,417 44,417 45 C417 46,417 47,418 47 C418 48,419 48,419 48 C420 48,421 48,422 47 C422 47,424 46,425 44 L427 46 C425 48,423 50,421 51 C420 52,418 52,416 52 C415 52,414 51,413 50 C412 49,411 48,411 46 C411 44,412 42,412 39 L415 27 Z" fill="#212121"/><path d="M460 41 L458 51 L432 51 L431 49 L451 26 C454 23,456 21,457 19 L457 19 L448 19 C447 19,446 19,446 19 C445 20,444 20,444 20 C443 21,443 21,442 22 C441 23,441 24,440 26 L436 26 L439 16 L465 16 L465 18 L448 38 C445 42,442 45,439 48 L439 48 L448 48 C449 48,450 48,451 48 C452 48,452 47,453 47 C454 47,454 46,455 45 C455 44,456 43,457 41 L460 41 Z" fill="#212121"/><path d="M497 44 C494 47,492 49,489 50 C487 51,484 52,481 52 C477 52,475 51,473 49 C471 47,470 44,470 40 C470 37,470 34,471 31 C472 28,474 25,476 23 C477 20,480 18,482 17 C485 16,488 15,491 15 C494 15,496 16,498 17 C499 18,500 20,500 23 C500 27,498 30,494 32 C490 34,484 35,477 35 C476 37,476 38,476 40 C476 43,477 45,478 46 C479 47,480 48,483 48 C485 48,487 48,489 47 C490 46,492 44,494 42 L497 44 Z M477 32 C481 32,484 32,486 31 C489 31,490 30,492 28 C493 27,494 25,494 23 C494 21,493 20,493 19 C492 19,491 18,490 18 C487 18,485 19,482 22 C480 24,478 28,477 32 L477 32 Z" fill="#212121"/><path d="M512 66 C512 67,512 68,512 68 C512 69,512 70,512 70 C512 71,512 71,512 72 C513 72,513 72,515 72 L514 74 L503 74 L503 72 C504 72,504 72,504 72 C505 72,505 71,505 71 C506 71,506 70,506 69 C506 69,507 67,507 66 L511 47 C512 46,512 45,512 45 C512 44,512 44,512 43 C512 42,512 42,511 42 C511 41,510 41,509 41 L509 39 L521 39 L520 41 C520 41,520 41,519 41 C519 42,519 42,519 42 C518 42,518 42,518 42 C518 43,518 43,518 43 C517 44,517 44,517 45 C517 45,517 46,517 47 L512 66 Z" fill="#212121"/><path d="M536 54 C538 52,539 50,541 49 C543 48,545 47,546 47 C548 47,550 48,551 49 C552 50,552 51,552 53 C552 53,552 54,552 55 C552 56,551 57,551 58 C551 59,551 60,550 61 C550 62,550 64,549 64 C549 65,549 66,549 67 C549 68,549 68,549 68 C549 69,549 69,549 70 C549 70,550 70,550 70 C551 70,551 70,551 70 C552 70,552 70,552 69 C553 69,553 69,553 68 C554 68,554 67,555 67 L557 68 C556 70,555 70,554 71 C554 72,553 72,552 73 C552 73,551 74,550 74 C549 74,549 74,548 74 C547 74,547 74,546 74 C546 74,545 73,545 73 C544 72,544 72,544 71 C544 71,544 70,544 69 C544 69,544 68,544 67 C544 66,544 65,544 64 C545 63,545 62,545 61 C546 60,546 59,546 58 C546 57,546 57,547 56 C547 55,547 55,547 54 C547 53,547 52,546 52 C546 51,545 51,544 51 C544 51,543 51,542 51 C542 52,541 52,540 53 C540 53,539 54,539 54 C538 55,537 56,537 56 C536 57,536 58,536 59 C535 60,535 60,535 61 L532 74 L527 74 L531 57 C531 56,531 56,531 55 C531 54,531 53,531 53 C531 52,531 51,530 51 C529 51,529 51,529 51 C528 51,528 52,528 52 C527 52,527 53,526 53 C526 53,526 54,525 54 L523 53 C524 52,525 51,526 50 C526 49,527 49,528 48 C528 48,529 48,530 47 C530 47,531 47,532 47 C533 47,534 48,535 48 C536 49,536 50,536 52 C536 52,536 53,536 53 C536 53,536 54,536 54 L536 54 Z" fill="#212121"/><path d="M567 57 C567 56,567 55,567 55 C567 54,567 54,567 53 C567 52,567 51,566 51 C565 51,565 51,565 51 C564 51,564 52,564 52 C563 52,563 53,563 53 C562 53,562 54,561 54 L559 53 C560 52,561 51,562 50 C562 49,563 49,564 48 C565 48,565 48,566 47 C567 47,567 47,568 47 C569 47,570 48,571 48 C572 49,572 50,572 52 C572 52,572 52,572 53 C572 53,572 54,572 54 L572 54 C574 52,576 50,577 49 C579 48,581 47,583 47 C585 47,586 48,587 49 C589 51,589 53,589 56 C589 58,589 61,588 63 C587 65,586 67,585 69 C583 70,582 72,580 73 C578 74,576 74,574 74 C573 74,572 74,571 74 C570 73,569 73,568 73 C568 73,568 74,568 75 C567 76,567 76,567 77 C567 78,567 78,567 79 C566 79,566 80,566 80 C566 81,566 81,567 81 C567 81,567 81,567 82 C567 82,568 82,568 82 C569 82,569 82,570 82 L570 84 L560 84 L567 57 Z M570 62 C570 64,570 64,570 65 C570 66,570 67,570 67 C570 69,570 70,571 70 C571 71,572 71,574 71 C575 71,576 71,577 70 C578 70,578 69,579 69 C580 68,581 67,581 66 C582 65,582 64,583 63 C583 62,583 61,583 60 C584 59,584 57,584 56 C584 55,584 53,583 52 C582 52,582 51,580 51 C579 51,578 51,577 52 C576 53,575 53,574 55 C574 55,573 56,573 56 C573 57,572 57,572 58 C572 58,571 59,571 60 C571 61,571 61,570 62 L570 62 Z" fill="#212121"/><path d="M615 67 C613 70,611 71,610 72 C608 74,606 74,604 74 C603 74,601 74,600 73 C599 72,599 70,599 69 C599 68,599 68,599 67 C599 66,599 65,600 64 C600 63,600 62,600 61 C601 60,601 59,601 58 C601 57,602 56,602 55 C602 54,602 53,602 53 C602 52,602 51,601 51 C600 51,600 51,600 51 C599 51,599 52,598 52 C598 52,598 53,597 53 C597 53,596 54,596 54 L594 53 C595 52,596 51,597 50 C597 49,598 49,599 48 C599 48,600 48,601 47 C601 47,602 47,603 47 C604 47,605 48,606 48 C607 49,607 50,607 52 C607 52,607 53,607 54 C607 55,607 56,606 57 C606 59,605 62,605 63 C604 65,604 66,604 67 C604 68,604 69,605 69 C605 70,606 70,607 70 C608 70,608 70,609 69 C610 69,611 68,612 67 C613 66,614 65,615 64 C615 62,616 61,616 60 L619 47 L624 47 L620 64 C620 65,620 66,620 66 C620 67,620 68,620 68 C620 69,620 69,620 70 C620 70,621 70,621 70 C621 70,622 70,622 70 C622 70,623 70,623 69 C624 69,624 69,624 68 C625 68,625 67,626 67 L628 68 C627 70,626 70,625 71 C624 72,624 72,623 73 C622 73,622 74,621 74 C620 74,620 74,619 74 C618 74,617 74,616 73 C615 72,615 71,615 70 C615 69,615 69,615 68 C615 68,615 68,615 67 L615 67 Z" fill="#212121"/><path d="M650 68 C649 69,648 70,647 71 C646 72,646 72,645 73 C644 73,643 74,642 74 C642 74,641 74,640 74 C638 74,636 74,635 73 C634 72,634 70,634 68 C634 68,634 67,634 66 C634 65,634 65,634 64 L637 50 L633 50 L633 48 C634 48,635 48,636 48 C637 48,637 47,637 47 C638 47,638 46,638 46 C638 46,639 45,639 45 C639 44,639 43,640 43 C640 42,640 41,641 40 L645 40 L643 47 L652 47 L652 50 L643 50 L640 61 C640 62,640 62,639 63 C639 64,639 64,639 64 C639 65,639 65,639 66 C639 66,639 66,639 67 C639 68,639 69,640 69 C640 70,641 70,642 70 C643 70,644 70,645 69 C646 69,647 68,648 66 L650 68 Z" fill="#212121"/><path d="M678 32 L678 27 L724 27 L724 32 L678 32 Z" fill="#212121"/><path d="M752 40 C752 43,753 45,754 47 C756 48,758 49,761 49 C764 49,767 48,769 46 C770 44,771 42,771 38 C771 37,771 35,771 34 C770 33,769 32,768 31 C767 30,766 29,764 27 C762 26,760 25,759 23 C758 22,757 21,756 20 C756 18,755 16,755 15 C755 12,756 9,757 7 C759 5,761 3,763 2 C766 1,769 0,772 0 C774 0,776 0,778 1 C780 1,782 1,785 2 L783 11 L779 11 C779 9,779 8,778 6 C778 5,777 4,776 4 C775 3,773 3,772 3 C770 3,768 4,766 4 C765 5,764 6,763 8 C762 9,762 11,762 13 C762 14,762 16,763 17 C764 19,766 21,769 22 C771 24,773 26,774 27 C775 28,776 30,777 31 C778 33,778 35,778 36 C778 40,777 42,776 45 C774 47,772 49,770 50 C767 51,764 52,761 52 C759 52,756 52,754 52 C751 51,749 51,747 50 L749 40 L752 40 Z" fill="#212121"/><path d="M807 1 L805 8 L799 8 L800 1 L807 1 Z M795 27 C795 24,796 22,796 21 C796 20,795 19,795 19 C794 18,793 18,792 18 L792 16 L801 15 L803 15 L798 40 C798 42,797 44,797 45 C797 46,797 47,798 47 C798 48,799 48,799 48 C800 48,801 48,802 47 C802 47,804 46,805 44 L807 46 C805 48,803 50,801 51 C800 52,798 52,796 52 C795 52,794 51,793 50 C792 49,791 48,791 46 C791 44,792 42,792 39 L795 27 Z" fill="#212121"/><path d="M840 41 L838 51 L812 51 L811 49 L831 26 C834 23,836 21,837 19 L837 19 L828 19 C827 19,826 19,826 19 C825 20,824 20,824 20 C823 21,823 21,822 22 C821 23,821 24,820 26 L816 26 L819 16 L845 16 L845 18 L828 38 C825 42,822 45,819 48 L819 48 L828 48 C829 48,830 48,831 48 C832 48,832 47,833 47 C834 47,834 46,835 45 C835 44,836 43,837 41 L840 41 Z" fill="#212121"/><path d="M877 44 C874 47,872 49,869 50 C867 51,864 52,861 52 C857 52,855 51,853 49 C851 47,850 44,850 40 C850 37,850 34,851 31 C852 28,854 25,856 23 C857 20,860 18,862 17 C865 16,868 15,871 15 C874 15,876 16,878 17 C879 18,880 20,880 23 C880 27,878 30,874 32 C870 34,864 35,857 35 C856 37,856 38,856 40 C856 43,857 45,858 46 C859 47,860 48,863 48 C865 48,867 48,869 47 C870 46,872 44,874 42 L877 44 Z M857 32 C861 32,864 32,866 31 C869 31,870 30,872 28 C873 27,874 25,874 23 C874 21,873 20,873 19 C872 19,871 18,870 18 C867 18,865 19,862 22 C860 24,858 28,857 32 L857 32 Z" fill="#212121"/><path d="M915 42 L913 51 L910 51 C910 50,910 49,910 48 C910 48,909 47,909 46 C909 46,909 46,908 45 C908 45,907 45,906 45 L898 45 L895 58 L900 58 C901 58,901 58,902 58 C902 58,902 58,903 57 C903 57,903 57,904 56 C904 56,904 55,905 54 L908 54 L905 65 L902 65 C902 64,902 63,902 63 C902 62,902 62,902 62 C902 61,902 61,901 61 C901 61,900 61,900 61 L894 61 C894 62,894 63,893 64 C893 66,893 67,893 68 C892 69,892 70,892 71 C892 72,892 73,892 73 C892 73,892 74,892 74 C892 74,892 74,892 75 C893 75,893 75,893 75 C894 75,894 75,895 75 L894 77 L883 77 L883 75 C884 75,884 75,884 75 C885 75,885 74,885 74 C886 74,886 73,886 72 C886 72,887 70,887 69 L891 50 C892 49,892 48,892 48 C892 47,892 47,892 46 C892 46,892 45,891 45 C891 44,890 44,889 44 L889 42 L915 42 Z" fill="#212121"/><path d="M932 39 L930 45 L925 45 L926 39 L932 39 Z M925 67 C925 68,925 69,925 70 C925 70,925 71,925 71 C925 72,925 72,925 73 C925 73,926 73,926 73 C927 73,927 73,927 73 C928 73,928 73,928 72 C929 72,929 72,929 71 C930 71,930 70,931 70 L933 72 C932 73,931 74,930 74 C929 75,929 75,928 76 C927 76,927 77,926 77 C925 77,925 77,924 77 C923 77,923 77,922 77 C922 77,921 76,921 76 C920 75,920 75,920 74 C920 74,919 73,919 72 C919 72,920 71,920 70 C920 69,920 68,920 67 C921 66,921 65,921 64 C921 63,922 62,922 61 C922 60,922 59,922 58 C923 57,923 56,923 55 C923 55,923 55,923 54 C923 54,923 54,923 54 C923 53,923 53,922 53 C922 52,921 52,920 52 L920 50 L928 50 L929 50 L925 67 Z" fill="#212121"/><path d="M939 66 C940 63,941 60,942 56 C942 53,943 50,943 48 C944 46,944 45,944 44 C944 44,944 43,944 43 C944 43,944 43,943 43 C943 42,943 42,942 42 C942 42,941 42,941 42 L941 40 L948 40 L950 40 C949 45,948 50,947 54 C946 58,945 62,944 67 C944 68,944 69,944 70 C944 70,943 71,943 71 C943 72,944 72,944 73 C944 73,945 73,945 73 C946 73,946 73,946 73 C947 73,947 73,947 72 C948 72,948 72,948 71 C949 71,949 70,950 70 L952 71 C951 73,950 73,949 74 C948 75,948 75,947 76 C946 76,946 77,945 77 C944 77,944 77,943 77 C942 77,942 77,941 77 C941 76,940 76,940 76 C939 75,939 75,939 74 C939 74,938 73,938 72 C938 71,939 71,939 70 C939 69,939 68,939 66 L939 66 Z" fill="#212121"/><path d="M974 71 C973 72,972 73,971 74 C970 75,970 75,969 76 C968 76,967 77,966 77 C966 77,965 77,964 77 C962 77,960 77,959 76 C958 75,958 73,958 71 C958 71,958 70,958 69 C958 68,958 68,958 67 L961 53 L957 53 L957 51 C958 51,959 51,960 51 C961 51,961 50,961 50 C962 50,962 49,962 49 C962 49,963 48,963 48 C963 47,963 46,964 46 C964 45,964 44,965 43 L969 43 L967 50 L976 50 L976 53 L967 53 L964 64 C964 65,964 65,963 66 C963 67,963 67,963 67 C963 68,963 68,963 69 C963 69,963 69,963 70 C963 71,963 72,964 72 C964 73,965 73,966 73 C967 73,968 73,969 72 C970 72,971 71,972 69 L974 71 Z" fill="#212121"/><path d="M1002 71 C1000 73,998 75,996 76 C994 77,992 77,989 77 C988 77,987 77,985 76 C984 76,983 76,983 75 C982 74,981 73,981 72 C981 71,980 70,980 68 C980 67,981 66,981 65 C981 64,981 63,982 62 C982 61,982 60,983 59 C984 58,984 57,985 56 C986 54,988 53,990 52 C992 51,995 50,997 50 C1000 50,1001 51,1003 52 C1004 53,1004 54,1004 56 C1004 62,998 65,986 65 C986 66,986 66,986 67 C986 67,986 68,986 68 C986 70,986 72,987 73 C988 74,989 74,991 74 C991 74,992 74,993 74 C994 74,995 73,995 73 C996 73,997 72,998 71 C998 71,999 70,1000 69 L1002 71 Z M986 62 C989 62,991 62,992 62 C994 61,995 61,996 60 C997 60,998 59,999 59 C999 58,999 57,999 56 C999 55,999 55,999 54 C998 53,997 53,996 53 C995 53,994 53,993 54 C992 54,991 55,991 56 C990 56,989 57,988 58 C988 59,987 61,986 62 L986 62 Z" fill="#212121"/><path d="M1016 60 C1016 59,1016 58,1016 58 C1016 57,1016 56,1016 56 C1016 55,1016 54,1015 54 C1014 54,1014 54,1014 54 C1013 54,1013 55,1013 55 C1012 55,1012 56,1011 56 C1011 56,1011 57,1010 57 L1008 56 C1009 55,1010 54,1011 53 C1011 52,1012 52,1013 51 C1013 51,1014 51,1015 50 C1015 50,1016 50,1017 50 C1018 50,1019 51,1020 51 C1021 52,1021 53,1021 55 C1021 56,1021 56,1021 57 L1021 57 C1023 55,1025 53,1026 52 C1028 51,1030 50,1032 50 C1032 50,1033 50,1033 50 C1034 50,1035 50,1035 50 L1034 57 L1031 57 C1031 56,1030 55,1030 55 C1030 54,1029 54,1028 54 C1028 54,1028 54,1027 54 C1026 55,1026 55,1025 56 C1025 56,1024 57,1023 57 C1023 58,1022 59,1022 60 C1021 60,1021 61,1020 62 C1020 63,1020 64,1020 65 L1017 77 L1012 77 L1016 60 Z" fill="#212121"/><path d="M634 157 C634 160,635 162,636 164 C638 165,640 166,643 166 C646 166,649 165,651 163 C652 161,653 159,653 155 C653 154,653 152,653 151 C652 150,651 149,650 148 C649 147,648 146,646 144 C644 143,642 142,641 140 C640 139,639 138,638 137 C638 135,637 133,637 132 C637 129,638 126,639 124 C641 122,643 120,645 119 C648 118,651 117,654 117 C656 117,658 117,660 118 C662 118,664 118,667 119 L665 128 L661 128 C661 126,661 125,660 123 C660 122,659 121,658 121 C657 120,655 120,654 120 C652 120,650 121,648 121 C647 122,646 123,645 125 C644 126,644 128,644 130 C644 131,644 133,645 134 C646 136,648 138,651 139 C653 141,655 143,656 144 C657 145,658 147,659 148 C660 150,660 152,660 153 C660 157,659 159,658 162 C656 164,654 166,652 167 C649 168,646 169,643 169 C641 169,638 169,636 169 C633 168,631 168,629 167 L631 157 L634 157 Z" fill="#212121"/><path d="M693 161 C691 164,689 166,687 167 C685 168,683 169,681 169 C676 169,674 167,674 161 C674 160,674 158,674 156 L678 137 L672 137 L673 135 C674 135,675 135,676 135 C677 135,677 134,678 134 C678 134,679 133,679 132 C680 132,680 131,681 130 C681 129,682 127,682 124 L687 124 L685 133 L696 133 L696 137 L685 137 L681 151 C681 154,680 156,680 158 C680 159,680 160,680 160 C680 164,681 165,684 165 C685 165,686 165,687 164 C688 163,690 161,691 159 L693 161 Z" fill="#212121"/><path d="M729 161 C726 164,724 166,721 167 C719 168,716 169,713 169 C709 169,707 168,705 166 C703 164,702 161,702 157 C702 154,702 151,703 148 C704 145,706 142,708 140 C709 137,712 135,714 134 C717 133,720 132,723 132 C726 132,728 133,730 134 C731 135,732 137,732 140 C732 144,730 147,726 149 C722 151,716 152,709 152 C708 154,708 155,708 157 C708 160,709 162,710 163 C711 164,712 165,715 165 C717 165,719 165,721 164 C722 163,724 161,726 159 L729 161 Z M709 149 C713 149,716 149,718 148 C721 148,722 147,724 145 C725 144,726 142,726 140 C726 138,725 137,725 136 C724 136,723 135,722 135 C719 135,717 136,714 139 C712 141,710 145,709 149 L709 149 Z" fill="#212121"/><path d="M745 145 C746 142,746 140,746 139 C746 138,746 137,745 137 C745 136,745 136,744 136 C743 136,742 136,741 137 C741 138,739 139,738 140 L736 138 C738 136,740 134,741 133 C743 133,745 132,747 132 C748 132,749 133,750 134 C751 135,751 136,751 137 C751 139,751 140,751 142 L751 142 C756 135,760 132,765 132 C767 132,769 133,771 135 C772 137,773 140,773 144 C773 148,772 152,771 156 C769 160,767 163,764 166 C761 168,758 169,754 169 C751 169,748 168,746 167 L744 175 C744 177,744 178,744 179 C744 180,744 181,745 181 C745 182,746 182,748 182 L748 184 L736 184 L745 145 Z M749 155 C748 157,748 158,748 158 C748 159,748 160,748 160 C748 162,749 164,749 165 C750 166,752 166,754 166 C755 166,756 166,757 165 C758 165,759 164,760 163 C761 162,762 161,763 160 C763 158,764 157,765 155 C765 153,766 151,766 149 C766 147,766 146,766 144 C766 141,766 139,765 138 C765 137,764 136,762 136 C761 136,760 137,758 137 C757 138,756 140,754 142 C753 144,752 145,751 147 C750 149,750 151,749 154 L749 155 Z" fill="#212121"/><rect x="366" y="94" width="673" height="5" fill="#212121"/><path d="M1087 99 L1087 120 L1081 120 L1081 99 L1061 99 L1061 94 L1081 94 L1081 73 L1087 73 L1087 94 L1107 94 L1107 99 L1087 99 Z" fill="#212121"/><path d="M1155 108 C1155 110,1155 111,1155 111 C1155 112,1156 113,1156 113 C1156 114,1157 114,1158 114 C1159 115,1159 115,1161 115 C1162 115,1163 115,1165 115 L1165 118 L1138 118 L1138 115 C1141 115,1143 115,1144 115 C1145 114,1146 114,1146 114 C1147 113,1147 113,1148 112 C1148 111,1148 110,1148 108 L1148 78 C1148 77,1148 77,1148 76 C1147 76,1147 76,1146 76 C1145 76,1144 76,1143 77 C1141 78,1140 79,1138 80 L1136 77 L1153 67 L1155 67 C1155 69,1155 73,1155 77 L1155 108 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">It is also important for us to note that the templates themselves are not specified during the design of the neural network but are selected during the training process.</span></p>
<p class="p_Text"><span class="f_Text">Next subsample layer is used to reduce the size of the feature array and filter noise. The application of this iteration is based on the assumption that the similarity of the original data to the template is primary, and the exact coordinates of the feature in the array of original data are not so important. This allows addressing the scaling issue, as it permits some variability in the distance between the sought-after objects.</span></p>
<p class="p_Text"><span class="f_Text">At this stage, data is condensed by maintaining the maximum or average value within a specified window. This way, only one value per data window is saved. Operations are carried out iteratively, shifting the window by a specified step with each new iteration. Data compaction is performed separately for each array of features.</span></p>
<p class="p_Text"><span class="f_Text">Pooling layers with a window and a step of two are often used, which makes it possible to halve the size of the feature array. However, in practice, it is also possible to use a larger window. Furthermore, consolidation iterations can be carried out both with overlapping (when the step size is smaller than the window size) and without.</span></p>
<p class="p_Text"><span class="f_Text">At the output of the pooling layer, we obtain arrays of features of smaller dimensions. </span></p>
<p class="p_Text"><span class="f_Text">Depending on the complexity of the tasks being solved, after the pooling layer, it is possible to use one or several groups of convolutional and pooling layers. The principles of their construction and functionality comply with the principles described above.</span></p>
<p class="p_Text"><span class="f_Text">In the general case, after one or several groups of &quot;convolution + compaction&quot;, arrays of features obtained from all filters are gathered into a single vector and fed into a multilayer perceptron for the neural network's decision-making.</span></p>
<p class="p_Text"><span class="f_Text">Convolutional neural networks are trained by the well-known method of error backpropagation. This method belongs to the unsupervised learning methods and imply propagating the error gradient from the output layer of neurons through hidden layers to the input layer of neurons with adjustment of weights towards the anti-gradient.</span></p>
<p class="p_Text"><span class="f_Text">Convolutional neural networks are trained by the well-known method of error backpropagation.</span></p>
<p class="p_Text"><span class="f_Text">In the pooling layer, the error gradient is calculated for each element in the feature array, analogous to the gradients of neurons in a fully connected perceptron. The algorithm for transferring the gradient to the previous layer depends on the compaction operation used. If only the maximum value is taken, then the entire gradient is passed to the neuron with the maximum value. For the other elements within the consolidation window, a zero gradient is set, as during the forward pass they did not influence the final result. If the operation of averaging is used within the window, then the gradient is evenly distributed to all elements within the window.</span></p>
<p class="p_Text"><span class="f_Text">Weights are not used in the compaction operation, therefore, nothing is adjusted during the training process.</span></p>
<p class="p_Text"><span class="f_Text">Operations are somewhat more complicated when training the neurons of the convolutional layer. The error gradient is calculated for each element of the feature array and descends to the corresponding neurons of the previous layer. The process of training the convolutional layer is based on convolution and reverse convolution operations.</span></p>
<p class="p_Text"><span class="f_Text">To propagate the error gradient from the pooling layer to the convolutional layer, first, the edges of the error gradient array, obtained from the pooling layer, are padded with zero elements, and then a convolution of the resulting array is performed with the convolution kernel rotated by 180&deg;. The output is an array of error gradients equal to the input data array, in which the gradient indices will correspond to the index of the corresponding neuron of the previous layer.</span></p>
<p class="p_Text"><span class="f_Text">To obtain the weight deltas, convolution is performed between the matrix of input values and the matrix of error gradients of this layer, rotated by 180&deg;. The output is an array of deltas with a size equal to the convolution core. The resulting deltas should be adjusted for the derivative of the activation function of the convolutional layer and the learning coefficient. After that, the weights of the convolution core change by the value of the corrected deltas.</span></p>
<p class="p_Text"><span class="f_Text">It probably sounds rather hard to understand. Let's try to clarify these points while considering the code in detail.</span></p>

</div>

</body>
</html>
