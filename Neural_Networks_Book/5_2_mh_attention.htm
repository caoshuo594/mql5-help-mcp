<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>5.2 Multi-Head attention</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="5_transformer.htm"> 5. Attention mechanisms </a>/ 5.2 Multi-Head attention
          </td>
          <td width="70" align="right">
          <a href="5_1_4_tr_comparison.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="5_2_1_mh_attention_description.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H2"><span class="f_H2">5.2 Multi-Head attention</span></p>
<p class="p_Text"><span class="f_Text">In the previous section, we got acquainted with the mechanism of </span><span class="f_Text" style="font-style: italic;">Self-Attention mechanism,</span><span class="f_Text"> which was introduced in June 2017 in the article <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="external" class="weblink">Attention Is All You Need</a>. The key feature of this mechanism is its ability to capture dependencies between individual elements in a sequence. We even implemented it and managed to test it on real data. The model demonstrated its effectiveness.</span></p>
<p class="p_Text"><span class="f_Text">Recall that the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm uses three trainable matrices of weights (</span><span class="f_Text" style="font-style: italic;">W</span><span class="f_Text" style="font-size: 7pt; font-style: italic; vertical-align: sub;">Q</span><span class="f_Text" style="font-style: italic;">, W</span><span class="f_Text" style="font-size: 7pt; font-style: italic; vertical-align: sub;">K</span><span class="f_Text">, and </span><span class="f_Text" style="font-style: italic;">W</span><span class="f_Text" style="font-size: 7pt; font-style: italic; vertical-align: sub;">V</span><span class="f_Text">). The matrix data is used to obtain 3 three entities: </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text">, </span><span class="f_Text" style="font-style: italic;">Key</span><span class="f_Text">, and </span><span class="f_Text" style="font-style: italic;">Value</span><span class="f_Text">. The first two determine the pairwise relationship between elements of the sequence, while the last one represents the context of the analyzed element.</span></p>
<p class="p_Text"><span class="f_Text">It's not a secret that situations are not always straightforward. The same situation can often be interpreted from various perspectives. With different points of view, the conclusions can be completely opposite. In such situations, it's important to consider all possible options and only draw a conclusion after a comprehensive analysis. That's why in the same paper, the authors of the method proposed using </span><span class="f_Text" style="font-style: italic;">Multi-Head Attention</span><span class="f_Text"> to address such problems. This is the launch of several parallel </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> threads, with different weights. Here, each 'head' has its own opinion, and the decision is made by a balanced vote. A solution like this should better identify connections between different elements of the sequence.</span></p>
<p class="p_Text" style="text-align: center;"><img class="help" alt="Self-Attention" title="Self-Attention" width="600" height="312" style="margin:0 auto 0 auto;width:600px;height:312px;border:none" src="self_attention.png"/></p>
<div class="p_Text" style="text-align: center;"><div style="margin:0 auto 0 auto;width:600px"><img class="help" alt="Multi-Headed Attention architecture diagram" title="Multi-Headed Attention architecture diagram" width="600" height="439" style="width:600px;height:439px;border:none" src="mh_self_attention.png"/><p style="text-align:center"><span class="f_ImageCaption">Multi-Headed Attention architecture diagram</span></p></div></div>
<p class="p_Text"><span class="f_Text">In the </span><span class="f_Text" style="font-style: italic;">Multi-Head Attention</span><span class="f_Text"> architecture, several </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> threads with different weights are used in parallel, which simulates a versatile analysis of the situation. The results of the threads are concatenated into a single tensor. The final result of the algorithm is determined by multiplying the tensor by </span><span class="f_Text" style="font-style: italic;">W</span><span class="f_Text" style="font-size: 7pt; font-style: italic; vertical-align: sub;">0</span><span class="f_Text"> matrix, the parameters of which are selected in the process of training the neural network. This whole architecture replaces the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> block in the encoder and decoder of the transformer architecture.</span></p>
<p class="p_Text"><span class="f_Text">It is the </span><span class="f_Text" style="font-style: italic;">Multi-Head Attention </span><span class="f_Text">architecture that is most often used to solve practical problems.</span></p>
<p class="p_Text"><span class="f_Text">&nbsp;</span></p>
<p class="p_Text"><span class="f_Text">&nbsp;</span></p>

</div>

</body>
</html>
