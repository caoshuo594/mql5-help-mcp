<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>5.2.3 Organizing parallel computing for Multi-Head Self-Attention</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="5_transformer.htm"> 5. Attention mechanisms </a> / <a class="h_m" href="5_2_mh_attention.htm"> 5.2 Multi-Head attention </a>/ 5.2.3 Organizing parallel computing for Multi-Head Self-Attention
          </td>
          <td width="70" align="right">
          <a href="5_2_2_3_mh_attention_save_load.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="5_2_4_mh_attention_python.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H3"><span class="f_H3">5.2.3 Organizing parallel computing for Multi-Head Self-Attention</span></p>
<p class="p_Text"><span class="f_Text">We continue our steady progress on the path of knowledge and building a library for creating machine learning models within the MQL5 environment. In this section, we are planning to complete work on </span><span class="f_Text" style="font-style: italic;">CNeuronMHAttention</span><span class="f_Text"> which is another class of neural layers. This class implements the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> algorithm. In the previous sections, we have already fully implemented the algorithm using standard MQL5 tools. Now let's supplement its functionality with the ability to use OpenCL technology to organize the computation process in multi-threaded mode using </span><span class="f_Text" style="font-style: italic;">GPU</span><span class="f_Text"> resources.</span></p>
<p class="p_Text"><span class="f_Text">We have already implemented similar work for each of the previously discussed neural layers. Let me remind you of the general algorithm for constructing this process. First, we create an OpenCL program. Next, we enhance the main program code with the functionality for calling this program and passing the necessary data in both directions. We will need to send the input data to the program before its execution and calculate the results after its execution.</span></p>
<p class="p_Text"><span class="f_Text">As usual, we start by creating an OpenCL program. Do we really need to create a new program? Why do we need to create new kernels? The answer is obvious: we need to implement functionality. But let's recall that we inherited our class from a similar class implementing the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm. We have repeatedly talked about the continuity of these algorithms. Can we use the kernels created earlier to implement processes in this class?</span></p>
<p class="p_Text"><span class="f_Text">Considering the similarity of processes, it would be more advantageous for us to use the same kernels for both implementations. Firstly, this reduces the number of OpenCL objects, and the system can only handle a limited number of them. Secondly, it's always more convenient to maintain and optimize one object rather than duplicating shared blocks across multiple similar objects, be it kernels or classes.</span></p>
<p class="p_Text"><span class="f_Text">So, how can we implement this? The previously created kernels work within the same head of attention. Of course, on the main program side, we can copy data into separate buffers and sequentially invoke kernels for each attention head. This approach is possible, but it is irrational. Excessive copying of data in itself is not the best solution. Moreover, calling kernels sequentially for each attention head doesn't allow for the simultaneous calculation of all attention heads in parallel threads.</span></p>
<p class="p_Text"><span class="f_Text">In fact, we can use the previously created kernels without unnecessary copying, by implementing some minor modifications.</span></p>
<p class="p_Text"><span class="f_Text">One thing we have already done from the very beginning when creating the class is fully utilizing concatenated data buffers. That is, all our data buffers contain data from all attention heads at once. By transferring data to context memory, we transfer data from all attention heads. This means that on the OpenCL side, we can work with all attention heads in parallel. We just need to correctly determine the offset in the data buffer to the required values. These are the changes that we must make to the kernel.</span></p>
<p class="p_Text"><span class="f_Text">To determine this bias, we need to understand the total number of attention heads used and the ordinal number of the working attention head. We can pass the total quantity in parameters but can't do the same for the serial number of the current one. To implement the transfer of such data, we would need to create a loop with a sequential kernel call for each attention head, which we try to avoid.</span></p>
<p class="p_Text"><span class="f_Text">Let's remember how the function of queuing the kernel is organized. The </span><span class="f_Text" style="font-style: italic;">CLExecute</span><span class="f_Text"> function has a </span><span class="f_Text" style="font-style: italic;">work_dim</span><span class="f_Text"> parameter, which is responsible for the dimension of the task space. The function also receives in parameters a dynamic array </span><span class="f_Text" style="font-style: italic;">global_work_size[]</span><span class="f_Text">, which indicates the total number of tasks being performed in each dimension.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">CLExecute</span><span class="f_CodeExample">(&nbsp;</span><br>
<span class="f_CodeExample" style="color: #808080;">//&nbsp;handle&nbsp;to&nbsp;the&nbsp;OpenCL&nbsp;program&nbsp;kernel&nbsp;</span><br>
<span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">kernel</span><span class="f_CodeExample">,</span><br>
<span class="f_CodeExample" style="color: #808080;">//&nbsp;dimension&nbsp;of&nbsp;the&nbsp;problem&nbsp;space&nbsp;&nbsp;</span><br>
<span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">uint</span><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">work_dim</span><span class="f_CodeExample">,</span><br>
<span class="f_CodeExample" style="color: #808080;">//&nbsp;initial&nbsp;offset&nbsp;in&nbsp;task&nbsp;space&nbsp;</span><br>
<span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">uint</span><span class="f_CodeExample">&amp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">global_work_offset</span><span class="f_CodeExample">[],</span><br>
<span class="f_CodeExample" style="color: #808080;">//&nbsp;total&nbsp;number&nbsp;of&nbsp;tasks&nbsp;</span><br>
<span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">uint</span><span class="f_CodeExample">&amp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">global_work_size</span><span class="f_CodeExample">[]</span><br>
<span class="f_CodeExample">&nbsp;&nbsp;&nbsp;);</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Earlier we used only one dimension, and now we can use two. We will continue to use one dimension for iterating over the elements of the sequence and the other dimension for iterating over the attention heads.</span></p>
<p class="p_Text"><span class="f_Text">Well, a solution has been found and we can begin implementation. But there is one more question: to create a new kernel or not. Everything points towards modifying the previously created one. But in this case, after finishing the work, we will have to take a step back and adjust the methods of the </span><span class="f_Text" style="font-style: italic;">CNeuronAttention</span><span class="f_Text"> class. Otherwise, we will get a critical error when trying to launch the kernel.</span></p>
<p class="p_Text"><span class="f_Text">For my part, I decided to make changes to the previously created kernel and the methods of the main program. You can choose your preferred options.</span></p>
<p class="p_Text"><span class="f_Text">Now, let's look at the changes made to the feed-forward kernel.</span></p>
<p class="p_Text"><span class="f_Text">In the kernel body, we request the identifiers of the launched thread and the total number of threads in two dimensions. The first dimension will specify the index of the processed request and the length of the sequence. The second dimension will indicate the number of the active attention head.</span></p>
<p class="p_Text"><span class="f_Text">We also determine the offset to the beginning of the vector being analyzed in the query tensor and the attention coefficient matrix.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ffffff;">&nbsp;</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">q</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">get_global_id</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">get_global_size</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">get_global_id</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">get_global_size</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">shift_query</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">key_size</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">q</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">shift_scores</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">q</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">);</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">As you can see, compared to the previous version, the kernel differs by having a second dimension that accounts for attention heads. Accordingly, the calculation of offsets is also adjusted to account for multi-head attention.</span></p>
<p class="p_Text"><span class="f_Text">Next, we create a system of two nested loops to calculate one vector of the matrix of dependence coefficients. This is due to the fact that to calculate one element of the sequence at the output of the attention block, we need a whole vector of the matrix of dependence coefficients. Such calculation applies to one attention head.</span></p>
<p class="p_Text"><span class="f_Text">Also, before starting the loop system, we will prepare a local variable </span><span class="f_Text" style="font-style: italic;">summ</span><span class="f_Text"> to sum all the values of the vector. We will need this sum later to normalize the vector values.</span></p>
<p class="p_Text"><span class="f_Text">The outer loop has a number of iterations equal to the number of sequence elements. It will immediately indicate the analyzed element in the key tensor </span><span class="f_Text" style="font-style: italic;">Key</span><span class="f_Text"> and the column number in the attention coefficient matrix. In the body of the loop, we will determine the offset in the key tensor to the beginning of the vector of the analyzed element in the sequence and prepare a variable for calculating the result of multiplying two vectors.</span></p>
<p class="p_Text"><span class="f_Text">In the nested loop with a number of iterations equal to the size of the key vector, we will perform the operation of multiplying the query vector by the key vector.</span></p>
<p class="p_Text"><span class="f_Text">After completing the iterations in the nested loop, we will take the exponential of the obtained result from the vector multiplication, record the resulting value in the tensor of attention coefficient matrices, and add it to our sum of vector values. </span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">summ</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">shift_key</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">key_size</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">key_size</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">&nbsp;++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">&nbsp;+=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_query</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">]&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_key</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">exp</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">&nbsp;/&nbsp;</span><span class="f_Functions">sqrt</span><span class="f_CodeExample">((</span><span class="f_CodeExample" style="color: #333333;">TYPE</span><span class="f_CodeExample">)</span><span class="f_CodeExample" style="color: #333333;">key_size</span><span class="f_CodeExample">));</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">summ</span><span class="f_CodeExample">&nbsp;+=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">scores</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_scores</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">]&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After completing all iterations of the loop system, we will have a vector with computed but unnormalized attention coefficients for one query vector with respect to all key vectors. To complete the vector normalization process, we need to divide the contents of the vector by the sum of all its values, which we have collected in the </span><span class="f_Text" style="font-style: italic;">summ</span><span class="f_Text"> variable.</span></p>
<p class="p_Text"><span class="f_Text">To perform this operation, we will create another loop with the number of iterations equal to the number of elements in the sequence.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">scores</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_scores</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">]&nbsp;/=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">summ</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">As you can see, this block differs from the previous implementation only in terms of calculating the offsets of elements in tensors. Now that we have the normalized attention vector for one query with respect to all elements in the key tensor sequence, we can calculate the weighted vector for one element of the sequence at the output of one attention head. To do this, we will create a system of two nested loops.</span></p>
<p class="p_Text"><span class="f_Text">First, we will determine the offset in the result tensor to the beginning of the vector for the analyzed element.</span></p>
<p class="p_Text"><span class="f_Text">Then we will create an outer loop based on the number of elements in the result vector. In the body of the loop, we will first prepare a variable for accumulating the value of one element of the vector. We will create a nested loop with the number of iterations equal to the number of sequence elements, in which we will iterate through all the elements of the tensor of values. In each element of the description vector of an element, we will take one value corresponding to the counter of the outer loop iteration and multiply it by the element of the normalized attention coefficient vector according to the counter of the nested loop iteration. After completing the full cycle of iterations in the nested loop, the </span><span class="f_Text" style="font-style: italic;">query</span><span class="f_Text"> variable will contain one value of the description vector for the analyzed element of the attention block sequence. We will write it to the corresponding element of the kernel work results buffer.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">shift_query</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">q</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">query</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">v</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">v</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">v</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">query</span><span class="f_CodeExample">&nbsp;+=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">values</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">v</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">)&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">]&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">scores</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_scores</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">v</span><span class="f_CodeExample">];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">outputs</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_query</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">]&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">query</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After completing the iterations of the outer loop, we will obtain a complete description vector for one element of the sequence in the result tensor buffer.</span></p>
<p class="p_Text"><span class="f_Text">As you can see, the operations of one kernel result in one description vector for an element of the sequence in the result tensor of one attention head. To calculate the complete tensor, we need to launch a task pool with a size equal to the product of the number of elements in the sequence and the number of attention heads. This is what we do when running the kernel in a two-dimensional task space.</span></p>
<p class="p_Text"><span class="f_Text">To transform the kernel from the single-head attention plane to multi-head attention, we simply needed to organize the kernel launch in a two-dimensional space and adjust the offset calculations in the data buffers.</span></p>
<p class="p_Text"><span class="f_Text">Let's do a similar job with backpropagation kernels. As you may recall, in the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> block, in contrast to the implementation of other neural layers, we implemented the propagation of the error gradient through the internal space of the hidden neural layer using two consecutive kernels. So, we need to transfer both kernels into the area of multi-head attention. However, let's consider things in order.</span></p>
<p class="p_Text"><span class="f_Text">First, we will look at the </span><span class="f_Text" style="font-style: italic;">AttentionCalcScoreGradient</span><span class="f_Text"> kernel. The kernel parameters remain unchanged. Here we have the same data buffers and one constant size of the description vector of one element.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">__kernel</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">void</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">AttentionCalcScoreGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">__global</span><span class="f_CodeExample">&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">scores</span><span class="f_CodeExample">,</span><br>
<span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">__global</span><span class="f_CodeExample">&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">scores_grad</span><span class="f_CodeExample">,</span><br>
<span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">__global</span><span class="f_CodeExample">&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">values</span><span class="f_CodeExample">,</span><br>
<span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">__global</span><span class="f_CodeExample">&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">values_grad</span><span class="f_CodeExample">,</span><br>
<span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">__global</span><span class="f_CodeExample">&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">outputs_grad</span><span class="f_CodeExample">,</span><br>
<span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">__global</span><span class="f_CodeExample">&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">scores_temp</span><span class="f_CodeExample">,</span><br>
<span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">)</span><br>
<span class="f_CodeExample">&nbsp;&nbsp;{</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">In the kernel body, similar to the feed-forward kernel, we add the retrieval of thread identification in the second dimension and adjust the calculation of offsets in data buffers accordingly.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">q</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">get_global_id</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">get_global_size</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">get_global_id</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">get_global_size</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">shift_value</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">q</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">shift_score</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">q</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">);</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We do not change the kernel algorithm. As with the implementation of the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm, the kernel can be logically divided into two blocks.</span></p>
<p class="p_Text"><span class="f_Text">In the first algorithm, we distribute the error gradient to the tensor of values </span><span class="f_Text" style="font-style: italic;">Values</span><span class="f_Text">&#8203;&#8203;. Here we create a system of two nested loops. The outer loop will have a number of iterations equal to the size of the description vector of one sequence element in the value tensor. In the loop body, we create a local variable to collect the error gradient of the analyzed element.</span></p>
<p class="p_Text"><span class="f_Text">It should be understood that during the feed-forward pass, each element in the sequence of the value tensor has a significant influence on the value of each element in the sequence of the result tensor. The strength of this influence is determined by the corresponding column of the attention coefficient matrix, where each row corresponds to one element in the sequence tensor of results. Therefore, to obtain the error gradient vector for one element in the sequence tensor of values, we need to multiply the corresponding column of the attention coefficient matrix by the error gradient tensor at the level of the attention block results.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:264px;height:25px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 1056 100"><path d="M30 51 C31 49,31 47,31 45 C31 44,31 43,30 43 C30 42,29 42,28 42 L28 40 L43 40 L42 42 C41 42,41 43,40 43 C40 43,39 43,39 44 C39 44,38 45,38 46 C38 47,37 49,37 51 L34 62 L31 63 L29 60 C27 62,25 62,23 63 C22 63,21 63,19 63 C14 63,11 62,9 59 C6 56,5 53,5 47 C5 44,6 39,7 35 C8 30,10 26,12 23 C14 19,17 16,21 14 C24 12,28 11,32 11 C34 11,36 11,38 12 C40 12,42 12,44 13 L42 23 L38 23 C38 20,38 18,36 16 C35 15,34 14,31 14 C29 14,27 15,25 16 C24 17,22 18,20 21 C19 23,17 25,16 29 C15 32,14 35,13 39 C12 42,12 46,12 49 C12 53,12 56,14 57 C15 59,17 60,20 60 C21 60,22 60,23 60 C24 60,25 59,26 59 C27 58,28 57,28 56 C29 55,30 54,30 53 L30 51 Z" fill="#212121"/><path d="M55 52 L55 54 C55 54,54 54,54 54 C53 55,53 55,53 55 C52 56,52 56,52 56 C52 57,52 58,52 58 C52 58,52 59,52 59 C52 60,52 60,52 61 C52 62,52 64,53 66 C53 68,53 70,53 71 C53 73,53 75,53 76 C53 78,53 79,53 80 L54 80 L64 62 C65 62,65 61,66 60 C66 60,66 59,67 59 C67 58,67 58,67 57 C67 57,67 57,67 56 C67 55,66 54,64 54 L65 52 L76 52 L76 54 C76 54,75 54,75 54 C75 54,74 55,74 55 C73 55,73 56,72 57 C72 57,71 58,70 60 L53 87 L49 87 C49 82,48 78,48 73 C48 69,47 64,47 59 C47 58,47 58,47 57 C46 56,46 56,46 56 C46 55,45 55,45 55 C45 54,44 54,43 54 L44 52 L55 52 Z" fill="#212121"/><path d="M98 80 C97 81,96 82,95 83 C94 84,93 85,92 85 C92 86,91 86,90 87 C89 87,88 87,87 87 C85 87,83 86,82 85 C81 83,80 81,80 78 C80 77,81 75,81 74 C81 72,82 71,83 69 C83 68,84 67,85 65 C86 64,87 63,88 63 C89 62,90 61,92 61 C93 60,94 60,95 60 C97 60,98 60,99 61 C100 61,101 61,102 62 L105 60 L107 61 L103 77 C103 78,103 79,102 80 C102 80,102 81,102 81 C102 82,102 82,103 83 C103 83,103 83,104 83 C104 83,105 83,105 83 C105 83,106 83,106 82 C106 82,107 82,107 81 C108 81,108 80,109 80 C109 80,109 80,109 81 C110 81,110 81,110 82 C109 83,109 83,108 84 C107 85,106 85,106 86 C105 86,104 87,104 87 C103 87,102 87,102 87 C100 87,99 87,99 86 C98 85,98 84,98 83 C98 82,98 81,98 80 L98 80 Z M99 72 C100 71,100 70,100 69 C100 68,100 68,100 67 C100 65,100 65,99 64 C98 63,97 63,96 63 C95 63,94 63,93 64 C92 64,91 65,90 66 C90 66,89 67,88 68 C88 69,87 70,87 71 C87 72,86 73,86 75 C86 76,86 77,86 78 C86 80,86 81,87 82 C87 83,88 83,89 83 C90 83,90 83,91 83 C92 83,92 82,93 82 C94 81,96 79,97 78 C98 76,99 74,99 72 L99 72 Z" fill="#212121"/><path d="M116 76 C117 73,118 70,119 66 C119 63,120 60,120 58 C121 56,121 55,121 54 C121 54,121 53,121 53 C121 53,121 53,120 53 C120 52,120 52,119 52 C119 52,118 52,118 52 L118 50 L125 50 L127 50 C126 55,125 60,124 64 C123 68,122 72,121 77 C121 78,121 79,121 80 C121 80,120 81,120 81 C120 82,121 82,121 83 C121 83,122 83,122 83 C123 83,123 83,123 83 C124 83,124 83,124 82 C125 82,125 82,125 81 C126 81,126 80,127 80 L129 81 C128 83,127 83,126 84 C125 85,125 85,124 86 C123 86,123 87,122 87 C121 87,121 87,120 87 C119 87,119 87,118 87 C118 86,117 86,117 86 C116 85,116 85,116 84 C116 84,115 83,115 82 C115 81,116 81,116 80 C116 79,116 78,116 76 L116 76 Z" fill="#212121"/><path d="M152 80 C150 83,148 84,147 85 C145 87,143 87,141 87 C140 87,138 87,137 86 C136 85,136 83,136 82 C136 81,136 81,136 80 C136 79,136 78,137 77 C137 76,137 75,137 74 C138 73,138 72,138 71 C138 70,139 69,139 68 C139 67,139 66,139 66 C139 65,139 64,138 64 C137 64,137 64,137 64 C136 64,136 65,135 65 C135 65,135 66,134 66 C134 66,133 67,133 67 L131 66 C132 65,133 64,134 63 C134 62,135 62,136 61 C136 61,137 61,138 60 C138 60,139 60,140 60 C141 60,142 61,143 61 C144 62,144 63,144 65 C144 65,144 66,144 67 C144 68,144 69,143 70 C143 72,142 75,142 76 C141 78,141 79,141 80 C141 81,141 82,142 82 C142 83,143 83,144 83 C145 83,145 83,146 82 C147 82,148 81,149 80 C150 79,151 78,152 77 C152 75,153 74,153 73 L156 60 L161 60 L157 77 C157 78,157 79,157 79 C157 80,157 81,157 81 C157 82,157 82,157 83 C157 83,158 83,158 83 C158 83,159 83,159 83 C159 83,160 83,160 82 C161 82,161 82,161 81 C162 81,162 80,163 80 L165 81 C164 83,163 83,162 84 C161 85,161 85,160 86 C159 86,159 87,158 87 C157 87,157 87,156 87 C155 87,154 87,153 86 C152 85,152 84,152 83 C152 82,152 82,152 81 C152 81,152 81,152 80 L152 80 Z" fill="#212121"/><path d="M191 81 C189 83,187 85,185 86 C183 87,181 87,178 87 C177 87,176 87,174 86 C173 86,172 86,172 85 C171 84,170 83,170 82 C170 81,169 80,169 78 C169 77,170 76,170 75 C170 74,170 73,171 72 C171 71,171 70,172 69 C173 68,173 67,174 66 C175 64,177 63,179 62 C181 61,184 60,186 60 C189 60,190 61,192 62 C193 63,193 64,193 66 C193 72,187 75,175 75 C175 76,175 76,175 77 C175 77,175 78,175 78 C175 80,175 82,176 83 C177 84,178 84,180 84 C180 84,181 84,182 84 C183 84,184 83,184 83 C185 83,186 82,187 81 C187 81,188 80,189 79 L191 81 Z M175 72 C178 72,180 72,181 72 C183 71,184 71,185 70 C186 70,187 69,188 69 C188 68,188 67,188 66 C188 65,188 65,188 64 C187 63,186 63,185 63 C184 63,183 63,182 64 C181 64,180 65,180 66 C179 66,178 67,177 68 C177 69,176 71,175 72 L175 72 Z" fill="#212121"/><path d="M223 36 L223 31 L269 31 L269 36 L223 36 Z M223 51 L223 46 L269 46 L269 51 L223 51 Z" fill="#212121"/><path d="M301 51 C301 54,302 56,303 58 C305 59,307 60,310 60 C313 60,316 59,318 57 C319 55,320 53,320 49 C320 48,320 46,320 45 C319 44,318 43,317 42 C316 41,315 40,313 38 C311 37,309 36,308 34 C307 33,306 32,305 31 C305 29,304 27,304 26 C304 23,305 20,306 18 C308 16,310 14,312 13 C315 12,318 11,321 11 C323 11,325 11,327 12 C329 12,331 12,334 13 L332 22 L328 22 C328 20,328 19,327 17 C327 16,326 15,325 15 C324 14,322 14,321 14 C319 14,317 15,315 15 C314 16,313 17,312 19 C311 20,311 22,311 24 C311 25,311 27,312 28 C313 30,315 32,318 33 C320 35,322 37,323 38 C324 39,325 41,326 42 C327 44,327 46,327 47 C327 51,326 53,325 56 C323 58,321 60,319 61 C316 62,313 63,310 63 C308 63,305 63,303 63 C300 62,298 62,296 61 L298 51 L301 51 Z" fill="#212121"/><path d="M363 36 C363 34,362 33,362 32 C362 31,361 30,361 30 C360 29,360 29,358 29 C356 29,354 30,352 32 C350 34,348 37,347 41 C346 45,345 48,345 51 C345 54,346 56,347 57 C348 58,349 59,351 59 C353 59,355 59,357 58 C358 57,360 56,362 54 L364 56 C362 59,360 60,357 61 C355 63,352 63,350 63 C346 63,343 62,342 60 C340 58,339 55,339 50 C339 48,339 45,340 42 C341 39,343 36,345 33 C346 31,349 29,351 28 C354 27,356 26,359 26 C363 26,365 26,368 27 L366 36 L363 36 Z" fill="#212121"/><path d="M386 63 C382 63,379 62,377 60 C375 58,374 55,374 50 C374 49,374 46,375 44 C376 40,377 37,379 34 C381 32,383 30,385 28 C388 27,391 26,394 26 C398 26,401 27,403 29 C405 32,406 35,406 39 C406 42,405 45,404 48 C403 51,402 54,400 56 C399 59,397 60,394 61 C392 63,389 63,386 63 L386 63 Z M380 52 C380 55,381 57,382 58 C383 59,385 60,387 60 C390 60,392 59,394 57 C395 55,397 52,398 48 C399 44,400 40,400 37 C400 35,399 33,398 31 C397 30,395 29,393 29 C391 29,388 30,387 32 C385 35,383 38,382 42 C381 46,380 49,380 52 L380 52 Z" fill="#212121"/><path d="M426 36 C429 32,431 30,433 28 C435 27,438 26,440 26 C442 26,443 26,444 26 L443 35 L439 35 C439 34,438 33,438 33 C438 32,438 32,437 31 C437 31,436 31,436 31 C435 31,434 31,433 32 C432 33,431 34,429 36 C428 37,427 39,426 40 C426 42,425 44,424 46 L421 62 L415 62 L420 38 C420 37,421 36,421 35 C421 34,421 34,421 33 C421 32,421 31,420 31 C420 30,420 30,419 30 C418 30,417 30,416 31 C416 32,414 33,413 34 L411 32 C413 30,415 28,417 27 C418 27,420 26,422 26 C423 26,424 27,425 28 C426 29,426 30,426 31 C426 33,426 34,426 35 L426 36 Z" fill="#212121"/><path d="M477 55 C474 58,472 60,469 61 C467 62,464 63,461 63 C457 63,455 62,453 60 C451 58,450 55,450 51 C450 48,450 45,451 42 C452 39,454 36,456 34 C457 31,460 29,462 28 C465 27,468 26,471 26 C474 26,476 27,478 28 C479 29,480 31,480 34 C480 38,478 41,474 43 C470 45,464 46,457 46 C456 48,456 49,456 51 C456 54,457 56,458 57 C459 58,460 59,463 59 C465 59,467 59,469 58 C470 57,472 55,474 53 L477 55 Z M457 43 C461 43,464 43,466 42 C469 42,470 41,472 39 C473 38,474 36,474 34 C474 32,473 31,473 30 C472 30,471 29,470 29 C467 29,465 30,462 33 C460 35,458 39,457 43 L457 43 Z" fill="#212121"/><path d="M490 35 L491 33 C491 33,491 33,492 33 C492 33,492 33,493 33 C493 32,493 32,493 32 C493 32,494 31,494 31 C494 31,494 30,494 29 C494 29,495 28,495 27 L500 3 L496 3 C496 3,495 3,495 3 C495 3,494 3,494 3 C493 4,493 4,493 4 C493 4,492 5,492 5 C492 6,491 6,491 7 C491 8,490 8,490 9 L487 9 L489 0 L518 0 L516 10 L513 10 L513 8 C513 8,513 8,513 7 C513 7,513 7,513 6 C513 6,513 5,513 5 C513 5,513 5,513 4 C512 4,512 4,512 4 C512 4,512 3,512 3 C511 3,511 3,511 3 C511 3,510 3,510 3 L505 3 L500 27 C500 28,500 29,500 29 C499 30,499 31,499 31 C499 31,499 32,500 32 C500 32,500 32,500 33 C500 33,501 33,501 33 C501 33,502 33,502 33 L502 35 L490 35 Z" fill="#212121"/><path d="M570 46 L568 50 L558 43 L560 55 L555 55 L556 43 L546 50 L544 46 L554 41 L544 36 L546 32 L556 39 L555 27 L560 27 L558 39 L568 32 L570 36 L560 41 L570 46 Z" fill="#212121"/><path d="M622 51 C623 49,623 47,623 45 C623 44,623 43,622 43 C622 42,621 42,620 42 L620 40 L635 40 L634 42 C633 42,633 43,632 43 C632 43,631 43,631 44 C631 44,630 45,630 46 C630 47,629 49,629 51 L626 62 L623 63 L621 60 C619 62,617 62,615 63 C614 63,613 63,611 63 C606 63,603 62,601 59 C598 56,597 53,597 47 C597 44,598 39,599 35 C600 30,602 26,604 23 C606 19,609 16,613 14 C616 12,620 11,624 11 C626 11,628 11,630 12 C632 12,634 12,636 13 L634 23 L630 23 C630 20,630 18,628 16 C627 15,626 14,623 14 C621 14,619 15,617 16 C616 17,614 18,612 21 C611 23,609 25,608 29 C607 32,606 35,605 39 C604 42,604 46,604 49 C604 53,604 56,606 57 C607 59,609 60,612 60 C613 60,614 60,615 60 C616 60,617 59,618 59 C619 58,620 57,620 56 C621 55,622 54,622 53 L622 51 Z" fill="#212121"/><path d="M641 80 C641 82,641 84,642 85 C643 86,645 86,647 86 C648 86,650 86,650 86 C651 86,652 85,653 84 C653 84,654 83,654 82 C655 82,655 81,655 80 C655 79,655 78,655 78 C654 77,654 76,654 76 C653 75,653 75,652 74 C651 73,650 73,649 72 C648 71,647 71,646 70 C645 69,645 69,644 68 C643 67,643 66,643 66 C642 65,642 64,642 63 C642 62,643 60,643 59 C644 58,645 57,646 56 C647 55,648 54,650 54 C651 53,653 53,655 53 C657 53,658 53,660 53 C661 54,663 54,665 55 L663 61 L660 61 C660 60,660 59,660 58 C660 58,659 57,659 57 C658 56,658 56,657 56 C656 56,655 55,654 55 C653 55,652 56,652 56 C651 56,650 57,649 57 C649 58,648 58,648 59 C648 60,647 60,647 61 C647 62,648 63,648 63 C648 64,648 64,649 65 C649 65,649 66,650 67 C651 67,652 68,653 68 C654 69,655 70,656 71 C657 71,658 72,658 73 C659 74,659 75,660 75 C660 76,660 77,660 78 C660 80,660 81,659 83 C658 84,658 85,656 86 C655 87,654 88,652 88 C651 89,649 89,647 89 C645 89,644 89,642 89 C640 88,638 88,636 87 L638 80 L641 80 Z" fill="#212121"/><path d="M690 82 C688 84,686 86,684 87 C682 88,680 88,677 88 C676 88,675 88,673 87 C672 87,671 87,671 86 C670 85,669 84,669 83 C669 82,668 81,668 79 C668 78,669 77,669 76 C669 75,669 74,670 73 C670 72,670 71,671 70 C672 69,672 68,673 67 C674 65,676 64,678 63 C680 62,683 61,685 61 C688 61,689 62,691 63 C692 64,692 65,692 67 C692 73,686 76,674 76 C674 77,674 77,674 78 C674 78,674 79,674 79 C674 81,674 83,675 84 C676 85,677 85,679 85 C679 85,680 85,681 85 C682 85,683 84,683 84 C684 84,685 83,686 82 C686 82,687 81,688 80 L690 82 Z M674 73 C677 73,679 73,680 73 C682 72,683 72,684 71 C685 71,686 70,687 70 C687 69,687 68,687 67 C687 66,687 66,687 65 C686 64,685 64,684 64 C683 64,682 64,681 65 C680 65,679 66,679 67 C678 67,677 68,676 69 C676 70,675 72,674 73 L674 73 Z" fill="#212121"/><path d="M700 77 C701 74,702 71,703 67 C703 64,704 61,704 59 C705 57,705 56,705 55 C705 55,705 54,705 54 C705 54,705 54,704 54 C704 53,704 53,703 53 C703 53,702 53,702 53 L702 51 L709 51 L711 51 C710 56,709 61,708 65 C707 69,706 73,705 78 C705 79,705 80,705 81 C705 81,704 82,704 82 C704 83,705 83,705 84 C705 84,706 84,706 84 C707 84,707 84,707 84 C708 84,708 84,708 83 C709 83,709 83,709 82 C710 82,710 81,711 81 L713 82 C712 84,711 84,710 85 C709 86,709 86,708 87 C707 87,707 88,706 88 C705 88,705 88,704 88 C703 88,703 88,702 88 C702 87,701 87,701 87 C700 86,700 86,700 85 C700 85,699 84,699 83 C699 82,700 82,700 81 C700 80,700 79,700 77 L700 77 Z" fill="#212121"/><path d="M728 87 C727 91,726 94,724 96 C722 98,719 99,716 99 C715 99,715 99,714 99 C714 98,715 98,715 97 C715 97,715 97,715 96 C715 96,715 96,715 96 C716 96,716 96,716 96 C717 96,718 96,718 96 C719 96,719 95,720 95 C720 94,721 93,721 92 C722 91,722 90,722 88 L728 64 L723 64 L723 62 C724 62,725 62,726 62 C726 62,727 62,727 61 C728 61,728 61,728 60 C729 57,731 55,733 53 C735 52,738 51,741 51 C742 51,743 51,744 51 C744 51,745 51,746 52 L745 57 L742 57 C742 56,742 55,741 55 C741 54,740 54,739 54 C738 54,738 54,737 55 C736 55,736 56,735 56 C735 57,734 58,734 59 C734 60,733 60,733 61 L741 61 L740 64 L733 64 L728 87 Z" fill="#212121"/><path d="M753 74 L753 70 L783 70 L783 74 L753 74 Z" fill="#212121"/><path d="M810 88 L810 86 C811 86,811 86,811 86 C812 86,812 85,812 85 C813 85,813 84,813 84 C813 84,813 83,813 82 C813 82,813 81,813 81 C813 81,813 80,813 80 C813 79,813 79,813 78 C813 78,813 78,813 77 L801 77 C800 78,800 78,800 79 C799 79,799 80,799 81 C798 81,798 82,798 82 C798 83,797 83,797 84 C797 85,798 86,800 86 L800 88 L788 88 L789 86 C789 86,789 86,790 86 C790 86,791 85,791 85 C791 84,792 84,792 83 C793 83,794 82,794 80 L811 53 L816 53 L818 81 C818 82,819 83,819 83 C819 84,819 84,819 85 C820 85,820 85,820 86 C821 86,821 86,822 86 L821 88 L810 88 Z M812 59 C810 61,809 64,807 66 C805 69,804 72,802 74 L813 74 C813 73,813 72,812 71 C812 70,812 69,812 67 C812 66,812 65,812 63 C812 62,812 61,812 59 L812 59 Z" fill="#212121"/><path d="M845 82 C844 83,843 84,842 85 C841 86,841 86,840 87 C839 87,838 88,837 88 C837 88,836 88,835 88 C833 88,831 88,830 87 C829 86,829 84,829 82 C829 82,829 81,829 80 C829 79,829 79,829 78 L832 64 L828 64 L828 62 C829 62,830 62,831 62 C832 62,832 61,832 61 C833 61,833 60,833 60 C833 60,834 59,834 59 C834 58,834 57,835 57 C835 56,835 55,836 54 L840 54 L838 61 L847 61 L847 64 L838 64 L835 75 C835 76,835 76,834 77 C834 78,834 78,834 78 C834 79,834 79,834 80 C834 80,834 80,834 81 C834 82,834 83,835 83 C835 84,836 84,837 84 C838 84,839 84,840 83 C841 83,842 82,843 80 L845 82 Z" fill="#212121"/><path d="M869 82 C868 83,867 84,866 85 C865 86,865 86,864 87 C863 87,862 88,861 88 C861 88,860 88,859 88 C857 88,855 88,854 87 C853 86,853 84,853 82 C853 82,853 81,853 80 C853 79,853 79,853 78 L856 64 L852 64 L852 62 C853 62,854 62,855 62 C856 62,856 61,856 61 C857 61,857 60,857 60 C857 60,858 59,858 59 C858 58,858 57,859 57 C859 56,859 55,860 54 L864 54 L862 61 L871 61 L871 64 L862 64 L859 75 C859 76,859 76,858 77 C858 78,858 78,858 78 C858 79,858 79,858 80 C858 80,858 80,858 81 C858 82,858 83,859 83 C859 84,860 84,861 84 C862 84,863 84,864 83 C865 83,866 82,867 80 L869 82 Z" fill="#212121"/><path d="M897 82 C895 84,893 86,891 87 C889 88,887 88,884 88 C883 88,882 88,880 87 C879 87,878 87,878 86 C877 85,876 84,876 83 C876 82,875 81,875 79 C875 78,876 77,876 76 C876 75,876 74,877 73 C877 72,877 71,878 70 C879 69,879 68,880 67 C881 65,883 64,885 63 C887 62,890 61,892 61 C895 61,896 62,898 63 C899 64,899 65,899 67 C899 73,893 76,881 76 C881 77,881 77,881 78 C881 78,881 79,881 79 C881 81,881 83,882 84 C883 85,884 85,886 85 C886 85,887 85,888 85 C889 85,890 84,890 84 C891 84,892 83,893 82 C893 82,894 81,895 80 L897 82 Z M881 73 C884 73,886 73,887 73 C889 72,890 72,891 71 C892 71,893 70,894 70 C894 69,894 68,894 67 C894 66,894 66,894 65 C893 64,892 64,891 64 C890 64,889 64,888 65 C887 65,886 66,886 67 C885 67,884 68,883 69 C883 70,882 72,881 73 L881 73 Z" fill="#212121"/><path d="M916 68 C918 66,919 64,921 63 C923 62,925 61,926 61 C928 61,930 62,931 63 C932 64,932 65,932 67 C932 67,932 68,932 69 C932 70,931 71,931 72 C931 73,931 74,930 75 C930 76,930 78,929 78 C929 79,929 80,929 81 C929 82,929 82,929 82 C929 83,929 83,929 84 C929 84,930 84,930 84 C931 84,931 84,931 84 C932 84,932 84,932 83 C933 83,933 83,933 82 C934 82,934 81,935 81 L937 82 C936 84,935 84,934 85 C934 86,933 86,932 87 C932 87,931 88,930 88 C929 88,929 88,928 88 C927 88,927 88,926 88 C926 88,925 87,925 87 C924 86,924 86,924 85 C924 85,924 84,924 83 C924 83,924 82,924 81 C924 80,924 79,924 78 C925 77,925 76,925 75 C926 74,926 73,926 72 C926 71,926 71,927 70 C927 69,927 69,927 68 C927 67,927 66,926 66 C926 65,925 65,924 65 C924 65,923 65,922 65 C922 66,921 66,920 67 C920 67,919 68,919 68 C918 69,917 70,917 70 C916 71,916 72,916 73 C915 74,915 74,915 75 L912 88 L907 88 L911 71 C911 70,911 70,911 69 C911 68,911 67,911 67 C911 66,911 65,910 65 C909 65,909 65,909 65 C908 65,908 66,908 66 C907 66,907 67,906 67 C906 67,906 68,905 68 L903 67 C904 66,905 65,906 64 C906 63,907 63,908 62 C908 62,909 62,910 61 C910 61,911 61,912 61 C913 61,914 62,915 62 C916 63,916 64,916 66 C916 66,916 67,916 67 C916 67,916 68,916 68 L916 68 Z" fill="#212121"/><path d="M959 82 C958 83,957 84,956 85 C955 86,955 86,954 87 C953 87,952 88,951 88 C951 88,950 88,949 88 C947 88,945 88,944 87 C943 86,943 84,943 82 C943 82,943 81,943 80 C943 79,943 79,943 78 L946 64 L942 64 L942 62 C943 62,944 62,945 62 C946 62,946 61,946 61 C947 61,947 60,947 60 C947 60,948 59,948 59 C948 58,948 57,949 57 C949 56,949 55,950 54 L954 54 L952 61 L961 61 L961 64 L952 64 L949 75 C949 76,949 76,948 77 C948 78,948 78,948 78 C948 79,948 79,948 80 C948 80,948 80,948 81 C948 82,948 83,949 83 C949 84,950 84,951 84 C952 84,953 84,954 83 C955 83,956 82,957 80 L959 82 Z" fill="#212121"/><path d="M979 50 L977 56 L972 56 L973 50 L979 50 Z M972 78 C972 79,972 80,972 81 C972 81,972 82,972 82 C972 83,972 83,972 84 C972 84,973 84,973 84 C974 84,974 84,974 84 C975 84,975 84,975 83 C976 83,976 83,976 82 C977 82,977 81,978 81 L980 83 C979 84,978 85,977 85 C976 86,976 86,975 87 C974 87,974 88,973 88 C972 88,972 88,971 88 C970 88,970 88,969 88 C969 88,968 87,968 87 C967 86,967 86,967 85 C967 85,966 84,966 83 C966 83,967 82,967 81 C967 80,967 79,967 78 C968 77,968 76,968 75 C968 74,969 73,969 72 C969 71,969 70,969 69 C970 68,970 67,970 66 C970 66,970 66,970 65 C970 65,970 65,970 65 C970 64,970 64,969 64 C969 63,968 63,967 63 L967 61 L975 61 L976 61 L972 78 Z" fill="#212121"/><path d="M994 88 C993 88,991 88,990 87 C989 87,988 86,987 86 C986 85,986 84,985 83 C985 82,984 80,984 79 C984 76,985 74,986 72 C987 70,988 68,989 66 C990 65,992 63,994 62 C996 62,998 61,1000 61 C1002 61,1003 61,1004 62 C1006 62,1007 63,1008 64 C1008 65,1009 66,1009 67 C1010 68,1010 69,1010 71 C1010 73,1010 76,1009 78 C1008 80,1007 82,1005 83 C1004 85,1002 86,1000 87 C998 88,996 88,994 88 L994 88 Z M990 79 C990 81,990 83,991 84 C992 85,993 85,995 85 C996 85,997 85,998 84 C999 84,999 83,1000 83 C1001 82,1001 81,1002 80 C1003 79,1003 78,1003 77 C1004 76,1004 75,1004 73 C1005 72,1005 71,1005 70 C1005 68,1004 67,1003 66 C1003 65,1001 64,1000 64 C999 64,998 64,997 65 C996 65,995 66,995 66 C994 67,993 68,993 69 C992 70,991 71,991 72 C991 73,990 75,990 76 C990 77,990 78,990 79 L990 79 Z" fill="#212121"/><path d="M1027 68 C1029 66,1030 64,1032 63 C1034 62,1036 61,1037 61 C1039 61,1041 62,1042 63 C1043 64,1043 65,1043 67 C1043 67,1043 68,1043 69 C1043 70,1042 71,1042 72 C1042 73,1042 74,1041 75 C1041 76,1041 78,1040 78 C1040 79,1040 80,1040 81 C1040 82,1040 82,1040 82 C1040 83,1040 83,1040 84 C1040 84,1041 84,1041 84 C1042 84,1042 84,1042 84 C1043 84,1043 84,1043 83 C1044 83,1044 83,1044 82 C1045 82,1045 81,1046 81 L1048 82 C1047 84,1046 84,1045 85 C1045 86,1044 86,1043 87 C1043 87,1042 88,1041 88 C1040 88,1040 88,1039 88 C1038 88,1038 88,1037 88 C1037 88,1036 87,1036 87 C1035 86,1035 86,1035 85 C1035 85,1035 84,1035 83 C1035 83,1035 82,1035 81 C1035 80,1035 79,1035 78 C1036 77,1036 76,1036 75 C1037 74,1037 73,1037 72 C1037 71,1037 71,1038 70 C1038 69,1038 69,1038 68 C1038 67,1038 66,1037 66 C1037 65,1036 65,1035 65 C1035 65,1034 65,1033 65 C1033 66,1032 66,1031 67 C1031 67,1030 68,1030 68 C1029 69,1028 70,1028 70 C1027 71,1027 72,1027 73 C1026 74,1026 74,1026 75 L1023 88 L1018 88 L1022 71 C1022 70,1022 70,1022 69 C1022 68,1022 67,1022 67 C1022 66,1022 65,1021 65 C1020 65,1020 65,1020 65 C1019 65,1019 66,1019 66 C1018 66,1018 67,1017 67 C1017 67,1017 68,1016 68 L1014 67 C1015 66,1016 65,1017 64 C1017 63,1018 63,1019 62 C1019 62,1020 62,1021 61 C1021 61,1022 61,1023 61 C1024 61,1025 62,1026 62 C1027 63,1027 64,1027 66 C1027 66,1027 67,1027 67 C1027 67,1027 68,1027 68 L1027 68 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">To perform this operation, we organize a nested loop with the number of iterations equal to the number of elements in the sequence. In the body of this loop, we will multiply two vectors and write the result to the corresponding element of the error gradient buffer of the value tensor.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;Distributing&nbsp;the&nbsp;gradient&nbsp;on&nbsp;Values</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">&nbsp;+=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">scores</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">)&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">q</span><span class="f_CodeExample">]&nbsp;*</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;outputs_grad</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">g</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">)&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">values_grad</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_value</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">]&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Here, we made changes only in terms of determining the offsets to the analyzed elements in the data buffers.</span></p>
<p class="p_Text"><span class="f_Text">The second block of this kernel is responsible for propagating the gradient to the level of the dependency coefficient matrix. First, we create a system of two nested loops and calculate the error gradient for one row of the dependency coefficient matrix. There is a very important moment here. We calculate the error gradient specifically for a matrix row, not a column. The normalization of the matrix with the </span><span class="f_Text" style="font-style: italic;">Softmax </span><span class="f_Text">function was performed row-wise, so we should also adjust it row-wise with respect to the </span><span class="f_Text" style="font-style: italic;">Softmax</span><span class="f_Text"> derivative. To determine the error gradient for one row of the matrix, we need to take the corresponding vector from the error gradient tensor at the level of attention block results and multiply it by the key tensor of the corresponding attention head.</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:228px;height:27px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 912 108"><path d="M30 57 C31 55,31 53,31 51 C31 50,31 49,30 49 C30 48,29 48,28 48 L28 46 L43 46 L42 48 C41 48,41 49,40 49 C40 49,39 49,39 50 C39 50,38 51,38 52 C38 53,37 55,37 57 L34 68 L31 69 L29 66 C27 68,25 68,23 69 C22 69,21 69,19 69 C14 69,11 68,9 65 C6 62,5 59,5 53 C5 50,6 45,7 41 C8 36,10 32,12 29 C14 25,17 22,21 20 C24 18,28 17,32 17 C34 17,36 17,38 18 C40 18,42 18,44 19 L42 29 L38 29 C38 26,38 24,36 22 C35 21,34 20,31 20 C29 20,27 21,25 22 C24 23,22 24,20 27 C19 29,17 31,16 35 C15 38,14 41,13 45 C12 48,12 52,12 55 C12 59,12 62,14 63 C15 65,17 66,20 66 C21 66,22 66,23 66 C24 66,25 65,26 65 C27 64,28 63,28 62 C29 61,30 60,30 59 L30 57 Z" fill="#212121"/><path d="M49 83 C49 85,49 87,50 88 C51 89,53 89,55 89 C56 89,58 89,58 89 C59 89,60 88,61 87 C61 87,62 86,62 85 C63 85,63 84,63 83 C63 82,63 81,63 81 C62 80,62 79,62 79 C61 78,61 78,60 77 C59 76,58 76,57 75 C56 74,55 74,54 73 C53 72,53 72,52 71 C51 70,51 69,51 69 C50 68,50 67,50 66 C50 65,51 63,51 62 C52 61,53 60,54 59 C55 58,56 57,58 57 C59 56,61 56,63 56 C65 56,66 56,68 56 C69 57,71 57,73 58 L71 64 L68 64 C68 63,68 62,68 61 C68 61,67 60,67 60 C66 59,66 59,65 59 C64 59,63 58,62 58 C61 58,60 59,60 59 C59 59,58 60,57 60 C57 61,56 61,56 62 C56 63,55 63,55 64 C55 65,56 66,56 66 C56 67,56 67,57 68 C57 68,57 69,58 70 C59 70,60 71,61 71 C62 72,63 73,64 74 C65 74,66 75,66 76 C67 77,67 78,68 78 C68 79,68 80,68 81 C68 83,68 84,67 86 C66 87,66 88,64 89 C63 90,62 91,60 91 C59 92,57 92,55 92 C53 92,52 92,50 92 C48 91,46 91,44 90 L46 83 L49 83 Z" fill="#212121"/><path d="M95 72 C95 71,95 70,95 70 C95 69,95 69,95 68 C94 68,94 68,94 67 C93 67,93 67,92 67 C91 67,90 67,89 68 C88 68,88 69,87 69 C86 70,85 71,85 72 C84 73,84 74,83 75 C83 76,82 78,82 79 C82 80,82 81,82 82 C82 84,82 86,83 87 C84 88,85 88,86 88 C87 88,88 88,89 88 C89 88,90 87,91 87 C91 87,92 86,93 86 C93 85,94 84,95 83 C95 84,96 84,96 85 C96 85,97 85,97 86 C95 87,93 89,91 90 C89 91,87 91,85 91 C82 91,80 90,79 89 C77 87,76 85,76 82 C76 80,77 79,77 77 C77 76,78 74,79 73 C79 72,80 71,81 69 C82 68,83 67,84 67 C86 66,87 65,88 65 C90 64,91 64,93 64 C94 64,95 64,97 64 C98 64,99 65,100 65 L98 72 L95 72 Z" fill="#212121"/><path d="M114 91 C113 91,111 91,110 90 C109 90,108 89,107 89 C106 88,106 87,105 86 C105 85,104 83,104 82 C104 79,105 77,106 75 C107 73,108 71,109 69 C110 68,112 66,114 65 C116 65,118 64,120 64 C122 64,123 64,124 65 C126 65,127 66,128 67 C128 68,129 69,129 70 C130 71,130 72,130 74 C130 76,130 79,129 81 C128 83,127 85,125 86 C124 88,122 89,120 90 C118 91,116 91,114 91 L114 91 Z M110 82 C110 84,110 86,111 87 C112 88,113 88,115 88 C116 88,117 88,118 87 C119 87,119 86,120 86 C121 85,121 84,122 83 C123 82,123 81,123 80 C124 79,124 78,124 76 C125 75,125 74,125 73 C125 71,124 70,123 69 C123 68,121 67,120 67 C119 67,118 67,117 68 C116 68,115 69,115 69 C114 70,113 71,113 72 C112 73,111 74,111 75 C111 76,110 78,110 79 C110 80,110 81,110 82 L110 82 Z" fill="#212121"/><path d="M142 74 C142 73,142 72,142 72 C142 71,142 70,142 70 C142 69,142 68,141 68 C140 68,140 68,140 68 C139 68,139 69,139 69 C138 69,138 70,137 70 C137 70,137 71,136 71 L134 70 C135 69,136 68,137 67 C137 66,138 66,139 65 C139 65,140 65,141 64 C141 64,142 64,143 64 C144 64,145 65,146 65 C147 66,147 67,147 69 C147 70,147 70,147 71 L147 71 C149 69,151 67,152 66 C154 65,156 64,158 64 C158 64,159 64,159 64 C160 64,161 64,161 64 L160 71 L157 71 C157 70,156 69,156 69 C156 68,155 68,154 68 C154 68,154 68,153 68 C152 69,152 69,151 70 C151 70,150 71,149 71 C149 72,148 73,148 74 C147 74,147 75,146 76 C146 77,146 78,146 79 L143 91 L138 91 L142 74 Z" fill="#212121"/><path d="M187 85 C185 87,183 89,181 90 C179 91,177 91,174 91 C173 91,172 91,170 90 C169 90,168 90,168 89 C167 88,166 87,166 86 C166 85,165 84,165 82 C165 81,166 80,166 79 C166 78,166 77,167 76 C167 75,167 74,168 73 C169 72,169 71,170 70 C171 68,173 67,175 66 C177 65,180 64,182 64 C185 64,186 65,188 66 C189 67,189 68,189 70 C189 76,183 79,171 79 C171 80,171 80,171 81 C171 81,171 82,171 82 C171 84,171 86,172 87 C173 88,174 88,176 88 C176 88,177 88,178 88 C179 88,180 87,180 87 C181 87,182 86,183 85 C183 85,184 84,185 83 L187 85 Z M171 76 C174 76,176 76,177 76 C179 75,180 75,181 74 C182 74,183 73,184 73 C184 72,184 71,184 70 C184 69,184 69,184 68 C183 67,182 67,181 67 C180 67,179 67,178 68 C177 68,176 69,176 70 C175 70,174 71,173 72 C173 73,172 75,171 76 L171 76 Z" fill="#212121"/><path d="M219 42 L219 37 L265 37 L265 42 L219 42 Z M219 57 L219 52 L265 52 L265 57 L219 57 Z" fill="#212121"/><path d="M321 57 C322 55,322 53,322 51 C322 50,322 49,321 49 C321 48,320 48,319 48 L319 46 L334 46 L333 48 C332 48,332 49,331 49 C331 49,330 49,330 50 C330 50,329 51,329 52 C329 53,328 55,328 57 L325 68 L322 69 L320 66 C318 68,316 68,314 69 C313 69,312 69,310 69 C305 69,302 68,300 65 C297 62,296 59,296 53 C296 50,297 45,298 41 C299 36,301 32,303 29 C305 25,308 22,312 20 C315 18,319 17,323 17 C325 17,327 17,329 18 C331 18,333 18,335 19 L333 29 L329 29 C329 26,329 24,327 22 C326 21,325 20,322 20 C320 20,318 21,316 22 C315 23,313 24,311 27 C310 29,308 31,307 35 C306 38,305 41,304 45 C303 48,303 52,303 55 C303 59,303 62,305 63 C306 65,308 66,311 66 C312 66,313 66,314 66 C315 66,316 65,317 65 C318 64,319 63,319 62 C320 61,321 60,321 59 L321 57 Z" fill="#212121"/><path d="M340 86 C340 88,340 90,341 91 C342 92,344 92,346 92 C347 92,349 92,349 92 C350 92,351 91,352 90 C352 90,353 89,353 88 C354 88,354 87,354 86 C354 85,354 84,354 84 C353 83,353 82,353 82 C352 81,352 81,351 80 C350 79,349 79,348 78 C347 77,346 77,345 76 C344 75,344 75,343 74 C342 73,342 72,342 72 C341 71,341 70,341 69 C341 68,342 66,342 65 C343 64,344 63,345 62 C346 61,347 60,349 60 C350 59,352 59,354 59 C356 59,357 59,359 59 C360 60,362 60,364 61 L362 67 L359 67 C359 66,359 65,359 64 C359 64,358 63,358 63 C357 62,357 62,356 62 C355 62,354 61,353 61 C352 61,351 62,351 62 C350 62,349 63,348 63 C348 64,347 64,347 65 C347 66,346 66,346 67 C346 68,347 69,347 69 C347 70,347 70,348 71 C348 71,348 72,349 73 C350 73,351 74,352 74 C353 75,354 76,355 77 C356 77,357 78,357 79 C358 80,358 81,359 81 C359 82,359 83,359 84 C359 86,359 87,358 89 C357 90,357 91,355 92 C354 93,353 94,351 94 C350 95,348 95,346 95 C344 95,343 95,341 95 C339 94,337 94,335 93 L337 86 L340 86 Z" fill="#212121"/><path d="M389 88 C387 90,385 92,383 93 C381 94,379 94,376 94 C375 94,374 94,372 93 C371 93,370 93,370 92 C369 91,368 90,368 89 C368 88,367 87,367 85 C367 84,368 83,368 82 C368 81,368 80,369 79 C369 78,369 77,370 76 C371 75,371 74,372 73 C373 71,375 70,377 69 C379 68,382 67,384 67 C387 67,388 68,390 69 C391 70,391 71,391 73 C391 79,385 82,373 82 C373 83,373 83,373 84 C373 84,373 85,373 85 C373 87,373 89,374 90 C375 91,376 91,378 91 C378 91,379 91,380 91 C381 91,382 90,382 90 C383 90,384 89,385 88 C385 88,386 87,387 86 L389 88 Z M373 79 C376 79,378 79,379 79 C381 78,382 78,383 77 C384 77,385 76,386 76 C386 75,386 74,386 73 C386 72,386 72,386 71 C385 70,384 70,383 70 C382 70,381 70,380 71 C379 71,378 72,378 73 C377 73,376 74,375 75 C375 76,374 78,373 79 L373 79 Z" fill="#212121"/><path d="M399 83 C400 80,401 77,402 73 C402 70,403 67,403 65 C404 63,404 62,404 61 C404 61,404 60,404 60 C404 60,404 60,403 60 C403 59,403 59,402 59 C402 59,401 59,401 59 L401 57 L408 57 L410 57 C409 62,408 67,407 71 C406 75,405 79,404 84 C404 85,404 86,404 87 C404 87,403 88,403 88 C403 89,404 89,404 90 C404 90,405 90,405 90 C406 90,406 90,406 90 C407 90,407 90,407 89 C408 89,408 89,408 88 C409 88,409 87,410 87 L412 88 C411 90,410 90,409 91 C408 92,408 92,407 93 C406 93,406 94,405 94 C404 94,404 94,403 94 C402 94,402 94,401 94 C401 93,400 93,400 93 C399 92,399 92,399 91 C399 91,398 90,398 89 C398 88,399 88,399 87 C399 86,399 85,399 83 L399 83 Z" fill="#212121"/><path d="M427 93 C426 97,425 100,423 102 C421 104,418 105,415 105 C414 105,414 105,413 105 C413 104,414 104,414 103 C414 103,414 103,414 102 C414 102,414 102,414 102 C415 102,415 102,415 102 C416 102,417 102,417 102 C418 102,418 101,419 101 C419 100,420 99,420 98 C421 97,421 96,421 94 L427 70 L422 70 L422 68 C423 68,424 68,425 68 C425 68,426 68,426 67 C427 67,427 67,427 66 C428 63,430 61,432 59 C434 58,437 57,440 57 C441 57,442 57,443 57 C443 57,444 57,445 58 L444 63 L441 63 C441 62,441 61,440 61 C440 60,439 60,438 60 C437 60,437 60,436 61 C435 61,435 62,434 62 C434 63,433 64,433 65 C433 66,432 66,432 67 L440 67 L439 70 L432 70 L427 93 Z" fill="#212121"/><path d="M452 80 L452 76 L482 76 L482 80 L452 80 Z" fill="#212121"/><path d="M509 94 L509 92 C510 92,510 92,510 92 C511 92,511 91,511 91 C512 91,512 90,512 90 C512 90,512 89,512 88 C512 88,512 87,512 87 C512 87,512 86,512 86 C512 85,512 85,512 84 C512 84,512 84,512 83 L500 83 C499 84,499 84,499 85 C498 85,498 86,498 87 C497 87,497 88,497 88 C497 89,496 89,496 90 C496 91,497 92,499 92 L499 94 L487 94 L488 92 C488 92,488 92,489 92 C489 92,490 91,490 91 C490 90,491 90,491 89 C492 89,493 88,493 86 L510 59 L515 59 L517 87 C517 88,518 89,518 89 C518 90,518 90,518 91 C519 91,519 91,519 92 C520 92,520 92,521 92 L520 94 L509 94 Z M511 65 C509 67,508 70,506 72 C504 75,503 78,501 80 L512 80 C512 79,512 78,511 77 C511 76,511 75,511 73 C511 72,511 71,511 69 C511 68,511 67,511 65 L511 65 Z" fill="#212121"/><path d="M544 88 C543 89,542 90,541 91 C540 92,540 92,539 93 C538 93,537 94,536 94 C536 94,535 94,534 94 C532 94,530 94,529 93 C528 92,528 90,528 88 C528 88,528 87,528 86 C528 85,528 85,528 84 L531 70 L527 70 L527 68 C528 68,529 68,530 68 C531 68,531 67,531 67 C532 67,532 66,532 66 C532 66,533 65,533 65 C533 64,533 63,534 63 C534 62,534 61,535 60 L539 60 L537 67 L546 67 L546 70 L537 70 L534 81 C534 82,534 82,533 83 C533 84,533 84,533 84 C533 85,533 85,533 86 C533 86,533 86,533 87 C533 88,533 89,534 89 C534 90,535 90,536 90 C537 90,538 90,539 89 C540 89,541 88,542 86 L544 88 Z" fill="#212121"/><path d="M568 88 C567 89,566 90,565 91 C564 92,564 92,563 93 C562 93,561 94,560 94 C560 94,559 94,558 94 C556 94,554 94,553 93 C552 92,552 90,552 88 C552 88,552 87,552 86 C552 85,552 85,552 84 L555 70 L551 70 L551 68 C552 68,553 68,554 68 C555 68,555 67,555 67 C556 67,556 66,556 66 C556 66,557 65,557 65 C557 64,557 63,558 63 C558 62,558 61,559 60 L563 60 L561 67 L570 67 L570 70 L561 70 L558 81 C558 82,558 82,557 83 C557 84,557 84,557 84 C557 85,557 85,557 86 C557 86,557 86,557 87 C557 88,557 89,558 89 C558 90,559 90,560 90 C561 90,562 90,563 89 C564 89,565 88,566 86 L568 88 Z" fill="#212121"/><path d="M596 88 C594 90,592 92,590 93 C588 94,586 94,583 94 C582 94,581 94,579 93 C578 93,577 93,577 92 C576 91,575 90,575 89 C575 88,574 87,574 85 C574 84,575 83,575 82 C575 81,575 80,576 79 C576 78,576 77,577 76 C578 75,578 74,579 73 C580 71,582 70,584 69 C586 68,589 67,591 67 C594 67,595 68,597 69 C598 70,598 71,598 73 C598 79,592 82,580 82 C580 83,580 83,580 84 C580 84,580 85,580 85 C580 87,580 89,581 90 C582 91,583 91,585 91 C585 91,586 91,587 91 C588 91,589 90,589 90 C590 90,591 89,592 88 C592 88,593 87,594 86 L596 88 Z M580 79 C583 79,585 79,586 79 C588 78,589 78,590 77 C591 77,592 76,593 76 C593 75,593 74,593 73 C593 72,593 72,593 71 C592 70,591 70,590 70 C589 70,588 70,587 71 C586 71,585 72,585 73 C584 73,583 74,582 75 C582 76,581 78,580 79 L580 79 Z" fill="#212121"/><path d="M615 74 C617 72,618 70,620 69 C622 68,624 67,625 67 C627 67,629 68,630 69 C631 70,631 71,631 73 C631 73,631 74,631 75 C631 76,630 77,630 78 C630 79,630 80,629 81 C629 82,629 84,628 84 C628 85,628 86,628 87 C628 88,628 88,628 88 C628 89,628 89,628 90 C628 90,629 90,629 90 C630 90,630 90,630 90 C631 90,631 90,631 89 C632 89,632 89,632 88 C633 88,633 87,634 87 L636 88 C635 90,634 90,633 91 C633 92,632 92,631 93 C631 93,630 94,629 94 C628 94,628 94,627 94 C626 94,626 94,625 94 C625 94,624 93,624 93 C623 92,623 92,623 91 C623 91,623 90,623 89 C623 89,623 88,623 87 C623 86,623 85,623 84 C624 83,624 82,624 81 C625 80,625 79,625 78 C625 77,625 77,626 76 C626 75,626 75,626 74 C626 73,626 72,625 72 C625 71,624 71,623 71 C623 71,622 71,621 71 C621 72,620 72,619 73 C619 73,618 74,618 74 C617 75,616 76,616 76 C615 77,615 78,615 79 C614 80,614 80,614 81 L611 94 L606 94 L610 77 C610 76,610 76,610 75 C610 74,610 73,610 73 C610 72,610 71,609 71 C608 71,608 71,608 71 C607 71,607 72,607 72 C606 72,606 73,605 73 C605 73,605 74,604 74 L602 73 C603 72,604 71,605 70 C605 69,606 69,607 68 C607 68,608 68,609 67 C609 67,610 67,611 67 C612 67,613 68,614 68 C615 69,615 70,615 72 C615 72,615 73,615 73 C615 73,615 74,615 74 L615 74 Z" fill="#212121"/><path d="M658 88 C657 89,656 90,655 91 C654 92,654 92,653 93 C652 93,651 94,650 94 C650 94,649 94,648 94 C646 94,644 94,643 93 C642 92,642 90,642 88 C642 88,642 87,642 86 C642 85,642 85,642 84 L645 70 L641 70 L641 68 C642 68,643 68,644 68 C645 68,645 67,645 67 C646 67,646 66,646 66 C646 66,647 65,647 65 C647 64,647 63,648 63 C648 62,648 61,649 60 L653 60 L651 67 L660 67 L660 70 L651 70 L648 81 C648 82,648 82,647 83 C647 84,647 84,647 84 C647 85,647 85,647 86 C647 86,647 86,647 87 C647 88,647 89,648 89 C648 90,649 90,650 90 C651 90,652 90,653 89 C654 89,655 88,656 86 L658 88 Z" fill="#212121"/><path d="M678 56 L676 62 L671 62 L672 56 L678 56 Z M671 84 C671 85,671 86,671 87 C671 87,671 88,671 88 C671 89,671 89,671 90 C671 90,672 90,672 90 C673 90,673 90,673 90 C674 90,674 90,674 89 C675 89,675 89,675 88 C676 88,676 87,677 87 L679 89 C678 90,677 91,676 91 C675 92,675 92,674 93 C673 93,673 94,672 94 C671 94,671 94,670 94 C669 94,669 94,668 94 C668 94,667 93,667 93 C666 92,666 92,666 91 C666 91,665 90,665 89 C665 89,666 88,666 87 C666 86,666 85,666 84 C667 83,667 82,667 81 C667 80,668 79,668 78 C668 77,668 76,668 75 C669 74,669 73,669 72 C669 72,669 72,669 71 C669 71,669 71,669 71 C669 70,669 70,668 70 C668 69,667 69,666 69 L666 67 L674 67 L675 67 L671 84 Z" fill="#212121"/><path d="M693 94 C692 94,690 94,689 93 C688 93,687 92,686 92 C685 91,685 90,684 89 C684 88,683 86,683 85 C683 82,684 80,685 78 C686 76,687 74,688 72 C689 71,691 69,693 68 C695 68,697 67,699 67 C701 67,702 67,703 68 C705 68,706 69,707 70 C707 71,708 72,708 73 C709 74,709 75,709 77 C709 79,709 82,708 84 C707 86,706 88,704 89 C703 91,701 92,699 93 C697 94,695 94,693 94 L693 94 Z M689 85 C689 87,689 89,690 90 C691 91,692 91,694 91 C695 91,696 91,697 90 C698 90,698 89,699 89 C700 88,700 87,701 86 C702 85,702 84,702 83 C703 82,703 81,703 79 C704 78,704 77,704 76 C704 74,703 73,702 72 C702 71,700 70,699 70 C698 70,697 70,696 71 C695 71,694 72,694 72 C693 73,692 74,692 75 C691 76,690 77,690 78 C690 79,689 81,689 82 C689 83,689 84,689 85 L689 85 Z" fill="#212121"/><path d="M726 74 C728 72,729 70,731 69 C733 68,735 67,736 67 C738 67,740 68,741 69 C742 70,742 71,742 73 C742 73,742 74,742 75 C742 76,741 77,741 78 C741 79,741 80,740 81 C740 82,740 84,739 84 C739 85,739 86,739 87 C739 88,739 88,739 88 C739 89,739 89,739 90 C739 90,740 90,740 90 C741 90,741 90,741 90 C742 90,742 90,742 89 C743 89,743 89,743 88 C744 88,744 87,745 87 L747 88 C746 90,745 90,744 91 C744 92,743 92,742 93 C742 93,741 94,740 94 C739 94,739 94,738 94 C737 94,737 94,736 94 C736 94,735 93,735 93 C734 92,734 92,734 91 C734 91,734 90,734 89 C734 89,734 88,734 87 C734 86,734 85,734 84 C735 83,735 82,735 81 C736 80,736 79,736 78 C736 77,736 77,737 76 C737 75,737 75,737 74 C737 73,737 72,736 72 C736 71,735 71,734 71 C734 71,733 71,732 71 C732 72,731 72,730 73 C730 73,729 74,729 74 C728 75,727 76,727 76 C726 77,726 78,726 79 C725 80,725 80,725 81 L722 94 L717 94 L721 77 C721 76,721 76,721 75 C721 74,721 73,721 73 C721 72,721 71,720 71 C719 71,719 71,719 71 C718 71,718 72,718 72 C717 72,717 73,716 73 C716 73,716 74,715 74 L713 73 C714 72,715 71,716 70 C716 69,717 69,718 68 C718 68,719 68,720 67 C720 67,721 67,722 67 C723 67,724 68,725 68 C726 69,726 70,726 72 C726 72,726 73,726 73 C726 73,726 74,726 74 L726 74 Z" fill="#212121"/><path d="M799 52 L797 56 L787 49 L789 61 L784 61 L785 49 L775 56 L773 52 L783 47 L773 42 L775 38 L785 45 L784 33 L789 33 L787 45 L797 38 L799 42 L789 47 L799 52 Z" fill="#212121"/><path d="M840 18 L840 20 C839 20,838 21,837 21 C837 22,836 22,836 23 C836 24,836 25,836 27 C836 27,836 28,836 30 L837 57 L837 60 L837 60 L853 31 C854 30,855 28,855 27 C855 26,856 25,856 24 C856 22,854 21,852 20 L852 18 L867 18 L867 20 C866 20,866 20,865 21 C864 21,864 22,863 23 C862 24,861 26,860 28 L836 69 L831 69 L829 28 C829 27,829 25,828 24 C828 23,828 22,827 21 C827 21,826 20,824 20 L825 18 L840 18 Z" fill="#212121"/><path d="M873 35 L874 33 C874 33,874 33,875 33 C875 33,875 33,876 33 C876 32,876 32,876 32 C876 32,877 31,877 31 C877 31,877 30,877 29 C877 29,878 28,878 27 L883 3 L879 3 C879 3,878 3,878 3 C878 3,877 3,877 3 C876 4,876 4,876 4 C876 4,875 5,875 5 C875 6,874 6,874 7 C874 8,873 8,873 9 L870 9 L872 0 L901 0 L899 10 L896 10 L896 8 C896 8,896 8,896 7 C896 7,896 7,896 6 C896 6,896 5,896 5 C896 5,896 5,896 4 C895 4,895 4,895 4 C895 4,895 3,895 3 C894 3,894 3,894 3 C894 3,893 3,893 3 L888 3 L883 27 C883 28,883 29,883 29 C882 30,882 31,882 31 C882 31,882 32,883 32 C883 32,883 32,883 33 C883 33,884 33,884 33 C884 33,885 33,885 33 L885 35 L873 35 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">To perform the multiplication operation, we organize a nested loop.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;Gradient&nbsp;distribution&nbsp;on&nbsp;Score</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">&nbsp;+=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">outputs_grad</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_value</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">]&nbsp;*&nbsp;</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;values</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">window</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">)&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">scores_temp</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_score</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">]&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After running a full cycle of iterations of our tensor loop system, we will obtain a single row of error gradients for the dependency coefficient matrix. Before passing the error gradient further, it is necessary to correct it by the derivative of the </span><span class="f_Text" style="font-style: italic;">Softmax</span><span class="f_Text"> function.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;Adjust&nbsp;for&nbsp;the&nbsp;Softmax&nbsp;derivative</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">scores</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_score</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">&nbsp;+=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">scores</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_score</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">]&nbsp;*</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;((</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">)(</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;==&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">)&nbsp;-&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">)&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">scores_temp</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_score</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">scores_grad</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_score</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">]&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">The operation results are written into the corresponding elements of the error gradient tensor.</span></p>
<p class="p_Text"><span class="f_Text">This completes the work with the first kernel of the backpropagation algorithm. As you may have noticed, the changes affected only the definition of the offset in the data buffers and the additional dimension of the task space.</span></p>
<p class="p_Text"><span class="f_Text">Let's move on to the second kernel of the </span><span class="f_Text" style="font-style: italic;">AttentionCalcHiddenGradient</span><span class="f_Text"> error backpropagation algorithm. In this kernel, we need to propagate the error gradient from the dependency coefficient matrix to the buffers of the </span><span class="f_Text" style="font-style: italic;">m_cQuerys </span><span class="f_Text">and </span><span class="f_Text" style="font-style: italic;">m_cKeys</span><span class="f_Text"> internal neural layers.</span></p>
<p class="p_Text"><span class="f_Text">This operation is not difficult from a mathematical point of view. We have already determined the error gradient at the level of the dependency coefficient matrix in the previous kernel. Now we need to multiply the dependency coefficient matrix by the opposite tensor.</span></p>
<p class="p_Text"><span class="f_Text">As in the previous kernel, the kernel header and parameters have not changed at all. Here we see the same set of buffers and parameters.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">__kernel</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">void</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">AttentionCalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">__global</span><span class="f_CodeExample">&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">querys</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">__global</span><span class="f_CodeExample">&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">querys_grad</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">__global</span><span class="f_CodeExample">&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">__global</span><span class="f_CodeExample">&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">keys_grad</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">__global</span><span class="f_CodeExample">&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">scores_grad</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">key_size</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">In the kernel body, we identify the thread in two dimensions of tasks. The second dimension has been added for the identification of the active attention head. We adjust the offsets in the gradient buffers accordingly, ensuring they are aligned with the elements of the sequence being analyzed.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">q</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">get_global_id</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">get_global_size</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">get_global_id</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Functions">get_global_size</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">shift_query</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">key_size</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">q</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">shift_score</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">q</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">);</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">As mentioned earlier, in the kernel body, we need to distribute the error gradient to two internal neural layers from a single source. The same algorithm is used for gradient error distribution in both directions. And both recipient vectors have the same size. All of this allows us to calculate the error gradient for both tensors in parallel within the body of a single loop system. The number of iterations in the outer loop is equal to the size of the vector for which we are calculating the error gradient. In its body, we prepare variables for accumulating the error gradients and create a nested loop with a number of iterations equal to the number of elements in the sequence. In the body of the nested loop, we simultaneously calculate values from the product of two pairs of vectors.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;Propagate&nbsp;the&nbsp;gradient&nbsp;on&nbsp;Querys&nbsp;and&nbsp;Keys</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">const</span><span class="f_CodeExample">&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">&nbsp;/&nbsp;</span><span class="f_Functions">sqrt</span><span class="f_CodeExample">((</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">TYPE</span><span class="f_CodeExample">)</span><span class="f_CodeExample" style="color: #333333;">key_size</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">key_size</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad_q</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad_k</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad_q</span><span class="f_CodeExample">&nbsp;+=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">key_size</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">)&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">]&nbsp;*</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scores_grad</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_score</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad_k</span><span class="f_CodeExample">&nbsp;+=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">key_size</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">)&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">]&nbsp;*</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scores_grad</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">units</span><span class="f_CodeExample">&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">s</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">heads</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">h</span><span class="f_CodeExample">)&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">q</span><span class="f_CodeExample">];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys_grad</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_query</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">]&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad_q</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">keys_grad</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">shift_query</span><span class="f_CodeExample">&nbsp;+&nbsp;</span><span class="f_CodeExample" style="color: #333333;">i</span><span class="f_CodeExample">]&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">grad_k</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">k</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After exiting the nested loop, each variable has one value for the error gradient vectors of the required tensors. We write them into the corresponding elements of the tensors. After completing the full number of iterations of the loop system, we obtain the two desired vectors of error gradients.</span></p>
<p class="p_Text"><span class="f_Text">We finish working with OpenCL program kernels. Here, we have only made slight changes to the kernels of the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm to transfer them to the area of &#8203;&#8203;multi-headed attention.</span></p>
<p class="p_Text"><span class="f_Text">Now we have to supplement the main program with the functionality of calling kernel data from methods of both the </span><span class="f_Text" style="font-style: italic;">CNeuronAttention</span><span class="f_Text"> class and the </span><span class="f_Text" style="font-style: italic;">CNeuronMHAttention</span><span class="f_Text"> class. We usually start this work by creating constants for working with kernels. But in this case, the constants have already been created.</span></p>
<p class="p_Text"><span class="f_Text">Next, we created kernels in the OpenCL context. But this time we did not create new kernels. The ones that we slightly adjusted are already declared in the body of the main program. Therefore, we skip this step too.</span></p>
<p class="p_Text"><span class="f_Text">Let's move on to making changes directly to class methods. For new kernels to work in the </span><span class="f_Text" style="font-style: italic;">CNeuronAttention </span><span class="f_Text">class, we add a second element to the offset and task space arrays. For offset, we specify 0 in both dimensions. For the task space, we leave the first value unchanged, and in the second element of the array, we introduce 1 (indicating the use of a single attention head). Additionally, when enqueueing the kernel for execution, we specify the two-dimensionality of the task space.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">off_set</span><span class="f_CodeExample">[]&nbsp;=&nbsp;{</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">};</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">NDRange</span><span class="f_CodeExample">[]&nbsp;=&nbsp;{</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">};</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Execute</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">def_k_AttentionFeedForward</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">2</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">off_set</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">NDRange</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After this, we can fully use the updated feed-forward kernel. </span></p>
<p class="p_Text"><span class="f_Text">We do such simple manipulations to call all three kernels in the methods of the </span><span class="f_Text" style="font-style: italic;">CNeuronAttention</span><span class="f_Text"> class.</span></p>
<p class="p_Text"><span class="f_Text">So, we have restored the functionality of the methods of the </span><span class="f_Text" style="font-style: italic;">CNeuronAttention</span><span class="f_Text"> class, which implements the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm. There are also some changes on the main program side.</span></p>
<p class="p_Text"><span class="f_Text">Let's move on to working on our </span><span class="f_Text" style="font-style: italic;">CNeuronMHAttention</span><span class="f_Text"> class with the implementation of the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> algorithm. As usual, we'll start with the feed-forward method. Before we queue the kernel for operations, we need to do some preparatory work. First of all, we check the presence of the necessary buffers in the OpenCL context memory.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronMHAttention</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">FeedForward</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;......</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;branching&nbsp;of&nbsp;the&nbsp;algorithm&nbsp;across&nbsp;the&nbsp;computing&nbsp;device</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">out</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;......</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//&nbsp;OpenCL&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;check&nbsp;data&nbsp;buffers</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cScores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After checking all the necessary buffers, we pass pointers to the buffers to the kernel parameters. There we also pass the constants necessary for the operation of the kernel.</span></p>
<p class="p_Text"><span class="f_Text">Please note that when passing parameters to the kernel, we specified the </span><span class="f_Text" style="font-style: italic;">m_iKeysSize</span><span class="f_Text"> variable, which contains the size of the key vector for one element of the sequence, twice. We specified it for both the key vector size parameter and the value vector size parameter. Two parameters in the kernel are a necessary measure. When using a single attention head, for the size of the value vector, we would need to specify the size of the input data vector. This is a requirement of the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm. However, when using multi-head attention, the </span><span class="f_Text" style="font-style: italic;">W0</span><span class="f_Text"> matrix allows us to use different sizes for the value vector.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;pass&nbsp;parameters&nbsp;to&nbsp;the&nbsp;kernel</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionFeedForward</span><span class="f_CodeExample">,</span>
<br><span class="f_Definition">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def_attff_keys</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionFeedForward</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_attff_outputs</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionFeedForward</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_attff_querys</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionFeedForward</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_attff_scores</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cScores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionFeedForward</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_attff_values</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgument</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionFeedForward</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_attff_key_size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgument</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionFeedForward</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_attff_window</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">This concludes the preparatory work, and we can move on to organizing the kernel launch procedure. To do this, we indicate the size of the problem space in two dimensions. In the first dimension, we indicate the size of the sequence; in the second one, we indicate the number of attention heads. Let's call the method for adding the kernel to the execution queue.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;putting&nbsp;the&nbsp;kernel&nbsp;into&nbsp;the&nbsp;execution&nbsp;queue</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">off_set</span><span class="f_CodeExample">[]&nbsp;=&nbsp;{</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">};</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">NDRange</span><span class="f_CodeExample">[]&nbsp;=&nbsp;{</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">};</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Execute</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionFeedForward</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">2</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">off_set</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">NDRange</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Here, we conclude our work on the feed-forward method and transition to the </span><span class="f_Text" style="font-style: italic;">CalcHiddenGradient</span><span class="f_Text"> method that propagates the error gradient through the hidden layer. To implement the process of this method, we have prepared two kernels, which we need to launch sequentially. First, we will run the error gradient propagation kernel up to the </span><span class="f_Text" style="font-style: italic;">AttentionCalcScoreGradient</span><span class="f_Text"> dependency coefficient matrix.</span></p>
<p class="p_Text"><span class="f_Text">The algorithm for carrying out the preparatory work and launching the kernel is similar to what we used above when launching the forward pass kernel.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronMHAttention</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;branching&nbsp;of&nbsp;the&nbsp;algorithm&nbsp;across&nbsp;the&nbsp;computing&nbsp;device</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;......</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//&nbsp;MQL5&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//&nbsp;OpenCL&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;check&nbsp;data&nbsp;buffers</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cScores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cScoreGrad</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cScoreTemp</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After checking the buffers, we pass pointers to them and the necessary constants as parameters to the kernel.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;passing&nbsp;parameters&nbsp;to&nbsp;the&nbsp;kernel</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionScoreGradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_attscr_outputs_grad</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionScoreGradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_attscr_scores</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cScores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionScoreGradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_attscr_scores_grad</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cScoreGrad</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionScoreGradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_attscr_scores_temp</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cScoreTemp</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionScoreGradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_attscr_values</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionScoreGradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_attscr_values_grad</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgument</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionScoreGradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_attscr_window</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We place the kernel in the queue for performing operations. As with the feed-forward pass, we create a two-dimensional task space. In the first dimension, we specify the number of elements being analyzed in the sequence, and in the second dimension, we specify the number of attention heads.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;place&nbsp;the&nbsp;kernel&nbsp;into&nbsp;the&nbsp;execution&nbsp;queue</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">off_set</span><span class="f_CodeExample">[]&nbsp;=&nbsp;{</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">};</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">NDRange</span><span class="f_CodeExample">[]&nbsp;=&nbsp;{</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">};</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Execute</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionScoreGradients</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">2</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">off_set</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">NDRange</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We immediately begin the preparatory work before launching the second kernel. Checking the data buffers in the OpenCL context memory. Only those buffers that we did not check when launching the first kernel are subject to verification.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;check&nbsp;data&nbsp;buffers</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()&nbsp;&lt;&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">We pass pointers to data buffers to the parameters of the second kernel. We also add the necessary constants there.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;pass&nbsp;arguments&nbsp;to&nbsp;the&nbsp;kernel</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionHiddenGradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_atthgr_keys</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionHiddenGradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_atthgr_keys_grad</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionHiddenGradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_atthgr_querys</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionHiddenGradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_atthgr_querys_grad</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">GetIndex</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgumentBuffer</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionHiddenGradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_atthgr_scores_grad</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cScoreGrad</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SetArgument</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionHiddenGradients</span><span class="f_CodeExample">,&nbsp;</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">def_atthgr_key_size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">After completing the preparatory work, we call the method for placing the kernel in the tasks execution queue. Please note that this time we are not creating new arrays specifying the task space because it has not changed, and we can use the existing arrays from the previous kernel launch.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;place&nbsp;the&nbsp;kernel&nbsp;into&nbsp;the&nbsp;execution&nbsp;queue</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Execute</span><span class="f_CodeExample">(</span><span class="f_Definition">def_k_AttentionHiddenGradients</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">2</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">off_set</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">NDRange</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">This concludes our work on the implementation of the </span><span class="f_Text" style="font-style: italic;">Multi-Head Self-Attention</span><span class="f_Text"> algorithm, including general and multi-threaded calculations. We have implemented all the functionality of the </span><span class="f_Text" style="font-style: italic;">CNeuronMHAttention</span><span class="f_Text"> class. Now we can proceed with comprehensive testing of its performance using training and testing datasets.</span></p>

</div>

</body>
</html>
