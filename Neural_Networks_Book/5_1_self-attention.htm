<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>5.1 Self-Attention</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="5_transformer.htm"> 5. Attention mechanisms </a>/ 5.1 Self-Attention
          </td>
          <td width="70" align="right">
          <a href="5_transformer.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="5_1_1_tr_descrption.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H2"><span class="f_H2">5.1 Self-Attention</span></p>
<p class="p_Text"><span class="f_Text">The models described above utilize recurrent blocks, the training of which incurs significant costs. In June 2017, in the article <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="external" class="weblink">Attention Is All You Need</a> the authors proposed a new neural network architecture called the Transformer, which eliminated the use of recurrent blocks and proposed a new </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm. In contrast to the algorithm described above, the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm analyzes pairwise dependencies within the same sequence. The Transformer performed better on the tests, and today the model and its derivatives are used in many models, including </span><span class="f_Text" style="font-style: italic;">GPT-2</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">GPT-3</span><span class="f_Text">. We will consider the </span><span class="f_Text" style="font-style: italic;">Self-Attention</span><span class="f_Text"> algorithm in more detail.</span></p>

</div>

</body>
</html>
