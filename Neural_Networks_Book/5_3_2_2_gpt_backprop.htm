<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
  <title>5.3.2.2 GPT backpropagation methods</title>
  <meta name="keywords" content="" />
  <link type="text/css" href="default.css" rel="stylesheet" />

   <script type="text/javascript" src="jquery.js"></script>
   <script type="text/javascript" src="helpman_settings.js"></script>
   <script type="text/javascript" src="helpman_topicinit.js"></script>


</head>

<body style="background-color:#FFFFFF; font-family:'Trebuchet MS',Tahoma,Arial,Helvetica,sans-serif; margin:0px;">



<table width="100%" height="49"  border="0" cellpadding="0" cellspacing="0" style="margin-top:0px; background-color:#1660af;">
  <tr>
    <td></td>
    <td valign="middle">
      <table style="margin-top:4px; margin-bottom:5px;" width="100%"  border="0" cellspacing="0" cellpadding="5">
        <tr valign="middle">
          <td class="nav">
<a class="h_m" href="index.htm">          Neural Networks for Algorithmic Trading with MQL5 </a> / <a class="h_m" href="5_transformer.htm"> 5. Attention mechanisms </a> / <a class="h_m" href="5_3_gpt.htm"> 5.3 GPT architecture </a> / <a class="h_m" href="5_3_2_gpt_mql5.htm"> 5.3.2 Building a GPT model in MQL5 </a>/ 5.3.2.2 GPT backpropagation methods
          </td>
          <td width="70" align="right">
          <a href="5_3_2_1_gpt_feedforward.htm"><img style="vertical-align:middle;" src="previous.png" alt="?????" width="27" height="27" border=0></a>&nbsp;
          <a href="5_3_2_3_gpt_save_load.htm"><img style="vertical-align:middle;" src="next.png" alt="??????" width="27" height="27" border="0"></a>
          </td>
        </tr>
      </table>
    </td>
    <td width="5"></td>
  </tr>
</table>



<div id="help">
<p class="p_H3"><span class="f_H3">5.3.2.2 GPT backpropagation methods</span></p>
<p class="p_Text"><span class="f_Text">In the previous sections, we looked at the architecture of the </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> model and even implemented methods to initialize our new class and implement the feed-forward pass through the model algorithm. Now let's look at a possible implementation of the backpropagation pass for this algorithm.</span></p>
<p class="p_Text"><span class="f_Text">To implement the backpropagation pass in each new class we override three methods:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li" style="font-style: italic;">CalcHiddenGradient</span><span class="f_li"> – method for calculating the error gradient through the hidden layer</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">CalcDeltaWeights</span><span class="f_li"> – method for calculating the error gradient to the level of the weight matrix</span></li>
<li class="p_li"><span class="f_li" style="font-style: italic;">UpdateWeights</span><span class="f_li"> – method for updating weights</span></li>
</ul>
<p class="p_Text"><span class="f_Text">This class will not be an exception, and we will redefine all three methods. Let's start with the first backpropagation and, probably, the most complex method: error gradient propagation through the hidden layer. It is in this method that we have to repeat the entire feed-forward algorithm in reverse order.</span></p>
<p class="p_Text"><span class="f_Text">In the parameters, the method receives a pointer to the object of the previous layer, to which we have to pass the error gradient. Again, in the body of the method, we implement a block of checks. In it, according to the already established good tradition, we check the validity of pointers to all objects used in the method. This approach helps eliminate many critical errors during the execution of the method code.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronGPT</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;check&nbsp;the&nbsp;relevance&nbsp;of&nbsp;all&nbsp;objects</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cOutputs</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Total</span><span class="f_CodeExample">()&nbsp;!=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cGradients</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Total</span><span class="f_CodeExample">())</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, by analogy with the forward pass method, we organize a loop for searching through the internal neural layers. But this time, in accordance with the principles of backward pass, we also organize the cycle with a countdown of iterations. All further iterations will be performed in the body of the loop and repeated for all nested layers of our model.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #808080;">//---&nbsp;run&nbsp;a&nbsp;loop&nbsp;through&nbsp;all&nbsp;internal&nbsp;layers&nbsp;in&nbsp;reverse&nbsp;order</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">&nbsp;-&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;&gt;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">--)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Gradients</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;scale&nbsp;the&nbsp;gradient&nbsp;for&nbsp;normalization</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">NormlizeBufferGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">(),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Gradients</span><span class="f_CodeExample">,</span>
<br><span class="f_CodeExample" style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Functions">GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_dStd</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">]),&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">In the body of the loop, we first retrieve a pointer to the corresponding neural layer of the output of the </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> </span><span class="f_Text" style="font-style: italic;">FF2</span><span class="f_Text"> block and adjust its error gradient buffer to the derivative of the normalization function. We discussed the reasons for this operation in detail when constructing a similar method for the </span><span class="f_Text" style="font-style: italic;"><a href="5_1_2_2_tr_backprop.htm#normalization" class="topiclink">Self-Attention</a></span><span class="f_Text"> algorithm.</span></p>
<p class="p_Text"><span class="f_Text">After this, we sequentially call the error gradient distribution methods for the internal layers of the </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> block. We also call the methods in the reverse order: first for the second layer, and then for the first one.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;propagate&nbsp;a&nbsp;gradient&nbsp;through&nbsp;the&nbsp;Feed&nbsp;Forward&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">FF1</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">FF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">FF1</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">FF1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">During the feed-forward pass, we added up the results of the </span><span class="f_Text" style="font-style: italic;">Multi-Heads Self-Attention </span><span class="f_Text">and </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> blocks. Also, now we need to draw an error gradient in two directions. We add the error gradients at the output level of the specified blocks. Then we adjust the total tensor by the derivative of the layer normalization function.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CBufferType</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">attention_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">attention_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">SumArray</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">Gradients</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;scale&nbsp;the&nbsp;gradient&nbsp;for&nbsp;normalization</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">NormlizeBufferGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">(),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">attention_grad</span><span class="f_CodeExample">,</span>
<br><span class="f_Functions">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GetPointer</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_dStd</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">]),&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, we distribute the error gradient across the attention heads by calling the error gradient distribution method of the internal neural layer </span><span class="f_Text" style="font-style: italic;">W</span><span class="f_Text" style="font-size: 7pt; font-style: italic; vertical-align: sub;">0</span><span class="f_Text">.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;initialize&nbsp;Scores</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Scores</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cScores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Scores</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;distribute&nbsp;the&nbsp;error&nbsp;gradient&nbsp;across&nbsp;the&nbsp;heads&nbsp;of&nbsp;attention</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Until now, everything was simple and transparent. We simply called the corresponding methods of our internal neural layers in reverse order. But then comes the algorithm block that is not covered by the methods of internal neural layers. It was implemented inside the feed-forward method. Therefore, we also have to completely recreate the error gradient backpropagation functionality.</span></p>
<p class="p_Text"><span class="f_Text">First, let's do the preparatory work and create local pointers to </span><span class="f_Text" style="font-style: italic;">Querys</span><span class="f_Text">, </span><span class="f_Text" style="font-style: italic;">Keys</span><span class="f_Text">, and </span><span class="f_Text" style="font-style: italic;">Values</span><span class="f_Text"> objects. At this point, don’t forget to check the validity of the received object pointers.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;get&nbsp;pointers&nbsp;to&nbsp;Querys,&nbsp;Keys,&nbsp;Values&nbsp;objects</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cKeys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cValues</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, we need to create two options for implementing the algorithm: using standard MQL5 tools and in multi-threaded operations mode using OpenCL technology. We create a branching of the algorithm depending on the selected device for performing mathematical operations. As usual, in this section, we will look at the implementation of the algorithm using standard MQL5 tools and will return to the implementation of the multi-threaded operations block in other sections.</span></p>
<p class="p_Text"><span class="f_Text">To organize calculations using standard MQL5 tools, we prepare dynamic arrays. In one array, we load error gradient data from the buffer. Some arrays are filled with the results of the feed-forward pass and others are initialized with zero values for subsequent gradient error accumulation operations.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;branching&nbsp;of&nbsp;the&nbsp;algorithm&nbsp;across&nbsp;the&nbsp;computing&nbsp;device</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">attention_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">AttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">();</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cOpenCL</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">gradients</span><span class="f_CodeExample">[];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">attention_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Vsplit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">gradients</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">3</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">values</span><span class="f_CodeExample">[];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Values</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Vsplit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">values</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">[];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Keys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Vsplit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys</span><span class="f_CodeExample">[];</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">query</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">query</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">3</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">query</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Resize</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">query</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Cols</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">query</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Vsplit</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">Zeros</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">keys_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys_grad</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">values_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys_grad</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">First, we will distribute the error gradient to the </span><span class="f_Text" style="font-style: italic;">Value</span><span class="f_Text"> tensor. It's important to note that we'll be distributing the error gradient not across the entire tensor but only for the current element. This is reasonable when we consider the purpose of error gradient distribution. We aim to optimize the model parameters throughout the training process, and distributing the error gradient helps us obtain guidelines for this optimization.</span></p>
<p class="p_Text"><span class="f_Text">When distributing the error gradient to the </span><span class="f_Text" style="font-style: italic;">Value</span><span class="f_Text"> tensor, we need to pass it in two directions: to the previous layer and to the weight matrix responsible for forming the current layer's tensor.</span></p>
<p class="p_Text"><span class="f_Text">We can only transfer the error gradient for the current state to the previous layer. The buffer of the previous layer is unable to accept more because, during the feed-forward pass, it only provides the current state for which it expects the error gradient.</span></p>
<p class="p_Text"><span class="f_Text">Also, only the current state error gradient can be propagated to the weight matrix. To distribute the error from previous states, we would need the input data from those previous states. However, the previous layer does not provide this information, and we did not save it in the buffers of our layer.</span></p>
<p class="p_Text"><span class="f_Text">Therefore, distributing the gradient to the elements of the value tensor, except for the current state, is a dead-end task and does not make sense.</span></p>
<p class="p_Text"><span class="f_Text">The general approach is as follows: during the feed-forward pass, we calculate only the current state and additionally retrieve from memory those already calculated in previous iterations. A similar situation applies during the backpropagation pass: it is assumed that the error gradient from previous states has already been considered in the backpropagation methods in previous iterations. This significantly reduces the number of operations for each iteration of the feed-forward and backpropagation passes.</span></p>
<p class="p_Text"><span class="f_Text">I hope the logic is clear. Let's return to our backpropagation method. We paused at passing the error gradient to the </span><span class="f_Text" style="font-style: italic;">Value</span><span class="f_Text"> tensor. To execute this iteration, we will first create a local pointer to the attention coefficient vector and then organize a loop.</span></p>
<p class="p_Text"><span class="f_Text">Our loop will iterate through the active attention heads. Here, we immediately save the attention coefficient vector corresponding to the analyzed attention head in a local matrix. We multiply the gradient vector obtained from previous iterations by the attention coefficient for the current element of the sequence. The resulting values are saved in the error gradient matrix in the </span><span class="f_Text" style="font-style: italic;">Values</span><span class="f_Text"> buffer.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">Zeros</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">Scores</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetOutputs</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">m_mMatrix</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">),&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;distribution&nbsp;of&nbsp;the&nbsp;gradient&nbsp;on&nbsp;Values</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">values_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">((</span><span class="f_CodeExample" style="color: #333333;">gradients</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">]&nbsp;*&nbsp;</span>
<br><span class="f_CodeExample" style="color: #333333;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;score</span><span class="f_CodeExample">[</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iCurrentPosition</span><span class="f_CodeExample">]).</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Next, we need to distribute the gradient in the second direction: through the matrix of dependency coefficients on the </span><span class="f_Text" style="font-style: italic;">Query </span><span class="f_Text">and </span><span class="f_Text" style="font-style: italic;">Key</span><span class="f_Text"> tensors. But first, we need to propagate the gradient through the vector of dependence coefficients. We multiply the error gradient matrices at the output of the attention block and the </span><span class="f_Text" style="font-style: italic;">Values</span><span class="f_Text"> &#8203;&#8203;matrix and obtain a gradient at the level of the vector of dependence coefficients.</span></p>
<p class="p_Text"><span class="f_Text">So, we have a vector of error gradients for one attention head. But I would like to remind you that during the feed-forward pass, we normalized the vector of dependence coefficients with the </span><span class="f_Text" style="font-style: italic;">Softmax</span><span class="f_Text"> function. Therefore, the obtained error gradients are valid for normalized data. To further distribute the error gradients, we need to adjust the error gradients to the derivative of the specified function.</span></p>
<p class="p_Text"><span class="f_Text">A special feature of the </span><span class="f_Text" style="font-style: italic;">Softmax</span><span class="f_Text"> function is the requirement for a complete set of tensor values to compute the value of each element. Similarly, to compute the derivative of one element, we need a complete set of values for the function results. In our case, the results of the function are the normalized vector of dependency coefficients, which we obtained during the forward pass. We have also already obtained the vector of error gradients. Thus, we have all the necessary initial data to perform the operations of finding the derivative of a function and adjusting the error gradient. The formula for the derivative of the </span><span class="f_Text" style="font-style: italic;">Softmax </span><span class="f_Text"> function is as follows:</span></p>
<p class="p_Text" style="text-align: center;"><span style="display:inline-block;width:319px;height:48px;padding:0 0;overflow:visible"><svg xmlns="http://www.w3.org/2000/svg" style="width:100%;height:100%;overflow:visible" viewBox="0 0 1276 192"><path d="M13 98 C13 106,14 114,17 119 C20 125,24 128,30 130 L29 133 C22 131,16 127,13 121 C9 115,7 107,7 98 C7 89,9 82,13 75 C16 69,22 65,29 63 L30 66 C24 68,20 72,17 77 C14 83,13 89,13 98 L13 98 Z" fill="#212121"/><path d="M37 105 C37 108,38 110,39 112 C41 113,43 114,46 114 C49 114,52 113,54 111 C55 109,56 107,56 103 C56 102,56 100,56 99 C55 98,54 97,53 96 C52 95,51 94,49 92 C47 91,45 90,44 88 C43 87,42 86,41 85 C41 83,40 81,40 80 C40 77,41 74,42 72 C44 70,46 68,48 67 C51 66,54 65,57 65 C59 65,61 65,63 66 C65 66,67 66,70 67 L68 76 L64 76 C64 74,64 73,63 71 C63 70,62 69,61 69 C60 68,58 68,57 68 C55 68,53 69,51 69 C50 70,49 71,48 73 C47 74,47 76,47 78 C47 79,47 81,48 82 C49 84,51 86,54 87 C56 89,58 91,59 92 C60 93,61 95,62 96 C63 98,63 100,63 101 C63 105,62 107,61 110 C59 112,57 114,55 115 C52 116,49 117,46 117 C44 117,41 117,39 117 C36 116,34 116,32 115 L34 105 L37 105 Z" fill="#212121"/><path d="M87 117 C83 117,80 116,78 114 C76 112,75 109,75 104 C75 103,75 100,76 98 C77 94,78 91,80 88 C82 86,84 84,86 82 C89 81,92 80,95 80 C99 80,102 81,104 83 C106 86,107 89,107 93 C107 96,106 99,105 102 C104 105,103 108,101 110 C100 113,98 114,95 115 C93 117,90 117,87 117 L87 117 Z M81 106 C81 109,82 111,83 112 C84 113,86 114,88 114 C91 114,93 113,95 111 C96 109,98 106,99 102 C100 98,101 94,101 91 C101 89,100 87,99 85 C98 84,96 83,94 83 C92 83,89 84,88 86 C86 89,84 92,83 96 C82 100,81 103,81 106 L81 106 Z" fill="#212121"/><path d="M127 117 C126 122,124 126,122 128 C119 131,116 132,113 132 C112 132,111 132,110 132 L111 129 C111 129,112 129,113 129 C114 129,115 129,116 128 C116 128,117 127,118 125 C118 124,119 122,120 120 L128 85 L121 85 L122 83 C124 83,125 83,125 83 C126 83,127 82,127 82 C128 82,128 81,128 81 C128 80,129 79,129 77 C131 73,133 70,135 67 C138 65,141 64,146 64 C148 64,150 64,152 65 L150 71 L147 71 C147 70,146 69,146 68 C145 67,144 67,143 67 C142 67,141 67,140 68 C139 69,138 70,137 71 C137 73,136 75,135 77 L134 81 L144 81 L143 85 L134 85 L127 117 Z" fill="#212121"/><path d="M177 109 C175 112,173 114,171 115 C169 116,167 117,165 117 C160 117,158 115,158 109 C158 108,158 106,158 104 L162 85 L156 85 L157 83 C158 83,159 83,160 83 C161 83,161 82,162 82 C162 82,163 81,163 80 C164 80,164 79,165 78 C165 77,166 75,166 72 L171 72 L169 81 L180 81 L180 85 L169 85 L165 99 C165 102,164 104,164 106 C164 107,164 108,164 108 C164 112,165 113,168 113 C169 113,170 113,171 112 C172 111,174 109,175 107 L177 109 Z" fill="#212121"/><path d="M198 90 C200 86,203 84,205 82 C207 81,209 80,212 80 C214 80,216 81,217 82 C218 83,219 85,219 88 L219 88 C219 88,219 88,219 89 C221 86,223 84,225 82 C227 81,229 80,231 80 C234 80,235 81,237 82 C238 83,239 85,239 88 C239 89,238 92,237 94 L235 104 C234 107,234 109,234 110 C234 111,234 112,235 112 C235 113,235 113,236 113 C237 113,238 113,238 112 C239 112,240 111,242 109 L244 111 C242 113,240 115,239 116 C237 117,235 117,233 117 C232 117,230 116,229 115 C228 114,228 113,228 111 C228 109,228 107,229 104 L231 97 C231 95,232 93,232 92 C232 91,232 90,232 89 C232 87,232 86,231 85 C231 84,230 84,229 84 C228 84,226 85,225 85 C224 86,223 87,222 89 C220 91,219 92,218 94 C218 95,217 97,216 100 L213 116 L207 116 L211 97 C212 95,212 93,212 92 C212 91,212 90,212 89 C212 87,212 86,211 85 C211 84,210 84,208 84 C208 84,207 85,205 85 C204 86,203 87,202 89 C200 91,199 92,198 94 C198 96,197 98,196 100 L193 116 L187 116 L192 92 C193 90,193 88,193 87 C193 86,193 85,192 85 C192 84,192 84,191 84 C190 84,189 84,188 85 C188 86,186 87,185 88 L183 86 C185 84,187 83,188 82 C190 81,192 80,194 80 C195 80,196 81,197 82 C198 83,199 84,199 85 C199 87,198 88,198 90 L198 90 Z" fill="#212121"/><path d="M276 83 L280 80 L282 81 L277 105 C276 107,276 109,276 110 C276 111,276 112,276 112 C277 113,277 113,278 113 C279 113,280 113,280 112 C281 112,282 111,284 109 L286 111 C284 113,282 115,280 116 C279 117,277 117,275 117 C274 117,273 117,272 116 C271 115,270 113,270 112 C270 111,271 109,271 108 L271 107 C268 111,266 113,264 115 C262 116,260 117,257 117 C255 117,253 116,251 114 C250 112,249 109,249 105 C249 101,250 97,251 93 C253 89,255 86,258 84 C261 81,264 80,268 80 C269 80,271 80,272 81 C274 81,275 82,276 83 L276 83 Z M273 94 C273 93,274 92,274 91 C274 90,274 90,274 89 C274 87,273 85,273 84 C272 84,270 83,268 83 C266 83,264 84,262 86 C260 88,258 91,257 95 C256 98,255 102,255 105 C255 108,256 110,256 111 C257 112,258 113,260 113 C262 113,263 112,265 111 C266 110,268 107,269 105 C271 102,272 99,273 95 L273 94 Z" fill="#212121"/><path d="M304 99 C304 97,304 96,303 94 C303 92,303 90,302 89 C302 87,301 86,301 85 C301 85,301 85,300 84 C300 84,300 84,299 84 C299 84,298 84,298 84 C297 85,297 85,296 86 C296 86,295 87,294 88 L292 86 C294 84,295 83,296 82 C298 81,300 80,301 80 C302 80,303 80,303 80 C304 81,305 81,305 81 C305 82,306 82,306 83 C307 83,307 84,307 85 C308 86,308 87,308 89 C308 90,309 92,309 93 L309 93 C312 90,313 87,314 86 C316 84,317 83,318 82 C318 81,319 81,320 81 C321 80,322 80,323 80 C324 80,325 80,326 81 L324 88 L322 88 C322 87,321 86,321 86 C320 86,320 86,320 86 C320 86,319 86,319 87 C319 87,318 87,317 88 C317 89,316 90,315 91 C314 92,313 93,312 94 L310 97 C310 100,311 102,311 103 C312 105,312 107,312 108 C313 109,313 110,313 111 C313 111,314 112,314 112 C314 113,314 113,315 113 C315 113,315 113,316 113 C316 113,317 113,317 112 C318 112,319 111,320 109 L323 111 C321 113,319 115,318 116 C317 117,315 117,313 117 C312 117,311 117,310 116 C310 116,309 115,308 115 C308 114,307 112,307 111 C306 107,306 105,306 103 L305 103 C303 107,301 110,299 111 C298 113,297 114,296 115 C296 116,295 116,294 117 C293 117,292 117,291 117 C290 117,289 117,288 117 L290 109 L292 109 C292 110,293 111,293 111 C294 111,294 111,295 111 C295 111,295 110,296 109 C297 109,298 108,299 106 C300 105,302 102,304 99 L304 99 Z" fill="#212121"/><path d="M342 98 C342 106,343 114,346 119 C349 125,353 128,359 130 L358 133 C351 131,345 127,342 121 C338 115,336 107,336 98 C336 89,338 82,342 75 C345 69,351 65,358 63 L359 66 C353 68,349 72,346 77 C343 83,342 89,342 98 L342 98 Z" fill="#212121"/><path d="M377 99 C377 97,377 96,376 94 C376 92,376 90,375 89 C375 87,374 86,374 85 C374 85,374 85,373 84 C373 84,373 84,372 84 C372 84,371 84,371 84 C370 85,370 85,369 86 C369 86,368 87,367 88 L365 86 C367 84,368 83,369 82 C371 81,373 80,374 80 C375 80,376 80,376 80 C377 81,378 81,378 81 C378 82,379 82,379 83 C380 83,380 84,380 85 C381 86,381 87,381 89 C381 90,382 92,382 93 L382 93 C385 90,386 87,387 86 C389 84,390 83,391 82 C391 81,392 81,393 81 C394 80,395 80,396 80 C397 80,398 80,399 81 L397 88 L395 88 C395 87,394 86,394 86 C393 86,393 86,393 86 C393 86,392 86,392 87 C392 87,391 87,390 88 C390 89,389 90,388 91 C387 92,386 93,385 94 L383 97 C383 100,384 102,384 103 C385 105,385 107,385 108 C386 109,386 110,386 111 C386 111,387 112,387 112 C387 113,387 113,388 113 C388 113,388 113,389 113 C389 113,390 113,390 112 C391 112,392 111,393 109 L396 111 C394 113,392 115,391 116 C390 117,388 117,386 117 C385 117,384 117,383 116 C383 116,382 115,381 115 C381 114,380 112,380 111 C379 107,379 105,379 103 L378 103 C376 107,374 110,372 111 C371 113,370 114,369 115 C369 116,368 116,367 117 C366 117,365 117,364 117 C363 117,362 117,361 117 L363 109 L365 109 C365 110,366 111,366 111 C367 111,367 111,368 111 C368 111,368 110,369 109 C370 109,371 108,372 106 C373 105,375 102,377 99 L377 99 Z" fill="#212121"/><path d="M411 104 L409 110 L404 110 L405 104 L411 104 Z M404 132 C404 133,404 134,404 135 C404 135,404 136,404 136 C404 137,404 137,404 138 C404 138,405 138,405 138 C406 138,406 138,406 138 C407 138,407 138,407 137 C408 137,408 137,408 136 C409 136,409 135,410 135 L412 137 C411 138,410 139,409 139 C408 140,408 140,407 141 C406 141,406 142,405 142 C404 142,404 142,403 142 C402 142,402 142,401 142 C401 142,400 141,400 141 C399 140,399 140,399 139 C399 139,398 138,398 137 C398 137,399 136,399 135 C399 134,399 133,399 132 C400 131,400 130,400 129 C400 128,401 127,401 126 C401 125,401 124,401 123 C402 122,402 121,402 120 C402 120,402 120,402 119 C402 119,402 119,402 119 C402 118,402 118,401 118 C401 117,400 117,399 117 L399 115 L407 115 L408 115 L404 132 Z" fill="#212121"/><path d="M434 98 C434 89,433 83,430 77 C427 72,423 68,417 66 L418 63 C425 65,431 69,434 75 C438 82,440 89,440 98 C440 107,438 115,434 121 C431 127,425 131,418 133 L417 130 C423 128,427 125,430 119 C433 114,434 106,434 98 L434 98 Z" fill="#212121"/><path d="M465 98 C465 89,464 83,461 77 C458 72,454 68,448 66 L449 63 C456 65,462 69,465 75 C469 82,471 89,471 98 C471 107,469 115,465 121 C462 127,456 131,449 133 L448 130 C454 128,458 125,461 119 C464 114,465 106,465 98 L465 98 Z" fill="#212121"/><path d="M489 51 L489 53 L480 75 L477 74 L482 51 L489 51 Z" fill="#212121"/><path d="M520 90 L520 85 L566 85 L566 90 L520 90 Z M520 105 L520 100 L566 100 L566 105 L520 105 Z" fill="#212121"/><path d="M596 93 C599 93,602 91,604 88 C606 85,607 81,607 75 C607 72,606 67,605 60 C604 52,603 46,603 41 C603 28,605 19,610 12 C614 5,620 1,627 0 L627 3 C625 4,622 5,620 6 C618 8,616 10,615 13 C614 16,612 19,612 23 C611 27,610 32,610 37 C610 41,611 47,612 55 C613 63,613 69,613 71 C613 78,612 83,610 87 C608 91,605 93,603 94 L603 95 C605 96,608 98,610 102 C612 106,613 112,613 118 C613 120,613 125,612 134 C611 144,610 150,610 152 C610 157,611 162,612 166 C612 170,613 173,615 176 C616 179,618 181,620 183 C622 185,624 186,627 186 L627 189 C620 189,614 185,610 178 C606 171,603 162,603 150 C603 146,604 139,605 131 C606 122,607 116,607 114 C607 108,606 104,604 101 C602 98,599 96,596 96 L596 93 Z" fill="#212121"/><path d="M650 5 L648 12 L642 12 L643 5 L650 5 Z M638 31 C638 28,639 26,639 25 C639 24,638 23,638 23 C637 22,636 22,635 22 L635 20 L644 19 L646 19 L641 44 C641 46,640 48,640 49 C640 50,640 51,641 51 C641 52,642 52,642 52 C643 52,644 52,645 51 C645 51,647 50,648 48 L650 50 C648 52,646 54,644 55 C643 56,641 56,639 56 C638 56,637 55,636 54 C635 53,634 52,634 50 C634 48,635 46,635 43 L638 31 Z" fill="#212121"/><path d="M681 29 L681 24 L727 24 L727 29 L681 29 Z M681 44 L681 39 L727 39 L727 44 L681 44 Z" fill="#212121"/><path d="M777 5 L776 12 L769 12 L771 5 L777 5 Z M766 55 C766 59,765 62,763 64 C762 67,760 68,758 69 C756 70,753 71,750 71 C749 71,748 71,747 71 L748 68 C749 68,750 68,751 68 C752 68,753 68,754 68 C755 67,755 67,756 66 C757 65,757 64,758 62 C759 61,759 59,760 57 L766 30 C766 28,766 26,766 25 C766 24,766 23,766 23 C765 22,764 22,763 22 L763 20 L772 19 L774 19 L766 55 Z" fill="#212121"/><path d="M974 55 C973 51,972 46,971 40 C970 33,969 29,968 27 C968 25,967 24,967 24 C967 23,966 23,966 23 C965 23,964 23,963 24 C963 25,962 26,961 27 L958 25 C960 23,962 22,963 21 C965 20,966 19,968 19 C969 19,970 19,971 19 C971 20,972 20,972 21 C973 21,973 22,973 22 C974 23,974 24,974 25 C975 26,975 28,975 30 C976 33,977 36,977 40 C978 43,978 47,978 50 C981 46,983 43,985 40 C986 36,987 34,988 31 C989 29,989 27,989 25 C989 24,989 23,988 23 C988 22,987 22,986 22 L986 20 L996 20 L997 22 C995 27,993 32,989 38 C986 44,983 50,979 55 C975 60,972 64,969 67 C967 68,965 70,964 70 C963 71,961 71,960 71 C959 71,959 71,958 71 C957 71,957 71,956 71 L958 64 L961 64 C961 65,961 66,962 66 C964 66,965 65,967 63 C969 62,971 59,974 55 L974 55 Z" fill="#212121"/><path d="M1003 45 L1001 51 L996 51 L997 45 L1003 45 Z M996 73 C996 74,996 75,996 76 C996 76,996 77,996 77 C996 78,996 78,996 79 C996 79,997 79,997 79 C998 79,998 79,998 79 C999 79,999 79,999 78 C1000 78,1000 78,1000 77 C1001 77,1001 76,1002 76 L1004 78 C1003 79,1002 80,1001 80 C1000 81,1000 81,999 82 C998 82,998 83,997 83 C996 83,996 83,995 83 C994 83,994 83,993 83 C993 83,992 82,992 82 C991 81,991 81,991 80 C991 80,990 79,990 78 C990 78,991 77,991 76 C991 75,991 74,991 73 C992 72,992 71,992 70 C992 69,993 68,993 67 C993 66,993 65,993 64 C994 63,994 62,994 61 C994 61,994 61,994 60 C994 60,994 60,994 60 C994 59,994 59,993 59 C993 58,992 58,991 58 L991 56 L999 56 L1000 56 L996 73 Z" fill="#212121"/><path d="M1021 37 C1021 45,1022 53,1025 58 C1028 64,1032 67,1038 69 L1037 72 C1030 70,1024 66,1021 60 C1017 54,1015 46,1015 37 C1015 28,1017 21,1021 14 C1024 8,1030 4,1037 2 L1038 5 C1032 7,1028 11,1025 16 C1022 22,1021 28,1021 37 L1021 37 Z" fill="#212121"/><path d="M1065 45 C1065 47,1065 48,1065 48 C1065 49,1066 50,1066 50 C1066 51,1067 51,1068 51 C1069 52,1069 52,1071 52 C1072 52,1073 52,1075 52 L1075 55 L1048 55 L1048 52 C1051 52,1053 52,1054 52 C1055 51,1056 51,1056 51 C1057 50,1057 50,1058 49 C1058 48,1058 47,1058 45 L1058 15 C1058 14,1058 14,1058 13 C1057 13,1057 13,1056 13 C1055 13,1054 13,1053 14 C1051 15,1050 16,1048 17 L1046 14 L1063 4 L1065 4 C1065 6,1065 10,1065 14 L1065 45 Z" fill="#212121"/><path d="M1103 36 L1103 31 L1149 31 L1149 36 L1103 36 Z" fill="#212121"/><path d="M1189 55 C1188 51,1187 46,1186 40 C1185 33,1184 29,1183 27 C1183 25,1182 24,1182 24 C1182 23,1181 23,1181 23 C1180 23,1179 23,1178 24 C1178 25,1177 26,1176 27 L1173 25 C1175 23,1177 22,1178 21 C1180 20,1181 19,1183 19 C1184 19,1185 19,1186 19 C1186 20,1187 20,1187 21 C1188 21,1188 22,1188 22 C1189 23,1189 24,1189 25 C1190 26,1190 28,1190 30 C1191 33,1192 36,1192 40 C1193 43,1193 47,1193 50 C1196 46,1198 43,1200 40 C1201 36,1202 34,1203 31 C1204 29,1204 27,1204 25 C1204 24,1204 23,1203 23 C1203 22,1202 22,1201 22 L1201 20 L1211 20 L1212 22 C1210 27,1208 32,1204 38 C1201 44,1198 50,1194 55 C1190 60,1187 64,1184 67 C1182 68,1180 70,1179 70 C1178 71,1176 71,1175 71 C1174 71,1174 71,1173 71 C1172 71,1172 71,1171 71 L1173 64 L1176 64 C1176 65,1176 66,1177 66 C1179 66,1180 65,1182 63 C1184 62,1186 59,1189 55 L1189 55 Z" fill="#212121"/><path d="M1218 45 L1216 51 L1211 51 L1212 45 L1218 45 Z M1211 73 C1211 74,1211 75,1211 76 C1211 76,1211 77,1211 77 C1211 78,1211 78,1211 79 C1211 79,1212 79,1212 79 C1213 79,1213 79,1213 79 C1214 79,1214 79,1214 78 C1215 78,1215 78,1215 77 C1216 77,1216 76,1217 76 L1219 78 C1218 79,1217 80,1216 80 C1215 81,1215 81,1214 82 C1213 82,1213 83,1212 83 C1211 83,1211 83,1210 83 C1209 83,1209 83,1208 83 C1208 83,1207 82,1207 82 C1206 81,1206 81,1206 80 C1206 80,1205 79,1205 78 C1205 78,1206 77,1206 76 C1206 75,1206 74,1206 73 C1207 72,1207 71,1207 70 C1207 69,1208 68,1208 67 C1208 66,1208 65,1208 64 C1209 63,1209 62,1209 61 C1209 61,1209 61,1209 60 C1209 60,1209 60,1209 60 C1209 59,1209 59,1208 59 C1208 58,1207 58,1206 58 L1206 56 L1214 56 L1215 56 L1211 73 Z" fill="#212121"/><path d="M1241 37 C1241 28,1240 22,1237 16 C1234 11,1230 7,1224 5 L1225 2 C1232 4,1238 8,1241 14 C1245 21,1247 28,1247 37 C1247 46,1245 54,1241 60 C1238 66,1232 70,1225 72 L1224 69 C1230 67,1234 64,1237 58 C1240 53,1241 45,1241 37 L1241 37 Z" fill="#212121"/><path d="M650 98 L648 105 L642 105 L643 98 L650 98 Z M638 124 C638 121,639 119,639 118 C639 117,638 116,638 116 C637 115,636 115,635 115 L635 113 L644 112 L646 112 L641 137 C641 139,640 141,640 142 C640 143,640 144,641 144 C641 145,642 145,642 145 C643 145,644 145,645 144 C645 144,647 143,648 141 L650 143 C648 145,646 147,644 148 C643 149,641 149,639 149 C638 149,637 148,636 147 C635 146,634 145,634 143 C634 141,635 139,635 136 L638 124 Z" fill="#212121"/><path d="M681 137 L681 132 L700 132 L704 122 L681 122 L681 117 L706 117 L711 105 L716 105 L711 117 L727 117 L727 122 L709 122 L704 132 L727 132 L727 137 L702 137 L697 150 L692 150 L698 137 L681 137 Z" fill="#212121"/><path d="M777 98 L776 105 L769 105 L771 98 L777 98 Z M766 148 C766 152,765 155,763 157 C762 160,760 161,758 162 C756 163,753 164,750 164 C749 164,748 164,747 164 L748 161 C749 161,750 161,751 161 C752 161,753 161,754 161 C755 160,755 160,756 159 C757 158,757 157,758 155 C759 154,759 152,760 150 L766 123 C766 121,766 119,766 118 C766 117,766 116,766 116 C765 115,764 115,763 115 L763 113 L772 112 L774 112 L766 148 Z" fill="#212121"/><path d="M961 129 L961 124 L1007 124 L1007 129 L961 129 Z" fill="#212121"/><path d="M1047 148 C1046 144,1045 139,1044 133 C1043 126,1042 122,1041 120 C1041 118,1040 117,1040 117 C1040 116,1039 116,1039 116 C1038 116,1037 116,1036 117 C1036 118,1035 119,1034 120 L1031 118 C1033 116,1035 115,1036 114 C1038 113,1039 112,1041 112 C1042 112,1043 112,1044 112 C1044 113,1045 113,1045 114 C1046 114,1046 115,1046 115 C1047 116,1047 117,1047 118 C1048 119,1048 121,1048 123 C1049 126,1050 129,1050 133 C1051 136,1051 140,1051 143 C1054 139,1056 136,1058 133 C1059 129,1060 127,1061 124 C1062 122,1062 120,1062 118 C1062 117,1062 116,1061 116 C1061 115,1060 115,1059 115 L1059 113 L1069 113 L1070 115 C1068 120,1066 125,1062 131 C1059 137,1056 143,1052 148 C1048 153,1045 157,1042 160 C1040 161,1038 163,1037 163 C1036 164,1034 164,1033 164 C1032 164,1032 164,1031 164 C1030 164,1030 164,1029 164 L1031 157 L1034 157 C1034 158,1034 159,1035 159 C1037 159,1038 158,1040 156 C1042 155,1044 152,1047 148 L1047 148 Z" fill="#212121"/><path d="M1076 138 L1074 144 L1069 144 L1070 138 L1076 138 Z M1069 166 C1069 167,1069 168,1069 169 C1069 169,1069 170,1069 170 C1069 171,1069 171,1069 172 C1069 172,1070 172,1070 172 C1071 172,1071 172,1071 172 C1072 172,1072 172,1072 171 C1073 171,1073 171,1073 170 C1074 170,1074 169,1075 169 L1077 171 C1076 172,1075 173,1074 173 C1073 174,1073 174,1072 175 C1071 175,1071 176,1070 176 C1069 176,1069 176,1068 176 C1067 176,1067 176,1066 176 C1066 176,1065 175,1065 175 C1064 174,1064 174,1064 173 C1064 173,1063 172,1063 171 C1063 171,1064 170,1064 169 C1064 168,1064 167,1064 166 C1065 165,1065 164,1065 163 C1065 162,1066 161,1066 160 C1066 159,1066 158,1066 157 C1067 156,1067 155,1067 154 C1067 154,1067 154,1067 153 C1067 153,1067 153,1067 153 C1067 152,1067 152,1066 152 C1066 151,1065 151,1064 151 L1064 149 L1072 149 L1073 149 L1069 166 Z" fill="#212121"/><path d="M1099 148 C1098 144,1097 139,1096 133 C1095 126,1094 122,1093 120 C1093 118,1092 117,1092 117 C1092 116,1091 116,1091 116 C1090 116,1089 116,1088 117 C1088 118,1087 119,1086 120 L1083 118 C1085 116,1087 115,1088 114 C1090 113,1091 112,1093 112 C1094 112,1095 112,1096 112 C1096 113,1097 113,1097 114 C1098 114,1098 115,1098 115 C1099 116,1099 117,1099 118 C1100 119,1100 121,1100 123 C1101 126,1102 129,1102 133 C1103 136,1103 140,1103 143 C1106 139,1108 136,1110 133 C1111 129,1112 127,1113 124 C1114 122,1114 120,1114 118 C1114 117,1114 116,1113 116 C1113 115,1112 115,1111 115 L1111 113 L1121 113 L1122 115 C1120 120,1118 125,1114 131 C1111 137,1108 143,1104 148 C1100 153,1097 157,1094 160 C1092 161,1090 163,1089 163 C1088 164,1086 164,1085 164 C1084 164,1084 164,1083 164 C1082 164,1082 164,1081 164 L1083 157 L1086 157 C1086 158,1086 159,1087 159 C1089 159,1090 158,1092 156 C1094 155,1096 152,1099 148 L1099 148 Z" fill="#212121"/><path d="M1140 138 L1139 144 L1133 144 L1134 138 L1140 138 Z M1132 175 C1131 178,1131 179,1130 181 C1129 182,1128 183,1127 184 C1126 185,1125 186,1124 186 C1122 187,1121 187,1119 187 C1119 187,1118 187,1118 187 C1117 187,1117 187,1117 187 L1117 184 C1118 184,1118 184,1118 184 C1119 184,1119 184,1119 184 C1120 184,1121 184,1122 184 C1123 183,1123 183,1124 182 C1124 182,1125 181,1125 180 C1126 179,1126 177,1126 176 C1127 173,1128 170,1128 168 C1129 165,1129 163,1130 161 C1130 160,1130 158,1130 157 C1131 156,1131 155,1131 155 C1131 154,1131 154,1131 154 C1131 153,1131 153,1131 153 C1131 152,1131 152,1130 152 C1130 151,1129 151,1128 151 L1128 149 L1135 149 L1137 149 L1132 175 Z" fill="#212121"/></svg></span></p>
<p class="p_Text"><span class="f_Text">The practical part of the error gradient adjustment operations is implemented using MQL5 matrix operations. After adjusting the error gradients, we divide the resulting vector by the square root of the dimension of the </span><span class="f_Text" style="font-style: italic;">Key </span><span class="f_Text">vector of one element of the sequence. We performed the same operation during the feed-forward pass to prevent uncontrolled growth of non-normalized dependency coefficients.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;gradient&nbsp;distribution&nbsp;to&nbsp;Querys&nbsp;and&nbsp;Keys</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">gradients</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">values</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">].</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">());</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">ident</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">Identity</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">ones</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">Ones</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iUnits</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">ones</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">()&nbsp;*&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">ident</span><span class="f_CodeExample">&nbsp;-&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score_grad</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">score</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Transpose</span><span class="f_CodeExample">())&nbsp;/</span>
<br><span class="f_Functions">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sqrt</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_Definition">MATRIX</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">MatMul</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">keys</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">]);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">querys_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">querys</span><span class="f_CodeExample">[</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">]&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">score_grad[</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample" style="color: #333333;">,&nbsp;m_iCurrentPosition]</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">keys_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #333333;">head</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">As a result of these operations, we obtain the adjusted error gradient for one element of the dependency coefficient vector. But we will not save it to the next data buffer. Instead, we will immediately distribute it to the corresponding elements of the </span><span class="f_Text" style="font-style: italic;">Query </span><span class="f_Text">and </span><span class="f_Text" style="font-style: italic;">Key</span><span class="f_Text"> tensors. To do this, we need to multiply this value by the vector of the opposite tensor. To determine the error gradient on the </span><span class="f_Parameters" style="font-style: italic;">Qwery </span><span class="f_Text">vector, </span><span class="f_Parameters">we have a complete set of sequence elements in the </span><span class="f_Parameters" style="font-style: italic;">Key</span><span class="f_Parameters"> tensor. However, in the </span><span class="f_Parameters" style="font-style: italic;">Qwery</span><span class="f_Parameters"> tensor, we only have one sequence element. Therefore, the error gradient on the </span><span class="f_Parameters" style="font-style: italic;">Key </span><span class="f_Parameters">tensor will be propagated only for the current element of the sequence. We save the obtained error gradient values into the matrices we prepared earlier.</span></p>
<p class="p_Text"><span class="f_Text">By obtaining error gradients at the levels of </span><span class="f_Text" style="font-style: italic;">Query</span><span class="f_Text"> and </span><span class="f_Text" style="font-style: italic;">Keys </span><span class="f_Text">tensors</span><span class="f_Text" style="font-style: italic;">,</span><span class="f_Text"> we complete the operations of the loop through attention heads.</span></p>
<p class="p_Text"><span class="f_Text">As soon as the full loop of iterations is completed, our </span><span class="f_Text" style="font-style: italic;">querys_grad</span><span class="f_Text">, </span><span class="f_Text" style="font-style: italic;">keys_grad</span><span class="f_Text">, and </span><span class="f_Text" style="font-style: italic;">values_grad</span><span class="f_Text"> matrices will contain the accumulated error gradients for the current sequence element across all attention heads. All we have to do is transfer its values to the error gradient buffer of our internal </span><span class="f_Text" style="font-style: italic;">Querys</span><span class="f_Text"> layer.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">querys_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">keys_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">values_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iHeads</span><span class="f_CodeExample">&nbsp;*&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iKeysSize</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_Functions">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">querys_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">keys_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">)&nbsp;||</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">values_grad</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">Row</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">),&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">2</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Reshape</span><span class="f_CodeExample">(</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">Total</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">else</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//&nbsp;OpenCL&nbsp;block</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">This concludes the block for separating the operations of the algorithm depending on the device for performing the operations. Next, we will continue executing the algorithm using the methods of our internal neural layers.</span></p>
<p class="p_Text"><span class="f_Text">Previously, we obtained a concatenated tensor of error gradients that includes data from all attention heads and from all three entities (</span><span class="f_Text" style="font-style: italic;">Query, Key, Value</span><span class="f_Text">). Now, using the method that propagates the gradient through the hidden layer of our internal neural layer </span><span class="f_Text" style="font-style: italic;">Querys.CalcHiddenGradient</span><span class="f_Text">, we can transfer the error gradient to the previous layer buffer. Before performing this operation, we need to decide in which object’s buffer we will write the error gradients. We created this class as a multi-layer block, and all operations of the method are performed in a loop iterating through the active layer of our block. Therefore, to the object of the previous neural layer, whose pointer we received in the parameters of this method, we transfer data only from the first neural layer of our block. It will have index 0 in the collection of nested neural layers of our </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> block. All other nested neural layers must pass the error gradient to the internal neural layer buffer </span><span class="f_Text" style="font-style: italic;">FF2</span><span class="f_Text"> of the previous nested neural layer. Let me remind you that </span><span class="f_Text" style="font-style: italic;">FF2</span><span class="f_Text"> is the internal neural layer with the results of the </span><span class="f_Text" style="font-style: italic;">Feed Forward</span><span class="f_Text"> block.</span></p>
<p class="p_Text"><span class="f_Text">Therefore, we will create a local pointer to the object of the previous neural layer and assign it a pointer to the required object depending on the index of the active nested neural layer in our </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> block. Only after obtaining the correct pointer to the object of the correct previous layer, we transfer the error gradient to its buffer.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #808080;">//---&nbsp;transfer&nbsp;the&nbsp;error&nbsp;gradient&nbsp;to&nbsp;the&nbsp;previous&nbsp;layer</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevL</span><span class="f_CodeExample">&nbsp;=&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;==&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">&nbsp;?&nbsp;</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">&nbsp;:&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;-&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">));</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">Querys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcHiddenGradient</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevL</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">prevL</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">().</span><span class="f_CodeExample" style="color: #333333;">SumArray</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">W0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">GetGradients</span><span class="f_CodeExample">()))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Please note that when constructing similar methods in the implementation classes of attention mechanisms, at this point, we created a complete procedure for summing error gradients from four directions. Now, thanks to the use of the concatenated error gradient buffer, we obtain the total error gradient from three directions by executing the method of only one neural layer. We still have to add gradients, but only once. To the obtained error gradient, we will add the error gradient at the level of the outputs of the multi-head attention block. You remember that during the feed-forward pass, we also added the original data with the tensor of the multi-head attention block's outputs. Therefore, the error gradient must go through all the steps that the signal goes through during the feed-forward pass, but in reverse order.</span></p>
<p class="p_Text"><span class="f_Text">This concludes the operations in the body of the loop iterating through the nested neural layers of our </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> block, as well as the overall operations of our method. We close the loop and exit the method.</span></p>
<p class="p_Text"><span class="f_Text">And once again, I want to emphasize: do not forget to monitor every step of the operation execution. This helps minimize the risk of critical errors and makes the program operation more controlled and reliable.</span></p>
<p class="p_Text"><span class="f_Text">We have discussed the organization of the error gradient propagation method to the previous layer. But this is only one of the three backpropagation methods that we must override for this class. Therefore, after propagating the error gradient to the previous neural layer, we need to propagate the error gradient to the internal weight matrices contained within the depths of a considerable number of internal objects of the neural layers. In accordance with the structure of our class methods, this functionality is performed in the </span><span class="f_Text" style="font-style: italic;">CalcDeltaWeights</span><span class="f_Text"> method.</span></p>
<p class="p_Text"><span class="f_Text">To propagate the error gradient to the weight matrix of any of the previously discussed neural layers, two things are necessary:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li">The error gradient at the output level of a given neural layer to the activation function.</span></li>
<li class="p_li"><span class="f_li">The initial data provided by the previous neural layer.</span></li>
</ul>
<p class="p_Text"><span class="f_Text">To organize this process, we already have all the necessary data. In the previous method, we distributed the error gradient to each neural layer. We will get a pointer to the previous neural layer in the parameters of the </span><span class="f_Text" style="font-style: italic;">CNeuronGPT::CalcDeltaWeights</span><span class="f_Text"> method.</span></p>
<p class="p_Text"><span class="f_Text">As usual, in the body of the method, we organize a control block to check the pointers of all used internal objects. The control block should be minimal and sufficient. Eliminate redundant and explicitly repetitive controls, as they do not add value to the program operation and can slow it down. Moreover, each operation, including control, requires resources and time. Let's think about the objects for which we should update weight matrices. These include:</span></p>
<ul style="list-style-type:disc">
<li class="p_li"><span class="f_li">The </span><span class="f_li" style="font-style: italic;">Query</span><span class="f_li"> neural layer, which returns a concatenated tensor of three entities (Query, Key, Value).</span></li>
<li class="p_li"><span class="f_li">The </span><span class="f_li" style="font-style: italic;">W</span><span class="f_li" style="font-size: 7pt; font-style: italic; vertical-align: sub;">0</span><span class="f_li"> matrix neural layer.</span></li>
<li class="p_li"><span class="f_li">Two neural layers of the </span><span class="f_li" style="font-style: italic;">Feed Forward</span><span class="f_li"> block.</span></li>
</ul>
<p class="p_Text"><span class="f_Text">All the mentioned objects are declared static. Therefore, there is no need to check their pointers since their presence is controlled by the system. This allows us to exclude the control block from this method.</span></p>
<p class="p_Text"><span class="f_Text">Everything else is straightforward and simple. Let's organize a loop through all the nested neural layers of our </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> block. In the body of the block, we extract all the objects of the above collections, one by one. First, we check the pointer to the object, and then we call its method to propagate the error gradient to the level of the weight matrix.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronGPT</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">read</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;in&nbsp;a&nbsp;loop,&nbsp;we&nbsp;call&nbsp;the&nbsp;method&nbsp;for&nbsp;each&nbsp;internal&nbsp;object</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">m_cAttentionOut</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">),&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">prevL</span><span class="f_CodeExample">&nbsp;=&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;==&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">&nbsp;?&nbsp;</span><span class="f_CodeExample" style="color: #333333;">prevLayer</span><span class="f_CodeExample">&nbsp;:&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;-&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">));</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">CalcDeltaWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">prevL</span><span class="f_CodeExample">,&nbsp;(</span><span class="f_CodeExample" style="color: #333333;">read</span><span class="f_CodeExample">&nbsp;&amp;&amp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;==&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">&nbsp;-&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">1</span><span class="f_CodeExample">)))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">It is worth mentioning a few words about the order in which methods of internal objects are called. From the perspective of mathematical operations, the order of method calls does not affect the final result. However, the order of method calls used in the loop body is not random. Note that in the loop body, we explicitly check the pointers for only two objects that do not serve as the input data for other internal layers. The reason is that the called methods of neural layers also have a control block that checks the incoming data, including the received pointers. To eliminate repeated checks of object pointers, we first pass a pointer to the object as input to another object, check the result of the operations of the called method, which, among other things, confirms the validity of the passed pointer, and then confidently access the object because its pointer was checked during the execution of the previous object method. In this way, we organize a comprehensive check of all object pointers without explicit control within the method body and eliminate redundant pointer checks that could slow down the program execution.</span></p>
<p class="p_Text"><span class="f_Text">Next, we will consider the method for updating model parameters. This function does not require external object data. There is not a single object pointer in the method parameters, as there are only parameter values for executing the specified parameter optimization algorithm.</span></p>
<p class="p_Text"><span class="f_Text">In the method body, we also organize a loop to iterate through the nested neural layers of our </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> block. In the loop body, we extract one object from each collection, check the validity of the pointer, and call the method to update the weight matrix of each object.</span></p>
<div style="text-align: left; text-indent: 0; line-height: 1.0; page-break-inside: avoid; page-break-after: avoid; border-color: #d8dfea; border-style: solid; border-width: thin; background: #fbf9f5; padding: 0 0 0 0; margin: 2px 17px 2px 17px;"><table cellspacing="0" cellpadding="3" border="0" style="text-align: justify;border:none; border-spacing:0;">
<tr>
<td style="vertical-align:top; padding:3px; border:none"><p class="p_CodeExample" style="page-break-inside: avoid; page-break-after: avoid;"><span class="f_CodeExample" style="color: #0000ff;">bool</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronGPT</span><span class="f_CodeExample">::</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">batch_size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Definition">TYPE</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,</span>
<br><span class="f_Definition">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VECTOR</span><span class="f_CodeExample">&nbsp;&amp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_Definition">VECTOR</span><span class="f_CodeExample">&nbsp;&amp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample" style="color: #808080;">//---&nbsp;in&nbsp;a&nbsp;loop&nbsp;we&nbsp;call&nbsp;the&nbsp;method&nbsp;for&nbsp;each&nbsp;internal&nbsp;object</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">for</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #0000ff;">int</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_Normal" style="font-family: Arial,Helvetica,sans-serif; color: #000000; background-color: transparent;">0</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">&nbsp;&lt;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_iLayers</span><span class="f_CodeExample">;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">++)</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">CNeuronBase</span><span class="f_CodeExample">&nbsp;*</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF2</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">batch_size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cFF1</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">batch_size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cW0</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">batch_size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;=&nbsp;</span><span class="f_CodeExample" style="color: #333333;">m_cQuerys</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">At</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">layer</span><span class="f_CodeExample">);</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">if</span><span class="f_CodeExample">(!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">&nbsp;||&nbsp;!</span><span class="f_CodeExample" style="color: #333333;">temp</span><span class="f_CodeExample">.</span><span class="f_CodeExample" style="color: #333333;">UpdateWeights</span><span class="f_CodeExample">(</span><span class="f_CodeExample" style="color: #333333;">batch_size</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">learningRate</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Beta</span><span class="f_CodeExample">,&nbsp;</span><span class="f_CodeExample" style="color: #333333;">Lambda</span><span class="f_CodeExample">))</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">false</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span>
<br><span class="f_CodeExample" style="color: #808080;">//---</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;&nbsp;</span><span class="f_CodeExample" style="color: #0000ff;">return</span><span class="f_CodeExample">&nbsp;</span><span class="f_CodeExample" style="color: #ff0000;">true</span><span class="f_CodeExample">;</span>
<br><span class="f_CodeExample">&nbsp;&nbsp;}</span></p>
</td>
</tr>
</table>
</div>
<p class="p_Text"><span class="f_Text">Since the called methods do not access external objects, our control optimization approach will not work here due to the absence of explicitly repetitive controls. Therefore, we need to explicitly check each object pointer before calling its method.</span></p>
<p class="p_Text"><span class="f_Text">We have discussed the implementation of three backpropagation methods and with that, we conclude our work on implementing the </span><span class="f_Text" style="font-style: italic;">GPT</span><span class="f_Text"> model algorithm in our </span><span class="f_Text" style="font-style: italic;">CNeuronGPT</span><span class="f_Text"> class. For the complete implementation of functionality using standard MQL5 tools, we need to override the methods for working with files. We've already discussed the importance of these methods for the operation of neural network models.</span></p>

</div>

</body>
</html>
